<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Block Pointer (Experimental) &mdash; Triton  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Group GEMM" href="11-grouped-gemm.html" />
    <link rel="prev" title="Libdevice (tl.math) function" href="07-math-functions.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Triton
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01-vector-add.html">Vector Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="02-fused-softmax.html">Fused Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="03-matrix-multiplication.html">Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="04-low-memory-dropout.html">Low-Memory Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="05-layer-norm.html">Layer Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="06-fused-attention.html">Fused Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="07-math-functions.html">Libdevice (<cite>tl.math</cite>) function</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Block Pointer (Experimental)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#motivations">Motivations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#make-a-block-pointer">Make a Block Pointer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-store-a-block-pointer">Load/Store a Block Pointer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advance-a-block-pointer">Advance a Block Pointer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#final-result">Final Result</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unit-test">Unit Test</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="11-grouped-gemm.html">Group GEMM</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.html">triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.language.html">triton.language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.testing.html">triton.testing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton MLIR Dialects</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dialects/dialects.html">Triton MLIR Dialects and Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Programming Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../programming-guide/chapter-1/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../programming-guide/chapter-2/related-work.html">Related Work</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Triton</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Block Pointer (Experimental)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="block-pointer-experimental">
<span id="sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py"></span><h1>Block Pointer (Experimental)<a class="headerlink" href="#block-pointer-experimental" title="Permalink to this heading">¶</a></h1>
<p>This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.
These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.
Note that this feature is still experimental and may change in the future.</p>
<section id="motivations">
<h2>Motivations<a class="headerlink" href="#motivations" title="Permalink to this heading">¶</a></h2>
<p>In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,
i.e., <code class="code docutils literal notranslate"><span class="pre">load(block&lt;pointer_type&lt;element_type&gt;&gt;)</span> <span class="pre">-&gt;</span> <span class="pre">block&lt;element_type&gt;</span></code>, which involved loading blocks of
elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex
data structures, such as tensors of trees or unstructured look-up tables.</p>
<p>However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to
optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the
optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial
data structures commonly used in machine learning workloads, this problem is likely to worsen.</p>
<p>To address this issue, we will use block pointers <code class="code docutils literal notranslate"><span class="pre">pointer_type&lt;block&lt;element_type&gt;&gt;</span></code> and load them into
<code class="code docutils literal notranslate"><span class="pre">block&lt;element_type&gt;</span></code>, in which way gives better friendliness for the compiler to optimize memory access
patterns.</p>
<p>Let’s start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer
semantics.</p>
</section>
<section id="make-a-block-pointer">
<h2>Make a Block Pointer<a class="headerlink" href="#make-a-block-pointer" title="Permalink to this heading">¶</a></h2>
<p>A block pointer pointers to a block in a parent tensor and is constructed by <code class="code docutils literal notranslate"><span class="pre">make_block_ptr</span></code> function,
which takes the following information as arguments:</p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">base</span></code>: the base pointer to the parent tensor;</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">shape</span></code>: the shape of the parent tensor;</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">offsets</span></code>: the offsets of the block;</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">block_shape</span></code>: the shape of the block;</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">order</span></code>: the order of the block, which means how the block is laid out in memory.</p></li>
</ul>
<p>For example, to a block pointer to a <code class="code docutils literal notranslate"><span class="pre">BLOCK_SIZE_M</span> <span class="pre">*</span> <span class="pre">BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by
offsets <code class="code docutils literal notranslate"><span class="pre">(pid_m</span> <span class="pre">*</span> <span class="pre">BLOCK_SIZE_M,</span> <span class="pre">0)</span></code> and strides <code class="code docutils literal notranslate"><span class="pre">(stride_am,</span> <span class="pre">stride_ak)</span></code>, we can use the following code
(exactly the same as the previous matrix multiplication tutorial):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="n">a_ptr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_am</span><span class="p">,</span> <span class="n">stride_ak</span><span class="p">),</span>
                                <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">),</span>
                                <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that the <code class="code docutils literal notranslate"><span class="pre">order</span></code> argument is set to <code class="code docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">0)</span></code>, which means the second axis is the inner dimension in
terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary
for some hardware backends to optimize for better performance.</p>
</section>
<section id="load-store-a-block-pointer">
<h2>Load/Store a Block Pointer<a class="headerlink" href="#load-store-a-block-pointer" title="Permalink to this heading">¶</a></h2>
<p>To load/store a block pointer, we can use <code class="code docutils literal notranslate"><span class="pre">load/store</span></code> function, which takes a block pointer as an argument,
de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument
<code class="code docutils literal notranslate"><span class="pre">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on,
out-of-bound values will be masked according to the <code class="code docutils literal notranslate"><span class="pre">padding_option</span></code> argument (load only), which can be
<code class="code docutils literal notranslate"><span class="pre">zero</span></code> or <code class="code docutils literal notranslate"><span class="pre">nan</span></code>. Temporarily, we do not support other values due to some hardware limitations. In this
mode of block pointer load/store does not support <code class="code docutils literal notranslate"><span class="pre">mask</span></code> or <code class="code docutils literal notranslate"><span class="pre">other</span></code> arguments in the legacy mode.</p>
<p>So to load the block pointer of A in the previous section, we can simply write
<code class="code docutils literal notranslate"><span class="pre">a</span> <span class="pre">=</span> <span class="pre">tl.load(a_block_ptr,</span> <span class="pre">boundary_check=(0,</span> <span class="pre">1))</span></code>. Boundary check may cost extra performance, so if you can
guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index
into the <code class="code docutils literal notranslate"><span class="pre">boundary_check</span></code> argument. For example, if we know that <code class="code docutils literal notranslate"><span class="pre">M</span></code> is a multiple of
<code class="code docutils literal notranslate"><span class="pre">BLOCK_SIZE_M</span></code>, we can replace with <code class="code docutils literal notranslate"><span class="pre">a</span> <span class="pre">=</span> <span class="pre">tl.load(a_block_ptr,</span> <span class="pre">boundary_check=(1,</span> <span class="pre">))</span></code>, since axis 0 is
always in bound.</p>
</section>
<section id="advance-a-block-pointer">
<h2>Advance a Block Pointer<a class="headerlink" href="#advance-a-block-pointer" title="Permalink to this heading">¶</a></h2>
<p>To advance a block pointer, we can use <code class="code docutils literal notranslate"><span class="pre">advance</span></code> function, which takes a block pointer and the increment for
each axis as arguments and returns a new block pointer with the same shape and strides as the original one,
but with the offsets advanced by the specified amount.</p>
<p>For example, to advance the block pointer by <code class="code docutils literal notranslate"><span class="pre">BLOCK_SIZE_K</span></code> in the second axis
(no need to multiply with strides), we can write <code class="code docutils literal notranslate"><span class="pre">a_block_ptr</span> <span class="pre">=</span> <span class="pre">tl.advance(a_block_ptr,</span> <span class="pre">(0,</span> <span class="pre">BLOCK_SIZE_K))</span></code>.</p>
</section>
<section id="final-result">
<h2>Final Result<a class="headerlink" href="#final-result" title="Permalink to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">triton</span>
<span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>


<span class="nd">@triton</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span>
    <span class="n">configs</span><span class="o">=</span><span class="p">[</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">triton</span><span class="o">.</span><span class="n">Config</span><span class="p">({</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;BLOCK_SIZE_K&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s1">&#39;GROUP_SIZE_M&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">},</span> <span class="n">num_stages</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                      <span class="n">num_warps</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">key</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;K&#39;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">matmul_kernel_with_block_pointers</span><span class="p">(</span>
        <span class="c1"># Pointers to matrices</span>
        <span class="n">a_ptr</span><span class="p">,</span> <span class="n">b_ptr</span><span class="p">,</span> <span class="n">c_ptr</span><span class="p">,</span>
        <span class="c1"># Matrix dimensions</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span>
        <span class="c1"># The stride variables represent how much to increase the ptr by when moving by 1</span>
        <span class="c1"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>
        <span class="c1"># by to get the element one row down (A has M rows).</span>
        <span class="n">stride_am</span><span class="p">,</span> <span class="n">stride_ak</span><span class="p">,</span>  <span class="c1">#</span>
        <span class="n">stride_bk</span><span class="p">,</span> <span class="n">stride_bn</span><span class="p">,</span>  <span class="c1">#</span>
        <span class="n">stride_cm</span><span class="p">,</span> <span class="n">stride_cn</span><span class="p">,</span>
        <span class="c1"># Meta-parameters</span>
        <span class="n">BLOCK_SIZE_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">GROUP_SIZE_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>
<span class="sd">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># -----------------------------------------------------------</span>
    <span class="c1"># Map program ids `pid` to the block of C it should compute.</span>
    <span class="c1"># This is done in a grouped ordering to promote L2 data reuse.</span>
    <span class="c1"># See the matrix multiplication tutorial for details.</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">num_pid_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_M</span><span class="p">)</span>
    <span class="n">num_pid_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">)</span>
    <span class="n">num_pid_in_group</span> <span class="o">=</span> <span class="n">GROUP_SIZE_M</span> <span class="o">*</span> <span class="n">num_pid_n</span>
    <span class="n">group_id</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">//</span> <span class="n">num_pid_in_group</span>
    <span class="n">first_pid_m</span> <span class="o">=</span> <span class="n">group_id</span> <span class="o">*</span> <span class="n">GROUP_SIZE_M</span>
    <span class="n">group_size_m</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_pid_m</span> <span class="o">-</span> <span class="n">first_pid_m</span><span class="p">,</span> <span class="n">GROUP_SIZE_M</span><span class="p">)</span>
    <span class="n">pid_m</span> <span class="o">=</span> <span class="n">first_pid_m</span> <span class="o">+</span> <span class="p">(</span><span class="n">pid</span> <span class="o">%</span> <span class="n">group_size_m</span><span class="p">)</span>
    <span class="n">pid_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">pid</span> <span class="o">%</span> <span class="n">num_pid_in_group</span><span class="p">)</span> <span class="o">//</span> <span class="n">group_size_m</span>

    <span class="c1"># ----------------------------------------------------------</span>
    <span class="c1"># Create block pointers for the first blocks of A and B.</span>
    <span class="c1"># We will advance this pointer as we move in the K direction and accumulate.</span>
    <span class="c1"># See above `Make a Block Pointer` section for details.</span>
    <span class="n">a_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="n">a_ptr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_am</span><span class="p">,</span> <span class="n">stride_ak</span><span class="p">),</span>
                                    <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">),</span>
                                    <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">b_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="n">b_ptr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_bk</span><span class="p">,</span> <span class="n">stride_bn</span><span class="p">),</span>
                                    <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pid_n</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span> <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_SIZE_K</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span>
                                    <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># -----------------------------------------------------------</span>
    <span class="c1"># Iterate to compute a block of the C matrix.</span>
    <span class="c1"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.</span>
    <span class="c1"># of fp32 values for higher accuracy.</span>
    <span class="c1"># `accumulator` will be converted back to fp16 after the loop.</span>
    <span class="n">accumulator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">):</span>
        <span class="c1"># Load with boundary checks, no need to calculate the mask manually.</span>
        <span class="c1"># For better performance, you may remove some axis from the boundary</span>
        <span class="c1"># check, if you can guarantee that the access is always in-bound in</span>
        <span class="c1"># that axis.</span>
        <span class="c1"># See above `Load/Store a Block Pointer` section for details.</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">a_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">b_block_ptr</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># We accumulate along the K dimension.</span>
        <span class="n">accumulator</span> <span class="o">+=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="c1"># Advance the block pointer to the next K block.</span>
        <span class="c1"># See above `Advance a Block Pointer` section for details.</span>
        <span class="n">a_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">a_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE_K</span><span class="p">))</span>
        <span class="n">b_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">advance</span><span class="p">(</span><span class="n">b_block_ptr</span><span class="p">,</span> <span class="p">(</span><span class="n">BLOCK_SIZE_K</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">accumulator</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

    <span class="c1"># ----------------------------------------------------------------</span>
    <span class="c1"># Write back the block of the output matrix C with boundary checks.</span>
    <span class="c1"># See above `Load/Store a Block Pointer` section for details.</span>
    <span class="n">c_block_ptr</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">make_block_ptr</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="n">c_ptr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride_cm</span><span class="p">,</span> <span class="n">stride_cn</span><span class="p">),</span>
                                    <span class="n">offsets</span><span class="o">=</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">pid_n</span> <span class="o">*</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span>
                                    <span class="n">block_shape</span><span class="o">=</span><span class="p">(</span><span class="n">BLOCK_SIZE_M</span><span class="p">,</span> <span class="n">BLOCK_SIZE_N</span><span class="p">),</span> <span class="n">order</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">c_block_ptr</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">boundary_check</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>


<span class="c1"># We can now create a convenience wrapper function that only takes two input tensors,</span>
<span class="c1"># and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</span>
<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Check constraints.</span>
    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;Incompatible dimensions&quot;</span>
    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s2">&quot;Matrix A must be contiguous&quot;</span>
    <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(),</span> <span class="s2">&quot;Matrix B must be contiguous&quot;</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">K</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># Allocates output.</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># 1D launch kernel where each block gets its own program.</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">META</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">META</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE_M&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">META</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE_N&#39;</span><span class="p">]),</span> <span class="p">)</span>
    <span class="n">matmul_kernel_with_block_pointers</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span>  <span class="c1">#</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span>  <span class="c1">#</span>
        <span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">a</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1">#</span>
        <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1">#</span>
        <span class="n">c</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">c</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">c</span>
</pre></div>
</div>
</section>
<section id="unit-test">
<h2>Unit Test<a class="headerlink" href="#unit-test" title="Permalink to this heading">¶</a></h2>
<p>Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">triton_output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;triton_output=</span><span class="si">{</span><span class="n">triton_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch_output=</span><span class="si">{</span><span class="n">torch_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">triton_output</span><span class="p">,</span> <span class="n">torch_output</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✅ Triton and Torch match&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;❌ Triton and Torch differ&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],
        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],
        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],
        ...,
        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],
        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],
        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],
       device=&#39;cuda:0&#39;, dtype=torch.float16)
torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],
        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],
        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],
        ...,
        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],
        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],
        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],
       device=&#39;cuda:0&#39;, dtype=torch.float16)
✅ Triton and Torch match
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  6.237 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">08-experimental-block-pointer.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">08-experimental-block-pointer.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="07-math-functions.html" class="btn btn-neutral float-left" title="Libdevice (tl.math) function" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="11-grouped-gemm.html" class="btn btn-neutral float-right" title="Group GEMM" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Philippe Tillet.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TritonNvidiaGPUOps &mdash; Triton  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />

  
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NVGPUOps" href="NVGPUOps.html" />
    <link rel="prev" title="TritonOps" href="TritonOps.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Triton
              <img src="https://cdn.openai.com/triton/assets/triton-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python-api/triton.html">triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/triton.language.html">triton.language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/triton.testing.html">triton.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/triton-semantics.html">Triton Semantics</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton MLIR Dialects</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="dialects.html">Triton MLIR Dialects and Ops</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="GluonDialect.html">‘gluon’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonNvidiaGPUDialect.html">‘ttng’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonGPUDialect.html">‘ttg’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProtonDialect.html">‘proton’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="NVGPUDialect.html">‘nvgpu’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProtonGPUDialect.html">‘proton_gpu’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="NVWSDialect.html">‘nvws’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonDialect.html">‘tt’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonInstrumentDialect.html">‘tti’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonAMDGPUDialect.html">‘amdgpu’ Dialect</a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonInstrumentOps.html">TritonInstrumentOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonAMDGPUOps.html">TritonAMDGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonGPUOps.html">TritonGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="ProtonGPUOps.html">ProtonGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="ProtonOps.html">ProtonOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="GluonOps.html">GluonOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="TritonOps.html">TritonOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">TritonNvidiaGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ttng-arrive-barrier-triton-nvidia-gpu-arrivebarrierop"><code class="docutils literal notranslate"><span class="pre">ttng.arrive_barrier</span></code> (triton::nvidia_gpu::ArriveBarrierOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#attributes">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#operands">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-copy-mbarrier-arrive-triton-nvidia-gpu-asynccopymbarrierarriveop"><code class="docutils literal notranslate"><span class="pre">ttng.async_copy_mbarrier_arrive</span></code> (triton::nvidia_gpu::AsyncCopyMbarrierArriveOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-copy-global-to-local-triton-nvidia-gpu-asynctmacopyglobaltolocalop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_copy_global_to_local</span></code> (triton::nvidia_gpu::AsyncTMACopyGlobalToLocalOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-copy-local-to-global-triton-nvidia-gpu-asynctmacopylocaltoglobalop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_copy_local_to_global</span></code> (triton::nvidia_gpu::AsyncTMACopyLocalToGlobalOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-gather-triton-nvidia-gpu-asynctmagatherop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_gather</span></code> (triton::nvidia_gpu::AsyncTMAGatherOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-reduce-triton-nvidia-gpu-asynctmareduceop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_reduce</span></code> (triton::nvidia_gpu::AsyncTMAReduceOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-scatter-triton-nvidia-gpu-asynctmascatterop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_scatter</span></code> (triton::nvidia_gpu::AsyncTMAScatterOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id9">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-barrier-expect-triton-nvidia-gpu-barrierexpectop"><code class="docutils literal notranslate"><span class="pre">ttng.barrier_expect</span></code> (triton::nvidia_gpu::BarrierExpectOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-cluster-arrive-triton-nvidia-gpu-clusterarriveop"><code class="docutils literal notranslate"><span class="pre">ttng.cluster_arrive</span></code> (triton::nvidia_gpu::ClusterArriveOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id12">Attributes:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-cluster-wait-triton-nvidia-gpu-clusterwaitop"><code class="docutils literal notranslate"><span class="pre">ttng.cluster_wait</span></code> (triton::nvidia_gpu::ClusterWaitOp)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-fence-async-shared-triton-nvidia-gpu-fenceasyncsharedop"><code class="docutils literal notranslate"><span class="pre">ttng.fence_async_shared</span></code> (triton::nvidia_gpu::FenceAsyncSharedOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id13">Attributes:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-init-barrier-triton-nvidia-gpu-initbarrierop"><code class="docutils literal notranslate"><span class="pre">ttng.init_barrier</span></code> (triton::nvidia_gpu::InitBarrierOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id14">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-inval-barrier-triton-nvidia-gpu-invalbarrierop"><code class="docutils literal notranslate"><span class="pre">ttng.inval_barrier</span></code> (triton::nvidia_gpu::InvalBarrierOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id16">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-reinterpret-tensor-descriptor-triton-nvidia-gpu-reinterprettensordescop"><code class="docutils literal notranslate"><span class="pre">ttng.reinterpret_tensor_descriptor</span></code> (triton::nvidia_gpu::ReinterpretTensorDescOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#results">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tc-gen5-commit-triton-nvidia-gpu-tcgen5commitop"><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_commit</span></code> (triton::nvidia_gpu::TCGen5CommitOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id18">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tc-gen5-mma-triton-nvidia-gpu-tcgen5mmaop"><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_mma</span></code> (triton::nvidia_gpu::TCGen5MMAOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id20">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tc-gen5-mma-scaled-triton-nvidia-gpu-tcgen5mmascaledop"><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_mma_scaled</span></code> (triton::nvidia_gpu::TCGen5MMAScaledOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id23">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id24">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id25">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-async-tma-store-wait-triton-nvidia-gpu-tmastorewaitop"><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_store_wait</span></code> (triton::nvidia_gpu::TMAStoreWaitOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id26">Attributes:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tmem-alloc-triton-nvidia-gpu-tmemallocop"><code class="docutils literal notranslate"><span class="pre">ttng.tmem_alloc</span></code> (triton::nvidia_gpu::TMEMAllocOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id27">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id28">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tmem-copy-triton-nvidia-gpu-tmemcopyop"><code class="docutils literal notranslate"><span class="pre">ttng.tmem_copy</span></code> (triton::nvidia_gpu::TMEMCopyOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id29">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tmem-load-triton-nvidia-gpu-tmemloadop"><code class="docutils literal notranslate"><span class="pre">ttng.tmem_load</span></code> (triton::nvidia_gpu::TMEMLoadOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id30">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id31">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tmem-store-triton-nvidia-gpu-tmemstoreop"><code class="docutils literal notranslate"><span class="pre">ttng.tmem_store</span></code> (triton::nvidia_gpu::TMEMStoreOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id32">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id33">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tmem-subslice-triton-nvidia-gpu-tmemsubsliceop"><code class="docutils literal notranslate"><span class="pre">ttng.tmem_subslice</span></code> (triton::nvidia_gpu::TMEMSubSliceOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id34">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id35">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id36">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tensormap-create-triton-nvidia-gpu-tensormapcreateop"><code class="docutils literal notranslate"><span class="pre">ttng.tensormap_create</span></code> (triton::nvidia_gpu::TensormapCreateOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id37">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id38">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-tensormap-fenceproxy-acquire-triton-nvidia-gpu-tensormapfenceproxyacquireop"><code class="docutils literal notranslate"><span class="pre">ttng.tensormap_fenceproxy_acquire</span></code> (triton::nvidia_gpu::TensormapFenceproxyAcquireOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id39">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-wait-barrier-triton-nvidia-gpu-waitbarrierop"><code class="docutils literal notranslate"><span class="pre">ttng.wait_barrier</span></code> (triton::nvidia_gpu::WaitBarrierOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id40">Operands:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-warp-group-dot-triton-nvidia-gpu-warpgroupdotop"><code class="docutils literal notranslate"><span class="pre">ttng.warp_group_dot</span></code> (triton::nvidia_gpu::WarpGroupDotOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id41">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id42">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id43">Results:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ttng-warp-group-dot-wait-triton-nvidia-gpu-warpgroupdotwaitop"><code class="docutils literal notranslate"><span class="pre">ttng.warp_group_dot_wait</span></code> (triton::nvidia_gpu::WarpGroupDotWaitOp)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id44">Attributes:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id45">Operands:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id46">Results:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="NVGPUOps.html">NVGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
<li class="toctree-l2"><a class="reference internal" href="NVWSOps.html">NVWSOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Programming Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/chapter-1/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/chapter-2/related-work.html">Related Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming-guide/chapter-3/debugging.html">Debugging Triton</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Triton</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="dialects.html">Triton MLIR Dialects and Ops</a></li>
      <li class="breadcrumb-item active">TritonNvidiaGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/dialects/TritonNvidiaGPUOps.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tritonnvidiagpuops">
<h1>TritonNvidiaGPUOps<!-- Autogenerated by mlir-tblgen; don't manually edit --><a class="headerlink" href="#tritonnvidiagpuops" title="Link to this heading">¶</a></h1>
<section id="ttng-arrive-barrier-triton-nvidia-gpu-arrivebarrierop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.arrive_barrier</span></code> (triton::nvidia_gpu::ArriveBarrierOp)<a class="headerlink" href="#ttng-arrive-barrier-triton-nvidia-gpu-arrivebarrierop" title="Link to this heading">¶</a></h2>
<p><em>Perform the arrive operation on an mbarrier</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.arrive_barrier` $alloc `,` $count (`,` $pred^)? attr-dict `:` qualified(type($alloc))
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ttng.arrive_barrier</span></code> operation performs the “arrive” operation on an
mbarrier object in shared memory. The operation requires a <code class="docutils literal notranslate"><span class="pre">count</span></code> attribute
of at least 1, and decreasing the pending arrival count of the mbarrier by
the specific count.</p>
<p>The operation accepts an optional predicate.</p>
<p>Example:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>ttng.arrive_barrier %barrier, 2 : !ttg.memdesc&lt;1xi64, #shared, #smem, mutable&gt;
ttng.arrive_barrier %barrier, 1, %pred : !ttg.memdesc&lt;1xi64, #shared, #smem, mutable&gt;
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="attributes">
<h3>Attributes:<a class="headerlink" href="#attributes" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>count</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
<section id="operands">
<h3>Operands:<a class="headerlink" href="#operands" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">alloc</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-copy-mbarrier-arrive-triton-nvidia-gpu-asynccopymbarrierarriveop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_copy_mbarrier_arrive</span></code> (triton::nvidia_gpu::AsyncCopyMbarrierArriveOp)<a class="headerlink" href="#ttng-async-copy-mbarrier-arrive-triton-nvidia-gpu-asynccopymbarrierarriveop" title="Link to this heading">¶</a></h2>
<p><em>Arrive on mbarrier once all previously issued copies are completed</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_copy_mbarrier_arrive` $barrier attr-dict `:` qualified(type($barrier))
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id1">
<h3>Attributes:<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>noIncrement</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr>
</table>
</section>
<section id="id2">
<h3>Operands:<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-copy-global-to-local-triton-nvidia-gpu-asynctmacopyglobaltolocalop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_copy_global_to_local</span></code> (triton::nvidia_gpu::AsyncTMACopyGlobalToLocalOp)<a class="headerlink" href="#ttng-async-tma-copy-global-to-local-triton-nvidia-gpu-asynctmacopyglobaltolocalop" title="Link to this heading">¶</a></h2>
<p><em>Copy data based on descriptor from global memory to local memory asynchronously</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_copy_global_to_local` $desc `[` $coord `]` $result `,` $barrier `,` $pred
              oilist(`cacheModifier` `=` $cache | `evictionPolicy` `=` $evict)
              attr-dict `:` qualified(type($desc)) `,` qualified(type($barrier)) `-&gt;` qualified(type($result))
</pre></div>
</div>
<p>This operation copies data from global memory to local memory
asynchronously.  This is analogue to tt.load except the data are copied to
local memory pointed by the memory descriptor instead of a distributed
tensor. The data copied depends on the global memory descriptor pointed to
by <code class="docutils literal notranslate"><span class="pre">desc</span></code>.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id3">
<h3>Attributes:<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>cache</code></td><td>::mlir::triton::CacheModifierAttr</td><td>allowed 32-bit signless integer cases: 1, 2, 3, 4, 5, 6, 7</td></tr>
<tr><td><code>evict</code></td><td>::mlir::triton::EvictionPolicyAttr</td><td>allowed 32-bit signless integer cases: 1, 2, 3</td></tr>
<tr><td><code>isVolatile</code></td><td>::mlir::BoolAttr</td><td>bool attribute</td></tr>
</table>
</section>
<section id="id4">
<h3>Operands:<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">coord</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-copy-local-to-global-triton-nvidia-gpu-asynctmacopylocaltoglobalop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_copy_local_to_global</span></code> (triton::nvidia_gpu::AsyncTMACopyLocalToGlobalOp)<a class="headerlink" href="#ttng-async-tma-copy-local-to-global-triton-nvidia-gpu-asynctmacopylocaltoglobalop" title="Link to this heading">¶</a></h2>
<p><em>Copy data based on descriptor from local memory to global memory asynchronously</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_copy_local_to_global` $desc `[` $coord `]` $src
              attr-dict `:` qualified(type($desc)) `,` qualified(type($src))
</pre></div>
</div>
<p>This operation copies data from local memory to global memory
asynchronously.  This is analogue to tt.store except the data are copied from
local memory pointed by the memory descriptor instead of a distributed
tensor. The data copied depends on the global memory descriptor pointed to
by <code class="docutils literal notranslate"><span class="pre">desc</span></code>.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id5">
<h3>Operands:<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">coord</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-gather-triton-nvidia-gpu-asynctmagatherop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_gather</span></code> (triton::nvidia_gpu::AsyncTMAGatherOp)<a class="headerlink" href="#ttng-async-tma-gather-triton-nvidia-gpu-asynctmagatherop" title="Link to this heading">¶</a></h2>
<p><em>Gather data based on descriptor from global memory to local memory asynchronously</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_gather` $desc `[` $x_offsets `,` $y_offset `]` $result `,` $barrier `,` $pred
              attr-dict `:` type(operands)
</pre></div>
</div>
<p>This operation gathers multiple rows of data from global memory matrix to
local memory asynchronously.  This is similar to
async_tma_copy_global_to_local except that each row is indexed independently.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id6">
<h3>Operands:<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">x_offsets</span></code></p></td>
<td><p>ranked tensor of 32-bit signless integer values</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">y_offset</span></code></p></td>
<td><p>32-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-reduce-triton-nvidia-gpu-asynctmareduceop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_reduce</span></code> (triton::nvidia_gpu::AsyncTMAReduceOp)<a class="headerlink" href="#ttng-async-tma-reduce-triton-nvidia-gpu-asynctmareduceop" title="Link to this heading">¶</a></h2>
<p><em>Reduce result in gmem based on a TMA descriptor</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_reduce` $kind `,` $desc `[` $coord `]` $src
              attr-dict `:` qualified(type($desc)) `,` qualified(type($src))
</pre></div>
</div>
<p>This operation copies data from local memory to global memory
asynchronously, and atomically performs the specified reduction kind.
Atomicity is at the granularity of individual elements, and only relaxed
semantics are implied.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span> <span class="pre">(MemoryEffectOpInterface)</span></code></p>
<p>Effects: <code class="docutils literal notranslate"><span class="pre">MemoryEffects::Effect{MemoryEffects::Read</span> <span class="pre">on</span> <span class="pre">::mlir::triton::GlobalMemory,</span> <span class="pre">MemoryEffects::Write</span> <span class="pre">on</span> <span class="pre">::mlir::triton::GlobalMemory}</span></code></p>
<section id="id7">
<h3>Attributes:<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>kind</code></td><td>::mlir::triton::DescriptorReduceKindAttr</td><td>allowed 32-bit signless integer cases: 1, 2, 3, 4, 5, 6, 7, 8</td></tr>
</table>
</section>
<section id="id8">
<h3>Operands:<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">coord</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-scatter-triton-nvidia-gpu-asynctmascatterop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_scatter</span></code> (triton::nvidia_gpu::AsyncTMAScatterOp)<a class="headerlink" href="#ttng-async-tma-scatter-triton-nvidia-gpu-asynctmascatterop" title="Link to this heading">¶</a></h2>
<p><em>Scatter data from local memory into global memory based on a descriptor asynchronously</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_scatter` $desc `[` $x_offsets `,` $y_offset `]` $src
              attr-dict `:` type(operands)
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ttng.async_tma_scatter</span></code> operation scatters multiple separately-indexed
rows of data from local memory into global memory asynchronously. The
operation scatters a 2D tensor in shared memory, laid out by core tensor
tiles nvmma_shared layout into separately indexed rows in global
memory at a given <code class="docutils literal notranslate"><span class="pre">y</span></code> offset.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id9">
<h3>Operands:<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">x_offsets</span></code></p></td>
<td><p>ranked tensor of 32-bit signless integer values</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">y_offset</span></code></p></td>
<td><p>32-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-barrier-expect-triton-nvidia-gpu-barrierexpectop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.barrier_expect</span></code> (triton::nvidia_gpu::BarrierExpectOp)<a class="headerlink" href="#ttng-barrier-expect-triton-nvidia-gpu-barrierexpectop" title="Link to this heading">¶</a></h2>
<p><em>Signal a barrier of an expected number of bytes to be copied.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.barrier_expect` $alloc `,` $size attr-dict `,` $pred `:` qualified(type($alloc))
</pre></div>
</div>
<p>This signal the barrier that <code class="docutils literal notranslate"><span class="pre">size</span></code> bytes are expected to be copied. The
associated barrier wait will block until the expected number of bytes are copied.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id10">
<h3>Attributes:<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>size</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
<section id="id11">
<h3>Operands:<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">alloc</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-cluster-arrive-triton-nvidia-gpu-clusterarriveop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.cluster_arrive</span></code> (triton::nvidia_gpu::ClusterArriveOp)<a class="headerlink" href="#ttng-cluster-arrive-triton-nvidia-gpu-clusterarriveop" title="Link to this heading">¶</a></h2>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.cluster_arrive` attr-dict
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id12">
<h3>Attributes:<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>relaxed</code></td><td>::mlir::IntegerAttr</td><td>1-bit signless integer attribute</td></tr>
</table>
</section>
</section>
<section id="ttng-cluster-wait-triton-nvidia-gpu-clusterwaitop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.cluster_wait</span></code> (triton::nvidia_gpu::ClusterWaitOp)<a class="headerlink" href="#ttng-cluster-wait-triton-nvidia-gpu-clusterwaitop" title="Link to this heading">¶</a></h2>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.cluster_wait` attr-dict
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
</section>
<section id="ttng-fence-async-shared-triton-nvidia-gpu-fenceasyncsharedop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.fence_async_shared</span></code> (triton::nvidia_gpu::FenceAsyncSharedOp)<a class="headerlink" href="#ttng-fence-async-shared-triton-nvidia-gpu-fenceasyncsharedop" title="Link to this heading">¶</a></h2>
<p><em>Fence proxy async</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.fence_async_shared` attr-dict
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id13">
<h3>Attributes:<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>bCluster</code></td><td>::mlir::BoolAttr</td><td>bool attribute</td></tr>
</table>
</section>
</section>
<section id="ttng-init-barrier-triton-nvidia-gpu-initbarrierop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.init_barrier</span></code> (triton::nvidia_gpu::InitBarrierOp)<a class="headerlink" href="#ttng-init-barrier-triton-nvidia-gpu-initbarrierop" title="Link to this heading">¶</a></h2>
<p><em>Initialize a barrier in the given shared memory allocation.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.init_barrier` $alloc `,` $count attr-dict `:` qualified(type($alloc))
</pre></div>
</div>
<p>Initializes a shared memory allocation with mbarrier information.
<code class="docutils literal notranslate"><span class="pre">alloc</span></code> is a descriptor to the shared memory allocation. <code class="docutils literal notranslate"><span class="pre">count</span></code> is the
number of arrives expected by the barrier.</p>
<p>This lowers to PTX mbarrier.init.shared::cta.b64.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id14">
<h3>Attributes:<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>count</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
<section id="id15">
<h3>Operands:<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">alloc</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-inval-barrier-triton-nvidia-gpu-invalbarrierop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.inval_barrier</span></code> (triton::nvidia_gpu::InvalBarrierOp)<a class="headerlink" href="#ttng-inval-barrier-triton-nvidia-gpu-invalbarrierop" title="Link to this heading">¶</a></h2>
<p><em>Invalidate a barrier allocation.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.inval_barrier` $alloc attr-dict `:` qualified(type($alloc))
</pre></div>
</div>
<p>Invalidate a barrier allocation so that it can be re-used. According to PTX
spec this has to be done before any reuse of the memory used by mbarrier.</p>
<p>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-inval</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id16">
<h3>Operands:<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">alloc</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-reinterpret-tensor-descriptor-triton-nvidia-gpu-reinterprettensordescop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.reinterpret_tensor_descriptor</span></code> (triton::nvidia_gpu::ReinterpretTensorDescOp)<a class="headerlink" href="#ttng-reinterpret-tensor-descriptor-triton-nvidia-gpu-reinterprettensordescop" title="Link to this heading">¶</a></h2>
<p><em>Reinterpret a pointer as a tensor descriptor</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.reinterpret_tensor_descriptor` $rawDesc attr-dict `:` qualified(type($rawDesc))  `to` qualified(type($result))
</pre></div>
</div>
<p>This Op exists to help the transition from untyped raw TMA objects to typed Tensor descriptor objects.
Ideally, we can remove this once the APIs are fully fleshed out.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AlwaysSpeculatableImplTrait</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">ConditionallySpeculatable</span></code>, <code class="docutils literal notranslate"><span class="pre">NoMemoryEffect</span> <span class="pre">(MemoryEffectOpInterface)</span></code></p>
<p>Effects: <code class="docutils literal notranslate"><span class="pre">MemoryEffects::Effect{}</span></code></p>
<section id="id17">
<h3>Operands:<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">rawDesc</span></code></p></td>
<td><p>ptr</p></td>
</tr>
</tbody>
</table>
</section>
<section id="results">
<h3>Results:<a class="headerlink" href="#results" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>Tensor descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::TensorDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tc-gen5-commit-triton-nvidia-gpu-tcgen5commitop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_commit</span></code> (triton::nvidia_gpu::TCGen5CommitOp)<a class="headerlink" href="#ttng-tc-gen5-commit-triton-nvidia-gpu-tcgen5commitop" title="Link to this heading">¶</a></h2>
<p><em>Make an mbarrier track completion of all prior async tcgen5 ops</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tc_gen5_commit` $barrier (`,` $pred^)? attr-dict `:` qualified(type($barrier))
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_commit</span></code> is an asynchronous operation that makes the
mbarrier object track the completion of all prior asynchronous tcgen5
operations. Upon completion of all asynchronous operations, the mbarrier
arrive operation is performed on the mbarrier with a count of 1.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">two_ctas</span></code> is set, then the mbarrier tracks all prior operations
initiated with <code class="docutils literal notranslate"><span class="pre">two_ctas</span></code> set as well. Otherwise, it tracks all prior
operations initiated without <code class="docutils literal notranslate"><span class="pre">two_ctas</span></code>.</p>
<p>Note that the completion mechanisms are guaranteed to occur sequentially in
the order the commit operations were issued. This means, for example:</p>
<div class="highlight-mlir notranslate"><div class="highlight"><pre><span></span>ttng.tmem_copy
ttng.tc_gen5_mma
ttng.tc_gen5_commit %barrierA
ttng.tc_gen5_commit %barrierB
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">%barrierA</span></code> tracks the completion of the previous TMEM copy and MMA
operations, but since the commit groups are sequential, the arrive-on
operation on <code class="docutils literal notranslate"><span class="pre">%barrierA</span></code> is guaranteed to be performed before the arrive-on
operation on <code class="docutils literal notranslate"><span class="pre">%barrierB</span></code>, even though its commit group is empty.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id18">
<h3>Attributes:<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>two_ctas</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr>
</table>
</section>
<section id="id19">
<h3>Operands:<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tc-gen5-mma-triton-nvidia-gpu-tcgen5mmaop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_mma</span></code> (triton::nvidia_gpu::TCGen5MMAOp)<a class="headerlink" href="#ttng-tc-gen5-mma-triton-nvidia-gpu-tcgen5mmaop" title="Link to this heading">¶</a></h2>
<p><em>Block level op mapping to tensorcore gen5 mma</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tc_gen5_mma` $a `,` $b `,` $d `` custom&lt;Token&gt;($acc_dep, type($token)) `,` $useD`,`
              $pred `` custom&lt;BarriersAndPreds&gt;($barriers, $barrier_preds)
              attr-dict `:` qualified(type($a)) `,` qualified(type($b)) `,`
              qualified(type($d)) (`,` qualified(type($barriers))^)?
</pre></div>
</div>
<p>$d += matrix_multiply($a, $b).
if is_async is false, the op executes synchronously. The barrier operands must not be present in that case.
Otherwise, if a barrier is given, the op will trigger a commit/arrive on it. The result will be safe to read after a barrier wait.
If $two_ctas is set the op will execute a matmul across two contiguous CTAs, it will read the data distributed across the two CTAs.
and syncronize both CTAs if the op is synchronous.</p>
<p>This operation takes and produces an optional token to indicate TMEM read
and write on its accumulator operand. When the tokens are present, they can
be used to check aliasing and modref on the accumulator memory.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AttrSizedOperandSegments</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">DotOpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">MMAv5OpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span></code></p>
<section id="id20">
<h3>Attributes:<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>is_async</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr>
<tr><td><code>two_ctas</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr>
</table>
</section>
<section id="id21">
<h3>Operands:<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">b</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">d</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">acc_dep</span></code></p></td>
<td><p>async token type</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">useD</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barriers</span></code></p></td>
<td><p>variadic of memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier_preds</span></code></p></td>
<td><p>variadic of 1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id22">
<h3>Results:<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">token</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tc-gen5-mma-scaled-triton-nvidia-gpu-tcgen5mmascaledop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tc_gen5_mma_scaled</span></code> (triton::nvidia_gpu::TCGen5MMAScaledOp)<a class="headerlink" href="#ttng-tc-gen5-mma-scaled-triton-nvidia-gpu-tcgen5mmascaledop" title="Link to this heading">¶</a></h2>
<p><em>Block level op mapping to tensorcore gen5 mma</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tc_gen5_mma_scaled` $a `,` $b `,` $d `` custom&lt;Token&gt;($acc_dep, type($token)) `,` $a_scale `,`
              $b_scale `,` $useD `,` $pred `lhs` `=` $a_type `rhs` `=` $b_type
              `` custom&lt;BarriersAndPreds&gt;($barriers, $barrier_preds)
              attr-dict `:` qualified(type($a)) `,` qualified(type($b)) `,`
              qualified(type($d)) `,` qualified(type($a_scale)) `,`
              qualified(type($b_scale)) (`,` qualified(type($barriers))^)?
</pre></div>
</div>
<p>$d += matrix_multiply(scale($lhs, $lhs_scale), scale(rlhs, $rhs_scale))
if is_async is false, the op executes synchronously. The barrier operands must not be present in that case.
Otherwise, if a barrier is given, the op will trigger a commit/arrive on it.
The result will be safe to read after a barrier wait.</p>
<p>This operation takes and produces an optional token to indicate TMEM read
and write on its accumulator operand. When the tokens are present, they can
be used to check aliasing and modref on the accumulator memory.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AttrSizedOperandSegments</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">DotOpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">MMAv5OpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span></code></p>
<section id="id23">
<h3>Attributes:<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>a_type</code></td><td>::mlir::triton::ScaleDotElemTypeAttr</td><td>allowed 32-bit signless integer cases: 0, 1, 2, 3, 4, 5, 6</td></tr>
<tr><td><code>b_type</code></td><td>::mlir::triton::ScaleDotElemTypeAttr</td><td>allowed 32-bit signless integer cases: 0, 1, 2, 3, 4, 5, 6</td></tr>
<tr><td><code>is_async</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr>
</table>
</section>
<section id="id24">
<h3>Operands:<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">b</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">d</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">acc_dep</span></code></p></td>
<td><p>async token type</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">a_scale</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">b_scale</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">useD</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barriers</span></code></p></td>
<td><p>variadic of memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier_preds</span></code></p></td>
<td><p>variadic of 1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id25">
<h3>Results:<a class="headerlink" href="#id25" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">token</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-async-tma-store-wait-triton-nvidia-gpu-tmastorewaitop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.async_tma_store_wait</span></code> (triton::nvidia_gpu::TMAStoreWaitOp)<a class="headerlink" href="#ttng-async-tma-store-wait-triton-nvidia-gpu-tmastorewaitop" title="Link to this heading">¶</a></h2>
<p><em>Wait until all the inputs are read.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.async_tma_store_wait` attr-dict
</pre></div>
</div>
<p>Wait until all the read operations are done from the associated store operations.
This is needed before the shared memory can be written to.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id26">
<h3>Attributes:<a class="headerlink" href="#id26" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>pendings</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
</section>
<section id="ttng-tmem-alloc-triton-nvidia-gpu-tmemallocop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tmem_alloc</span></code> (triton::nvidia_gpu::TMEMAllocOp)<a class="headerlink" href="#ttng-tmem-alloc-triton-nvidia-gpu-tmemallocop" title="Link to this heading">¶</a></h2>
<p><em>Allocate tensor memory</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tmem_alloc` ($src^)? attr-dict `:` functional-type(operands, results)
</pre></div>
</div>
<p>This operation allocates buffer in tensor memory and return a descriptor
containing the address and a view of the buffer.
This is similar to ttg.local_alloc except the buffer is allocated in tensor memory.</p>
<p>Explicitly deallocating a buffer is optional; see local_dealloc.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span></code></p>
<section id="id27">
<h3>Operands:<a class="headerlink" href="#id27" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>ranked tensor of floating-point or integer or ptr values</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id28">
<h3>Results:<a class="headerlink" href="#id28" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">token</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tmem-copy-triton-nvidia-gpu-tmemcopyop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tmem_copy</span></code> (triton::nvidia_gpu::TMEMCopyOp)<a class="headerlink" href="#ttng-tmem-copy-triton-nvidia-gpu-tmemcopyop" title="Link to this heading">¶</a></h2>
<p><em>Initiate an asynchronous copy operation from shared memory to the Tensor Memory.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tmem_copy` $src `,` $dst (`,` $barrier^)? attr-dict `:` qualified(type(operands))
</pre></div>
</div>
<p>2D blocks stored contiguously in SMEM are copied into TMEM as specified by the destination address.
The completion of the copy can be observed by waiting on the optional barrier. If this op is used
together with an MMA op, one barrier can be used to wait for both copy and MMA. We do not need to wait
for the completion of the copy before MMA, since tcgen05.cp followed by tcgen05.mma is guaranteed to
execute in that order.</p>
<p>This op lowers to the PTX instruction tcgen05.cp. This supports writing either to scales tmem layout as well as default tmem layout.
Currently the semantic is different when writing to tmem scale layout.</p>
<p>In case of default layout the copy doesn’t change the logical elements between the source and destination memdesc.</p>
<p>In case of scale layout:
Each 32x128b block in SMEM is duplicated over 4 warps and stored into 128 rows
and 4 columns of TMEM. The primary use case of this op is to copy blocked scales from SMEM to TMEM.</p>
<p>The shape of the input SMEM can be flexibily chosen depending on use cases. In the simplest case (e.g. unit test),
the source SMEM can be of shape (32 x num_blocks, 16), and the destination TMEM should be of shape (128, 16 x num_blocks),
for copying 8 bit values. For scaled GEMM, rep_m x rep_k copies of a 32x128b block need to be stored in SMEM, where
rep_m = BLOCK_M / 128, rep_k = BLOCK_K / scale_vec_size / 4, and scale_vec_size = 32 for MXFP.
Conceptually, the SMEM is organized in a high-dimensional layout, (rep_m, rep_k, 32, 4, 4B).
Some of axes can be flattened into one, to reduce the rank of the load. For example, the following patterns are supported:</p>
<ul class="simple">
<li><p>(rep_m, rep_k * 32 x 4 x 4B), 2D scale load with cp.async</p></li>
<li><p>(rep_m, rep_k, 32, 16B), 4D scale load with TMA</p></li>
<li><p>(rep_m, rep_k, 32, 4, 4B), 5D scale load with cp.async
Since rep_m blocks are not contiguous in SMEM, this axis cannot be flattened into inner ones.</p></li>
</ul>
<p>In Triton, the TMEM memdesc for blocked scales must be of the following form:</p>
<ul class="simple">
<li><p>Its shape must be (BLOCK_MN, BLOCK_K / scale_vec_size), representing the logical shape of blocked scales.</p></li>
<li><p>It must be attached with <code class="docutils literal notranslate"><span class="pre">tensor_memory_scales_encoding</span></code> to indicate the chunk-based layout and its duplication over 4 warps.</p></li>
</ul>
<p>In contrast, the src SMEM must be in the explicit chunk-based layout as described above. So the IR might look like this:</p>
<p>ttng.tmem_copy %1, %0 : (!ttg.memdesc&lt;1x1x32x4x4xi8, #shared1, #smem&gt;, !ttg.memdesc&lt;128x4xi8, #tmem_scales, #ttng.tensor_memory&gt;) -&gt; ()</p>
<p>We interpret the semantics of this copy operation as follows. The chunk-based layout in SMEM implies that
the logical shape (BLOCK_MN, BLOCK_K / scale_vec_size) in TMEM is the result of certain reshape and transpose operations.
In practice, to take an advantage of the native scale layout and the TMEM copy op,  users need to do
<code class="docutils literal notranslate"><span class="pre">scales5D.trans(0,</span> <span class="pre">3,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">4).reshape(BLOCK_M,</span> <span class="pre">BLOCK_K</span> <span class="pre">//</span> <span class="pre">scale_vec_size)</span></code> before feeding scales into dot_scaled.
When we use tmem_copy in the IR, such reshape and transpose operations are removed. But the change in the logical shape they have caused on
registers is now understood to be incorporated into tmem_copy itself. Ideally, we would lift reshape / transpose done on registers onto
the SMEM memdesc, making tmem_copy a straightforward 2D copy operation: (BLOCK_MN, BLOCK_K / scale_vec_size) -&gt; (BLOCK_MN, BLOCK_K / scale_vec_size).
In the absence of such operations on memdesc, we resort to implicitly encoding the reshape/transpose semantics in tmem_copy.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id29">
<h3>Operands:<a class="headerlink" href="#id29" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">dst</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">barrier</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tmem-load-triton-nvidia-gpu-tmemloadop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tmem_load</span></code> (triton::nvidia_gpu::TMEMLoadOp)<a class="headerlink" href="#ttng-tmem-load-triton-nvidia-gpu-tmemloadop" title="Link to this heading">¶</a></h2>
<p><em>Load a buffer from tensor memory into a distributed tensor</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tmem_load` $src `` custom&lt;Token&gt;($dep, type($token))
              attr-dict `:` qualified(type($src)) `-&gt;` type($result)
</pre></div>
</div>
<p>This is similar to ttg.local_load except the result layout is restricted to only few possibility.
Therefore we cannot combine this op with any convert layout like local_load.</p>
<p>This operation takes and produces an optional token to indicate TMEM read
on its source operand. When the tokens are present, they can
be used to check aliasing and modref on the TMEM buffer.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id30">
<h3>Operands:<a class="headerlink" href="#id30" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">dep</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id31">
<h3>Results:<a class="headerlink" href="#id31" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>ranked tensor of floating-point or integer or ptr values</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">token</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tmem-store-triton-nvidia-gpu-tmemstoreop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tmem_store</span></code> (triton::nvidia_gpu::TMEMStoreOp)<a class="headerlink" href="#ttng-tmem-store-triton-nvidia-gpu-tmemstoreop" title="Link to this heading">¶</a></h2>
<p><em>Store a distributed tensor into a buffer in tensor memory</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tmem_store` $src `,` $dst `` custom&lt;Token&gt;($dep, type($token)) `,` $pred
              attr-dict `:` type($src) `-&gt;` qualified(type($dst))
</pre></div>
</div>
<p>This is similar to ttg.local_store except the source layout is restricted to only few possibility.</p>
<p>This operation takes and produces an optional token to indicate TMEM write
on its source operand. When the tokens are present, they can
be used to check aliasing and modref on the TMEM buffer.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id32">
<h3>Operands:<a class="headerlink" href="#id32" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">dst</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">dep</span></code></p></td>
<td><p>async token type</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>ranked tensor of floating-point or integer or ptr values</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id33">
<h3>Results:<a class="headerlink" href="#id33" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">token</span></code></p></td>
<td><p>async token type</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tmem-subslice-triton-nvidia-gpu-tmemsubsliceop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tmem_subslice</span></code> (triton::nvidia_gpu::TMEMSubSliceOp)<a class="headerlink" href="#ttng-tmem-subslice-triton-nvidia-gpu-tmemsubsliceop" title="Link to this heading">¶</a></h2>
<p><em>Take a subslice of a tensor memory allocation</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tmem_subslice` $src attr-dict `:` qualified(type($src)) `-&gt;` qualified(type($result))
</pre></div>
</div>
<p>This operation takes a subslice of a tensor memory allocation and returns a new descriptor
containing the address and a view of the subslice.
This is similar to ttg.memdesc_subslice except we can only slice along the inner dimension
of a 2D memdesc as this is the only one we can do for TMem.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AlwaysSpeculatableImplTrait</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">ConditionallySpeculatable</span></code>, <code class="docutils literal notranslate"><span class="pre">NoMemoryEffect</span> <span class="pre">(MemoryEffectOpInterface)</span></code></p>
<p>Effects: <code class="docutils literal notranslate"><span class="pre">MemoryEffects::Effect{}</span></code></p>
<section id="id34">
<h3>Attributes:<a class="headerlink" href="#id34" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>N</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
<section id="id35">
<h3>Operands:<a class="headerlink" href="#id35" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">src</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id36">
<h3>Results:<a class="headerlink" href="#id36" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">result</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tensormap-create-triton-nvidia-gpu-tensormapcreateop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tensormap_create</span></code> (triton::nvidia_gpu::TensormapCreateOp)<a class="headerlink" href="#ttng-tensormap-create-triton-nvidia-gpu-tensormapcreateop" title="Link to this heading">¶</a></h2>
<p><em>Create a new TMA descriptor on device</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tensormap_create` $desc_ptr `,` $global_address `,`
              `[` $box_dim `]` `,`
              `[` $global_dim `]` `,`
              `[` $global_stride `]` `,`
              `[` $element_stride `]`
              attr-dict `:` functional-type(operands, results)
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AttrSizedOperandSegments</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span> <span class="pre">(MemoryEffectOpInterface)</span></code></p>
<p>Effects: <code class="docutils literal notranslate"><span class="pre">MemoryEffects::Effect{MemoryEffects::Read</span> <span class="pre">on</span> <span class="pre">::mlir::triton::GlobalMemory,</span> <span class="pre">MemoryEffects::Write</span> <span class="pre">on</span> <span class="pre">::mlir::triton::GlobalMemory}</span></code></p>
<section id="id37">
<h3>Attributes:<a class="headerlink" href="#id37" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>elem_type</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute whose value is non-negative whose maximum value is 15</td></tr>
<tr><td><code>interleave_layout</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute whose value is non-negative whose maximum value is 2</td></tr>
<tr><td><code>swizzle_mode</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute whose value is non-negative whose maximum value is 3</td></tr>
<tr><td><code>fill_mode</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute whose value is non-negative whose maximum value is 1</td></tr>
</table>
</section>
<section id="id38">
<h3>Operands:<a class="headerlink" href="#id38" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc_ptr</span></code></p></td>
<td><p>Pointer type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::PointerType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">global_address</span></code></p></td>
<td><p>Pointer type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::PointerType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">box_dim</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">global_dim</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">global_stride</span></code></p></td>
<td><p>variadic of 64-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">element_stride</span></code></p></td>
<td><p>variadic of 32-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-tensormap-fenceproxy-acquire-triton-nvidia-gpu-tensormapfenceproxyacquireop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.tensormap_fenceproxy_acquire</span></code> (triton::nvidia_gpu::TensormapFenceproxyAcquireOp)<a class="headerlink" href="#ttng-tensormap-fenceproxy-acquire-triton-nvidia-gpu-tensormapfenceproxyacquireop" title="Link to this heading">¶</a></h2>
<p><em>Acquire fence on a tensormap object</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.tensormap_fenceproxy_acquire` $desc_ptr attr-dict `:` qualified(type($desc_ptr))
</pre></div>
</div>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span> <span class="pre">(MemoryEffectOpInterface)</span></code></p>
<p>Effects: <code class="docutils literal notranslate"><span class="pre">MemoryEffects::Effect{MemoryEffects::Write</span> <span class="pre">on</span> <span class="pre">::mlir::triton::GlobalMemory}</span></code></p>
<section id="id39">
<h3>Operands:<a class="headerlink" href="#id39" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">desc_ptr</span></code></p></td>
<td><p>Pointer type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::PointerType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-wait-barrier-triton-nvidia-gpu-waitbarrierop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.wait_barrier</span></code> (triton::nvidia_gpu::WaitBarrierOp)<a class="headerlink" href="#ttng-wait-barrier-triton-nvidia-gpu-waitbarrierop" title="Link to this heading">¶</a></h2>
<p><em>Wait until the mbarrier phase completes.</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.wait_barrier` $alloc `,` $phase (`,` $pred^)? (`deps` $deps^)?
              attr-dict `:` qualified(type($alloc)) (`,` type($deps)^)?
</pre></div>
</div>
<p>Blocks the program progress until the mbarrier object in <code class="docutils literal notranslate"><span class="pre">alloc</span></code> completes
its current phase.</p>
<p>This lowers a waitloop using PTX instruction
mbarrier.try_wait.parity.shared.b64.</p>
<p>Accepts optional list of memory. If present, it is assumed that any of the
dependencies may be accessed until the barrier completes.</p>
<p>The barrier behavior is described here:
https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">AttrSizedOperandSegments</span></code>, <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<section id="id40">
<h3>Operands:<a class="headerlink" href="#id40" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">alloc</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">phase</span></code></p></td>
<td><p>32-bit signless integer</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">pred</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">deps</span></code></p></td>
<td><p>variadic of memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-warp-group-dot-triton-nvidia-gpu-warpgroupdotop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.warp_group_dot</span></code> (triton::nvidia_gpu::WarpGroupDotOp)<a class="headerlink" href="#ttng-warp-group-dot-triton-nvidia-gpu-warpgroupdotop" title="Link to this heading">¶</a></h2>
<p><em>Warp group dot</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.warp_group_dot` $a`,` $b`,` $c (`,` $useC^)? attr-dict
              `:` type($a) `*` qualified(type($b)) `-&gt;` type($d)
</pre></div>
</div>
<p>$d = matrix_multiply($a, $b) + $c. For docs on InputPrecisionAttr, see TT_DotOp</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">DotOpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">InferTypeOpInterface</span></code>, <code class="docutils literal notranslate"><span class="pre">MemoryEffectOpInterface</span></code></p>
<section id="id41">
<h3>Attributes:<a class="headerlink" href="#id41" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>inputPrecision</code></td><td>::mlir::triton::InputPrecisionAttr</td><td>allowed 32-bit signless integer cases: 0, 1, 2, 3, 4</td></tr>
<tr><td><code>maxNumImpreciseAcc</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
<tr><td><code>isAsync</code></td><td>::mlir::BoolAttr</td><td>bool attribute</td></tr>
</table>
</section>
<section id="id42">
<h3>Operands:<a class="headerlink" href="#id42" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p></td>
<td><p>TensorOrMemDesc instance</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">b</span></code></p></td>
<td><p>memory descriptor type (<code class="docutils literal notranslate"><span class="pre">::mlir::triton::gpu::MemDescType</span></code>) in Triton IR type system</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">c</span></code></p></td>
<td><p>ranked tensor of floating-point or integer values</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">useC</span></code></p></td>
<td><p>1-bit signless integer</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id43">
<h3>Results:<a class="headerlink" href="#id43" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">d</span></code></p></td>
<td><p>ranked tensor of floating-point or integer values</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="ttng-warp-group-dot-wait-triton-nvidia-gpu-warpgroupdotwaitop">
<h2><code class="docutils literal notranslate"><span class="pre">ttng.warp_group_dot_wait</span></code> (triton::nvidia_gpu::WarpGroupDotWaitOp)<a class="headerlink" href="#ttng-warp-group-dot-wait-triton-nvidia-gpu-warpgroupdotwaitop" title="Link to this heading">¶</a></h2>
<p><em>Warp group dot wait</em></p>
<p>Syntax:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>operation ::= `ttng.warp_group_dot_wait` $inputs attr-dict `:` type($inputs)
</pre></div>
</div>
<p>Waits until there are $pendings or fewer outstanding async dot operations.</p>
<p>$inputs must be the tensors corresponding to the async dot ops that we’re
waiting on.  For example, if there are N pending async dot ops and we call
<code class="docutils literal notranslate"><span class="pre">warp_group_dot_wait</span> <span class="pre">1</span></code>, then $inputs must be the result of the first dot op.</p>
<p>Traits: <code class="docutils literal notranslate"><span class="pre">VerifyTensorLayoutsTrait</span></code></p>
<p>Interfaces: <code class="docutils literal notranslate"><span class="pre">InferTypeOpInterface</span></code></p>
<section id="id44">
<h3>Attributes:<a class="headerlink" href="#id44" title="Link to this heading">¶</a></h3>
<table>
<tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr>
<tr><td><code>pendings</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr>
</table>
</section>
<section id="id45">
<h3>Operands:<a class="headerlink" href="#id45" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Operand</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">inputs</span></code></p></td>
<td><p>variadic of TensorOrMemDesc instance</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id46">
<h3>Results:<a class="headerlink" href="#id46" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Result</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><code class="docutils literal notranslate"><span class="pre">outputs</span></code></p></td>
<td><p>variadic of TensorOrMemDesc instance</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="TritonOps.html" class="btn btn-neutral float-left" title="TritonOps" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="NVGPUOps.html" class="btn btn-neutral float-right" title="NVGPUOps" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Philippe Tillet.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
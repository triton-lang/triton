{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Persistent Matmul\nThis script demonstrates persistent kernel implementations of matrix multiplication using Triton.\nVarious matmul methods are included, such as naive, persistent, and TMA (Tensor Memory Accelerator) based approaches.\nThe kernels support both FP16 and FP8 data types but the FP8 implementation is only available on CUDA devices with compute capability >= 9.0.\n\nTriton and cuBLAS implementations are benchmarked under different configurations and evaluated using the proton profiler.\nUsers can pass command-line arguments to specify matrix dimensions and iteration steps flexibly.\n\n```bash\n# FP8\npython 09-persistent-matmul.py --prec fp8 --K_range 128 1024 --K_step 128\n\n# FP16\npython 09-persistent-matmul.py --prec fp16 --K_range 128 1024 --K_step 128\n```\nNote that currently this tutorial will fail on devices with a small shared memory size, such as RTX-4090.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport itertools\n\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.profiler as proton\nfrom triton.tools.tensor_descriptor import TensorDescriptor\nfrom contextlib import contextmanager\n\nfrom typing import Optional\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip():\n    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n\n\nif is_cuda():\n    from triton._C.libtriton import nvidia\n    device_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n    device_blas = nvidia.cublas.CublasLt(device_workspace)\nelif is_hip():\n    from triton._C.libtriton import amd\n    device_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n    device_blas = amd.hipblas.HipblasLt(device_workspace)\nelse:\n    device_blas = None\n\n\ndef device_blas_name():\n    return 'cuBLAS' if is_cuda() else 'hipBLAS'\n\n\ndef supports_tma():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef is_hopper():\n    return torch.cuda.get_device_capability()[0] == 9\n\n\ndef supports_ws():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K, WS = args[\"M\"], args[\"N\"], args[\"K\"], args.get(\"WARP_SPECIALIZE\", False)\n    ws_str = \"_ws\" if WS else \"\"\n    ret[\"name\"] = f\"{kernel.name}{ws_str} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\nHAS_TENSOR_DESC = supports_tma() and hasattr(tl, \"make_tensor_descriptor\")\nHAS_HOST_TENSOR_DESC = supports_tma() and hasattr(triton.tools.tensor_descriptor, \"TensorDescriptor\")\nHAS_WARP_SPECIALIZE = supports_ws() and HAS_TENSOR_DESC\n\n\ndef matmul_get_configs(pre_hook=None):\n    return [\n        triton.Config({'BLOCK_SIZE_M': BM, 'BLOCK_SIZE_N': BN, \"BLOCK_SIZE_K\": BK, \"GROUP_SIZE_M\": 8}, num_stages=s,\n                      num_warps=w, pre_hook=pre_hook)\n        for BM in [128]\n        for BN in [128, 256]\n        for BK in [64, 128]\n        for s in ([2, 3, 4])\n        for w in [4, 8]\n    ]\n\n\n@triton.autotune(\n    configs=matmul_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                  M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  BLOCK_SIZE_M: tl.constexpr,  #\n                  BLOCK_SIZE_N: tl.constexpr,  #\n                  BLOCK_SIZE_K: tl.constexpr,  #\n                  GROUP_SIZE_M: tl.constexpr,  #\n                  ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if (c_ptr.dtype.element_ty == tl.float8e4nv):\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n    matmul_kernel[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n    )\n    return c\n\n\ndef matmul_tma_set_block_size_hook(nargs):\n    EPILOGUE_SUBTILE = nargs.get(\"EPILOGUE_SUBTILE\", False)\n    BLOCK_M = nargs[\"BLOCK_SIZE_M\"]\n    BLOCK_N = nargs[\"BLOCK_SIZE_N\"]\n    BLOCK_K = nargs[\"BLOCK_SIZE_K\"]\n    nargs[\"a_desc\"].block_shape = [BLOCK_M, BLOCK_K]\n    nargs[\"b_desc\"].block_shape = [BLOCK_N, BLOCK_K]\n    if EPILOGUE_SUBTILE:\n        nargs[\"c_desc\"].block_shape = [BLOCK_M, BLOCK_N // 2]\n    else:\n        nargs[\"c_desc\"].block_shape = [BLOCK_M, BLOCK_N]\n\n\n@triton.autotune(\n    configs=matmul_get_configs(pre_hook=matmul_tma_set_block_size_hook),\n    key=[\"M\", \"N\", \"K\", \"WARP_SPECIALIZE\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_tma(a_desc, b_desc, c_desc,  #\n                      M, N, K,  #\n                      BLOCK_SIZE_M: tl.constexpr,  #\n                      BLOCK_SIZE_N: tl.constexpr,  #\n                      BLOCK_SIZE_K: tl.constexpr,  #\n                      GROUP_SIZE_M: tl.constexpr,  #\n                      FP8_OUTPUT: tl.constexpr,  #\n                      WARP_SPECIALIZE: tl.constexpr,  #\n                      ):\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n\n    offs_am = pid_m * BLOCK_SIZE_M\n    offs_bn = pid_n * BLOCK_SIZE_N\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in tl.range(k_tiles, warp_specialize=WARP_SPECIALIZE):\n        offs_k = k * BLOCK_SIZE_K\n        a = a_desc.load([offs_am, offs_k])\n        b = b_desc.load([offs_bn, offs_k])\n        accumulator = tl.dot(a, b.T, accumulator)\n\n    c = accumulator.to(dtype)\n\n    offs_cm = pid_m * BLOCK_SIZE_M\n    offs_cn = pid_n * BLOCK_SIZE_N\n    c_desc.store([offs_cm, offs_cn], c)\n\n\ndef matmul_tma(a, b, warp_specialize: bool):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n\n    # A dummy block value that will be overwritten when we have the real block size\n    dummy_block = [1, 1]\n    a_desc = TensorDescriptor.from_tensor(a, dummy_block)\n    b_desc = TensorDescriptor.from_tensor(b, dummy_block)\n    c_desc = TensorDescriptor.from_tensor(c, dummy_block)\n\n    def grid(META):\n        BLOCK_M = META[\"BLOCK_SIZE_M\"]\n        BLOCK_N = META[\"BLOCK_SIZE_N\"]\n        return (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), )\n\n    matmul_kernel_tma[grid](\n        a_desc, b_desc, c_desc,  #\n        M, N, K,  #\n        FP8_OUTPUT=dtype == torch.float8_e4m3fn,  #\n        WARP_SPECIALIZE=warp_specialize,  #\n    )\n    return c\n\n\n@triton.jit\ndef _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS):\n    group_id = tile_id // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (tile_id % group_size_m)\n    pid_n = (tile_id % num_pid_in_group) // group_size_m\n    return pid_m, pid_n\n\n\n@triton.autotune(\n    configs=matmul_get_configs(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr,  #\n                             M, N, K,  #\n                             stride_am, stride_ak,  #\n                             stride_bk, stride_bn,  #\n                             stride_cm, stride_cn,  #\n                             BLOCK_SIZE_M: tl.constexpr,  #\n                             BLOCK_SIZE_N: tl.constexpr,  #\n                             BLOCK_SIZE_K: tl.constexpr,  #\n                             GROUP_SIZE_M: tl.constexpr,  #\n                             NUM_SMS: tl.constexpr,  #\n                             ):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    # NOTE: There is currently a bug in blackwell pipelining that means it can't handle a value being\n    # used in both the prologue and epilogue, so we duplicate the counters as a work-around.\n    tile_id_c = start_pid - NUM_SMS\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):\n        pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        start_m = pid_m * BLOCK_SIZE_M\n        start_n = pid_n * BLOCK_SIZE_N\n        offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n        offs_am = tl.where(offs_am < M, offs_am, 0)\n        offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n        offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n        offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for ki in range(k_tiles):\n            offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n            a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n            b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n            a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n            b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n            accumulator = tl.dot(a, b, accumulator)\n\n        tile_id_c += NUM_SMS\n        pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n        if (c_ptr.dtype.element_ty == tl.float8e4nv):\n            c = accumulator.to(tl.float8e4nv)\n        else:\n            c = accumulator.to(tl.float16)\n        tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_persistent(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n    # Allocates output.\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        NUM_SMS=NUM_SMS,  #\n    )\n    return c\n\n\ndef matmul_tma_persistent_get_configs(pre_hook=None):\n    return [\n        triton.Config(\n            {\n                'BLOCK_SIZE_M': BM, 'BLOCK_SIZE_N': BN, \"BLOCK_SIZE_K\": BK, \"GROUP_SIZE_M\": 8, \"EPILOGUE_SUBTILE\":\n                SUBTILE\n            }, num_stages=s, num_warps=w, pre_hook=pre_hook)  #\n        for BM in [128]  #\n        for BN in [128, 256]  #\n        for BK in [64, 128]  #\n        for s in ([2, 3, 4])  #\n        for w in [4, 8]  #\n        for SUBTILE in [True, False]  #\n    ]\n\n\n@triton.autotune(\n    configs=matmul_tma_persistent_get_configs(pre_hook=matmul_tma_set_block_size_hook),\n    key=[\"M\", \"N\", \"K\", \"WARP_SPECIALIZE\"],\n)\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_tma_persistent(a_desc, b_desc, c_desc,  #\n                                 M, N, K,  #\n                                 BLOCK_SIZE_M: tl.constexpr,  #\n                                 BLOCK_SIZE_N: tl.constexpr,  #\n                                 BLOCK_SIZE_K: tl.constexpr,  #\n                                 GROUP_SIZE_M: tl.constexpr,  #\n                                 FP8_OUTPUT: tl.constexpr,  #\n                                 EPILOGUE_SUBTILE: tl.constexpr,  #\n                                 NUM_SMS: tl.constexpr,  #\n                                 WARP_SPECIALIZE: tl.constexpr,  #\n                                 ):\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tile_id_c = start_pid - NUM_SMS\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    # Enable warp specialization to leverage async warp scheduling in the GPU.\n    # FIXME: This only works on Blackwell right now. On older GPUs, this will\n    # use software pipelining.\n    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True, warp_specialize=WARP_SPECIALIZE):\n        pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for ki in range(k_tiles):\n            offs_k = ki * BLOCK_SIZE_K\n            a = a_desc.load([offs_am, offs_k])\n            b = b_desc.load([offs_bn, offs_k])\n            accumulator = tl.dot(a, b.T, accumulator)\n\n        tile_id_c += NUM_SMS\n        pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        offs_am_c = pid_m * BLOCK_SIZE_M\n        offs_bn_c = pid_n * BLOCK_SIZE_N\n\n        # Epilogue subtiling is a technique to break our computation and stores into multiple pieces\n        # By subtiling we can reduce shared memory consumption by the epilogue and instead use that\n        # memory to increase our stage count.\n        # In this case we partition the accumulator into 2 BLOCK_SIZE_M x BLOCK_SIZE_N // 2 tensors\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_am_c, offs_bn_c], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)\n        else:\n            accumulator = accumulator.to(dtype)\n            c_desc.store([offs_am_c, offs_bn_c], accumulator)\n\n\ndef matmul_tma_persistent(a, b, warp_specialize: bool):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n\n    # A dummy block value that will be overwritten when we have the real block size\n    dummy_block = [1, 1]\n    a_desc = TensorDescriptor.from_tensor(a, dummy_block)\n    b_desc = TensorDescriptor.from_tensor(b, dummy_block)\n    c_desc = TensorDescriptor.from_tensor(c, dummy_block)\n\n    def grid(META):\n        nonlocal a_desc, b_desc, c_desc\n        BLOCK_M = META[\"BLOCK_SIZE_M\"]\n        BLOCK_N = META[\"BLOCK_SIZE_N\"]\n        return (min(\n            NUM_SMS,\n            triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),\n        ), )\n\n    matmul_kernel_tma_persistent[grid](\n        a_desc, b_desc, c_desc,  #\n        M, N, K,  #\n        FP8_OUTPUT=dtype == torch.float8_e4m3fn,  #\n        NUM_SMS=NUM_SMS,  #\n        WARP_SPECIALIZE=warp_specialize,  #\n    )\n    return c\n\n\ndef prune_invalid_configs(configs, named_args, **kwargs):\n    FLATTEN = kwargs[\"FLATTEN\"]\n    # Filter out configs where EPILOGUE_SUBTILE is true and HOPPER is true\n    return [conf for conf in configs if not (conf.kwargs.get(\"EPILOGUE_SUBTILE\", True) and FLATTEN is False)]\n\n\n@triton.autotune(configs=matmul_tma_persistent_get_configs(), key=[\"M\", \"N\", \"K\", \"WARP_SPECIALIZE\", \"FLATTEN\"],\n                 prune_configs_by={'early_config_prune': prune_invalid_configs})\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_descriptor_persistent(\n    a_ptr,\n    b_ptr,\n    c_ptr,  #\n    M,\n    N,\n    K,  #\n    BLOCK_SIZE_M: tl.constexpr,  #\n    BLOCK_SIZE_N: tl.constexpr,  #\n    BLOCK_SIZE_K: tl.constexpr,  #\n    GROUP_SIZE_M: tl.constexpr,  #\n    EPILOGUE_SUBTILE: tl.constexpr,  #\n    NUM_SMS: tl.constexpr,  #\n    WARP_SPECIALIZE: tl.constexpr,  #\n    FLATTEN: tl.constexpr,\n):\n    # Matmul using TMA and device-side descriptor creation\n    dtype = c_ptr.dtype.element_ty\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    a_desc = tl.make_tensor_descriptor(\n        a_ptr,\n        shape=[M, K],\n        strides=[K, 1],\n        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],\n    )\n    b_desc = tl.make_tensor_descriptor(\n        b_ptr,\n        shape=[N, K],\n        strides=[K, 1],\n        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],\n    )\n    c_desc = tl.make_tensor_descriptor(\n        c_ptr,\n        shape=[M, N],\n        strides=[N, 1],\n        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2],\n    )\n\n    # tile_id_c is used in the epilogue to break the dependency between\n    # the prologue and the epilogue\n    tile_id_c = start_pid - NUM_SMS\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=FLATTEN, warp_specialize=WARP_SPECIALIZE):\n        pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for ki in range(k_tiles):\n            offs_k = ki * BLOCK_SIZE_K\n            a = a_desc.load([offs_am, offs_k])\n            b = b_desc.load([offs_bn, offs_k])\n            accumulator = tl.dot(a, b.T, accumulator)\n\n        tile_id_c += NUM_SMS\n        pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n        offs_cm = pid_m * BLOCK_SIZE_M\n        offs_cn = pid_n * BLOCK_SIZE_N\n\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_cm, offs_cn], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_cm, offs_cn + BLOCK_SIZE_N // 2], c1)\n        else:\n            c = accumulator.to(dtype)\n            c_desc.store([offs_cm, offs_cn], c)\n\n\ndef matmul_descriptor_persistent(a, b, warp_specialize: bool):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n\n    # TMA descriptors require a global memory allocation\n    def alloc_fn(size: int, alignment: int, stream: Optional[int]):\n        return torch.empty(size, device=\"cuda\", dtype=torch.int8)\n\n    triton.set_allocator(alloc_fn)\n\n    # Hopper warpspec doesn't work with flatten\n    flatten = False if (warp_specialize and is_hopper()) else True\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_descriptor_persistent[grid](\n        a,\n        b,\n        c,  #\n        M,\n        N,\n        K,  #\n        NUM_SMS=NUM_SMS,  #\n        WARP_SPECIALIZE=warp_specialize,  #\n        FLATTEN=flatten,\n    )\n    return c\n\n\ndef device_blas_matmul(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    blas_name = device_blas_name()\n    with proton.scope(f\"{blas_name} [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        device_blas.matmul(a, b, c)\n    return c\n\n\ndef torch_matmul(a, b):\n    M, K = a.shape\n    N, K = b.shape\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    with proton.scope(f\"torch [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        c = torch.matmul(a, b.T)\n    return c\n\n\n@contextmanager\ndef proton_context():\n    proton.activate(0)\n    try:\n        yield\n    finally:\n        proton.deactivate(0)\n\n\ndef bench_fn(label, reps, warmup_reps, fn, *args):\n    print(f\"Benchmarking {label}: ...\", end=\"\")\n    for _ in range(warmup_reps):\n        fn(*args)\n    with proton_context():\n        for _ in range(reps):\n            fn(*args)\n    print(f\"\\rBenchmarking {label}: done\")\n\n\ndef bench(K, dtype, reps=10000, warmup_reps=10000):\n    M = 8192\n    N = 8192\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n\n    b = b.T.contiguous()\n\n    if device_blas is not None:\n        blas_name = device_blas_name()\n        bench_fn(blas_name, reps, warmup_reps, device_blas_matmul, a, b)\n    if dtype == torch.float16:\n        bench_fn(\"torch\", reps, warmup_reps, torch_matmul, a, b)\n    bench_fn(\"naive\", reps, warmup_reps, matmul, a, b.T)\n    bench_fn(\"persistent\", reps, warmup_reps, matmul_persistent, a, b.T)\n    warp_specialize = [False, True] if HAS_WARP_SPECIALIZE else [False]\n    for ws in warp_specialize:\n        ws_str = \"_ws\" if ws else \"\"\n        # disable on-host warpspec on Hopper\n        if HAS_HOST_TENSOR_DESC and not (is_hopper() and ws):\n            bench_fn(f\"tma_persistent{ws_str}\", reps, warmup_reps, lambda a, b: matmul_tma_persistent(a, b, ws), a, b)\n            bench_fn(f\"tma{ws_str}\", reps, warmup_reps, lambda a, b: matmul_tma(a, b, ws), a, b)\n        if HAS_TENSOR_DESC:\n            bench_fn(f\"descriptor_persistent{ws_str}\", reps, warmup_reps,\n                     lambda a, b: matmul_descriptor_persistent(a, b, ws), a, b)\n\n\ndef run_test(expect, fn, a, b, label, enabled=True):\n    print(f\"  {label}: ...\", end=\"\")\n    if enabled:\n        actual = fn(a, b)\n        passed = torch.allclose(expect, actual.to(expect.dtype), atol=1.0)\n        icon = \"\u2705\" if passed else \"\u274c\"\n    else:\n        icon = \"\u2b55\"\n    print(f\"\\r  {label}: {icon}  \")\n\n\ndef validate(M, N, K, dtype):\n    print(f\"{M=}, {N=}, {K=}, verification naive vs: \")\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = b.T.contiguous()\n\n    naive_result = matmul(a, b.T).to(torch.float16)\n    run_test(naive_result, torch_matmul, a, b, \"Torch\", enabled=dtype == torch.float16)\n    run_test(naive_result, device_blas_matmul, a, b, device_blas_name(), enabled=device_blas is not None)\n    run_test(naive_result, matmul_persistent, a, b.T, \"Persistent\")\n\n    kernels = [\n        (matmul_tma, \"TMA\", HAS_HOST_TENSOR_DESC),\n        (matmul_tma_persistent, \"TMA Persistent\", HAS_HOST_TENSOR_DESC),\n        (matmul_descriptor_persistent, \"Tensor Descriptor Persistent\", HAS_TENSOR_DESC),\n    ]\n    warp_specialize = [False, True] if HAS_WARP_SPECIALIZE else [False]\n\n    for (kernel, label, enabled), warp_specialize in itertools.product(kernels, warp_specialize):\n        label = f\"{label} (warp_specialize={warp_specialize})\"\n        # skip if hopper and warp_specialize and not on-device\n        skipped = is_hopper() and warp_specialize and kernel != matmul_descriptor_persistent\n        enabled = enabled and (not warp_specialize or HAS_TENSOR_DESC) and (not skipped)\n        run_test(naive_result, lambda a, b: kernel(a, b, warp_specialize), a, b, label, enabled)\n    print()\n\n\ndef show_profile(precision, profile_name):\n    import triton.profiler.viewer as proton_viewer\n    metric_names = [\"time/ms\"]\n    if precision == 'fp8':\n        metric_names = [\"tflop8/s\"] + metric_names\n    elif precision == 'fp16':\n        metric_names = [\"tflop16/s\"] + metric_names\n    file_name = f\"{profile_name}.hatchet\"\n    tree, metrics = proton_viewer.parse(metric_names, file_name)\n    proton_viewer.print_tree(tree, metrics)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-K\", type=int, required=False, default=512)\n    parser.add_argument(\"--K_range\", type=int, nargs=2)\n    parser.add_argument(\"--K_step\", type=int, default=512)\n    parser.add_argument(\"--prec\", type=str, choices=[\"fp8\", \"fp16\"], default=\"fp16\")\n    args = parser.parse_args()\n\n    if args.prec == 'fp8' and (not hasattr(torch, \"float8_e4m3fn\") or not is_cuda()):\n        print(\"This example requires CUDA/HIP with fp8 support.\")\n    else:\n        dtype = torch.float8_e4m3fn if args.prec == 'fp8' else torch.float16\n\n        if args.K and args.K_range is None:\n            args.K_range = [args.K, args.K]\n            args.K_step = 1  # doesn't matter as long as it's not 0\n\n        torch.manual_seed(0)\n\n        validate(32, 32, 32, dtype)\n        validate(8192, 8192, args.K_range[0], dtype)\n\n        proton.start(\"matmul\", hook=\"triton\")\n        proton.deactivate()\n        for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n            bench(K, dtype)\n        proton.finalize()\n        show_profile(args.prec, \"matmul\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
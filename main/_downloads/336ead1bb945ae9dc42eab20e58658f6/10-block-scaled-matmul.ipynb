{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Block Scaled Matrix Multiplication\nThis tutorial demonstrates a Triton implementation of block scaled matrix multiplication\nwhich is generic over FP4 and FP8 formats on NVIDIA and AMD GPUs.\nThe tutorial supports OCP microscaling formats such as mxfp4 and mxfp8, and NVIDIA's nvfp4\n(on NVIDIA GPUs) and mxfp4 (on AMD GPUs). These matrix multiplications are hardware-accelerated\nusing fifth-generation Tensor Cores on NVIDIA GPUs with compute capability 10, and by the CDNA4\nmatrix cores on AMD GPUs.\nUsers can run the tutorial with each of the supported formats by passing the `--format`\nargument and can benchmark the performance of each by specifying matrix dimensions\nand iteration steps.\n\n```bash\n# FP4\npython 10-block-scaled-matmul.py --format nvfp4\npython 10-block-scaled-matmul.py --format mxfp4 --K_range 512 8192 --bench\n\n# FP8\npython 10-block-scaled-matmul.py --format mxfp8 --K_range 8192 16384 --K_step 2048 --bench\n```\nFuture updates to this tutorial which support mixed precision block scaled matmul are planned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background\nScale preshuffling on NVIDIA GPUs\n\nCUDA devices that support PTX 8.7 and later can utlize block scaled matrix multiply\ninstructions. In order for low latency access to these scale factors in the fast\ninner loop over tensor core MMAs, it is important to ensure that the blocked\nscale factors are stored in a contiguous memory layout according to their access\npattern.\n\nThe block scaled matmul tensor core instructions compute the following product:\n\n    C = (A * scale_a) @ (B * scale_b)\n\nwhere scale_a and scale_b are the blocked scale factors for the A and B matrices.\nUnder block scaled matmul, each scale factor is broadcast and multiplied across a\nvector of elements from the A and B matrices, usually along their respective K axes.\nThe number of elements of A and B over which each scale factor is broadcast is herein\nrefered to as the vector size (VEC_SIZE).\n\nIn a linear row-major layout, the scale factors would take the shape\n\n    (M, K // VEC_SIZE) and (N, K // VEC_SIZE)   [1]\n\nin global memory. However, to avoid non-contiguous memory access, it is beneficial to\ninstead store the scale factors in a packed block layout. For the LHS matrix this layout\nis given by\n\n    (M // 32 // 4, K // VEC_SIZE // 4, 32, 4, 4)   [2].\n\nIn this way, each tensor core MMA in the fast inner loop over K blocks can achieve contiguous\naccess of a block of 128 rows of scale factors along the M axis, for each BLOCK_M x BLOCK_K\nsubtile of the matrix A.\n\nIn order to conform with Triton's language semantics for dot_scaled, the scale factors\nare prepared in the above 5D layout [2], but are then logically transposed and reshaped into\nthe 2D layout [1] expected by tl.dot_scaled.\n\nFor more detailed information on the scale factor layout, see\n 1. https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-mma-scale-factor-a-layout-1x\n 2. https://docs.nvidia.com/cuda/cublas/#d-block-scaling-factors-layout\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Scale preshuffling on AMD GPUs\n#\n# Similar to NVIDIA GPUs, on AMD GPUs with CDNA4 architecture, scaled MFMA instructions natively\n# support scaled matrix multiplication. Since it only supports OCP microscaling formats each\n# scale is an 8-bit value that scales 32 elements from A or B operand tensors.\n# Scales are stored as 8-bit tensors. Since MFMA instructions are warp-level instructions, that\n# means that each thread provides a fixed set of operand values to MFMA instructions.\n#\n# For example, in an MFMA instruction with shape 16x16x128:\n# - 4 threads contribute elements along the K dimension.\n# - 16 threads contribute elements along the M or N dimension.\n#\n# From the perspective of the scales tensor, even if the K dimension is stored contiguously in\n# shared memory, each thread sees its elements along K dim as strided due to interleaving with\n# other threads. This striding limits the ability to load scale values using vectorized memory\n# access.\n#\n# Our goal is to reorganize the scale tensor so that:\n# 1. Each thread stores the 4 scale values it needs for 4 MFMA ops in contiguous memory.\n# 2. Continuous threads access contiguous memory locations improving global memory coalescing when\n# bypassing LDS, which is especially beneficial for \"skinny\" matmuls.\n#\n# We consider two MFMA cases: one with non-K dimension 16, and one with 32.\n# In both, the minimum tile size for preshuffling is 32x32x256.\n# For example, for a 32x256 operand tile, the corresponding scale tensor has shape 32x8,\n# where each scale covers 32 elements along the K dimension.\n#\n# Each thread holds one scale per MFMA operation. We pack the 4 scale values\n# (for 4 different MFMA ops) next to each other in memory.\n#\n# Case 1: mfma_scaled_16x16x128\n#\n# Packing order: mfma_op_0, mfma_op_2, mfma_op_1, mfma_op_3\n#\n#            K = 128       K = 128\n#        +------------+ +------------+\n#    M=16|  MFMA op 0 | |  MFMA op 1 |\n#        +------------+ +------------+\n#    M=16|  MFMA op 2 | |  MFMA op 3 |\n#        +------------+ +------------+\n#\n# Case 2: mfma_scaled_32x32x64\n#\n# Packing order: mfma_op_0, mfma_op_1, mfma_op_2, mfma_op_3\n#\n#            K=64     K=64     K=64     K=64\n#        +--------+ +--------+ +--------+ +--------+\n#    M=32| op 0   | | op 1   | | op 2   | | op 3   |\n#        +--------+ +--------+ +--------+ +--------+\n\nimport argparse\n\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.profiler as proton\nfrom triton.tools.tensor_descriptor import TensorDescriptor\nfrom triton.tools.mxfp import MXFP4Tensor, MXScaleTensor\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef is_hip_cdna4():\n    target = triton.runtime.driver.active.get_current_target()\n    return target is not None and target.backend == 'hip' and target.arch == 'gfx950'\n\n\ndef supports_block_scaling():\n    return (is_cuda() and torch.cuda.get_device_capability()[0] in [10, 11]) or is_hip_cdna4()\n\n\nif is_cuda() and torch.cuda.get_device_capability()[0] in [10, 11]:\n    from triton._C.libtriton import nvidia\n    cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n    cublas = nvidia.cublas.CublasLt(cublas_workspace)\nelse:\n    cublas = None\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    kernel_name = kernel.name\n    if \"ELEM_PER_BYTE_A\" and \"ELEM_PER_BYTE_B\" and \"VEC_SIZE\" in args:\n        if args[\"ELEM_PER_BYTE_A\"] == 1 and args[\"ELEM_PER_BYTE_B\"] == 1:\n            kernel_name += \"_mxfp8\"\n        elif args[\"ELEM_PER_BYTE_A\"] == 1 and args[\"ELEM_PER_BYTE_B\"] == 2:\n            kernel_name += \"_mixed\"\n        elif args[\"ELEM_PER_BYTE_A\"] == 2 and args[\"ELEM_PER_BYTE_B\"] == 2:\n            if args[\"VEC_SIZE\"] == 16:\n                kernel_name += \"_nvfp4\"\n            elif args[\"VEC_SIZE\"] == 32:\n                kernel_name += \"_mxfp4\"\n    ret[\"name\"] = f\"{kernel_name} [M={M}, N={N}, K={K}]\"\n    ret[\"flops\"] = 2.0 * M * N * K\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef block_scaled_matmul_kernel(  #\n        a_desc,  #\n        a_scale_desc,  #\n        b_desc,  #\n        b_scale_desc,  #\n        c_desc,  #\n        M: tl.constexpr,  #\n        N: tl.constexpr,  #\n        K: tl.constexpr,  #\n        output_type: tl.constexpr,  #\n        ELEM_PER_BYTE_A: tl.constexpr,  #\n        ELEM_PER_BYTE_B: tl.constexpr,  #\n        VEC_SIZE: tl.constexpr,  #\n        BLOCK_M: tl.constexpr,  #\n        BLOCK_N: tl.constexpr,  #\n        BLOCK_K: tl.constexpr,  #\n        rep_m: tl.constexpr,  #\n        rep_n: tl.constexpr,  #\n        rep_k: tl.constexpr,  #\n        NUM_STAGES: tl.constexpr,  #\n):  #\n    if output_type == 0:\n        output_dtype = tl.float32\n    elif output_type == 1:\n        output_dtype = tl.float16\n    elif output_type == 2:\n        output_dtype = tl.float8e4nv\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n    offs_am = pid_m * BLOCK_M\n    offs_bn = pid_n * BLOCK_N\n    offs_k_a = 0\n    offs_k_b = 0\n    offs_scale_m = pid_m * rep_m\n    offs_scale_n = pid_n * rep_n\n    offs_scale_k = 0\n\n    MIXED_PREC: tl.constexpr = ELEM_PER_BYTE_A == 1 and ELEM_PER_BYTE_B == 2\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in tl.range(0, tl.cdiv(K, BLOCK_K), num_stages=NUM_STAGES):\n        a = a_desc.load([offs_am, offs_k_a])\n        b = b_desc.load([offs_bn, offs_k_b])\n        scale_a = a_scale_desc.load([0, offs_scale_m, offs_scale_k, 0, 0])\n        scale_b = b_scale_desc.load([0, offs_scale_n, offs_scale_k, 0, 0])\n\n        scale_a = scale_a.reshape(rep_m, rep_k, 32, 4, 4).trans(0, 3, 2, 1, 4).reshape(BLOCK_M, BLOCK_K // VEC_SIZE)\n        scale_b = scale_b.reshape(rep_n, rep_k, 32, 4, 4).trans(0, 3, 2, 1, 4).reshape(BLOCK_N, BLOCK_K // VEC_SIZE)\n\n        if MIXED_PREC:\n            accumulator = tl.dot_scaled(a, scale_a, \"e4m3\", b.T, scale_b, \"e2m1\", accumulator)\n        elif ELEM_PER_BYTE_A == 2 and ELEM_PER_BYTE_B == 2:\n            accumulator = tl.dot_scaled(a, scale_a, \"e2m1\", b.T, scale_b, \"e2m1\", accumulator)\n        else:\n            accumulator = tl.dot_scaled(a, scale_a, \"e4m3\", b.T, scale_b, \"e4m3\", accumulator)\n\n        offs_k_a += BLOCK_K // ELEM_PER_BYTE_A\n        offs_k_b += BLOCK_K // ELEM_PER_BYTE_B\n        offs_scale_k += rep_k\n\n    c_desc.store([offs_am, offs_bn], accumulator.to(output_dtype))\n\n\ndef block_scaled_matmul(a_desc, a_scale_desc, b_desc, b_scale_desc, dtype_dst, M, N, K, rep_m, rep_n, rep_k, configs):\n    output = torch.empty((M, N), dtype=dtype_dst, device=\"cuda\")\n    if dtype_dst == torch.float32:\n        dtype_dst = 0\n    elif dtype_dst == torch.float16:\n        dtype_dst = 1\n    elif dtype_dst == torch.float8_e4m3fn:\n        dtype_dst = 2\n    else:\n        raise ValueError(f\"Unsupported dtype: {dtype_dst}\")\n\n    BLOCK_M = configs[\"BLOCK_SIZE_M\"]\n    BLOCK_N = configs[\"BLOCK_SIZE_N\"]\n    c_desc = TensorDescriptor.from_tensor(output, [BLOCK_M, BLOCK_N])\n\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)\n    block_scaled_matmul_kernel[grid](\n        a_desc,\n        a_scale_desc,\n        b_desc,\n        b_scale_desc,\n        c_desc,\n        M,\n        N,\n        K,\n        dtype_dst,\n        configs[\"ELEM_PER_BYTE_A\"],\n        configs[\"ELEM_PER_BYTE_B\"],\n        configs[\"VEC_SIZE\"],\n        configs[\"BLOCK_SIZE_M\"],\n        configs[\"BLOCK_SIZE_N\"],\n        configs[\"BLOCK_SIZE_K\"],\n        rep_m,\n        rep_n,\n        rep_k,\n        configs[\"num_stages\"],\n    )\n    return output\n\n\ndef cublas_block_scaled_matmul(a, a_scale, b, b_scale, block_scale_type=\"mxfp8\"):\n    \"\"\"\n    cuBLAS block-scaled matmul baseline.\n\n    Args:\n        a: Input matrix A\n            - For mxfp8: (M, K) in FP8 E4M3\n            - For nvfp4: (M, K//2) in uint8 packed FP4 (2 elements per byte)\n        a_scale: Scale factors for A\n            - For mxfp8: E8M0 scales (flattened)\n            - For nvfp4: FP8 E4M3 scales in cublas layout (M, K//16)\n        b: Input matrix B\n            - For mxfp8: (N, K) in FP8 E4M3\n            - For nvfp4: (N, K//2) in uint8 packed FP4 (2 elements per byte)\n        b_scale: Scale factors for B\n            - For mxfp8: E8M0 scales (flattened)\n            - For nvfp4: FP8 E4M3 scales in cublas layout (N, K//16)\n        block_scale_type: Format type (\"mxfp8\" or \"nvfp4\")\n\n    Returns:\n        output: Result matrix (M, N) in FP16\n    \"\"\"\n    M, K_a = a.shape\n    N, K_b = b.shape\n\n    if block_scale_type == \"mxfp8\":\n        assert K_a == K_b, \"K dimensions must match\"\n        assert a.dtype == torch.float8_e4m3fn, \"Only FP8 E4M3 inputs supported for mxfp8\"\n        assert b.dtype == torch.float8_e4m3fn, \"Only FP8 E4M3 inputs supported for mxfp8\"\n        # MXFP8 cuBLAS outputs FP16\n        output = torch.empty((M, N), dtype=torch.float16, device=\"cuda\")\n        cublas.block_scaled_matmul_mxfp8(a, b, output, a_scale, b_scale)\n    elif block_scale_type == \"nvfp4\":\n        # For packed FP4, K_a and K_b are in bytes (K = K_a * 2 in elements)\n        assert K_a == K_b, \"K dimensions must match\"\n        assert a.dtype == torch.uint8, \"Only uint8 packed FP4 inputs supported for nvfp4\"\n        assert b.dtype == torch.uint8, \"Only uint8 packed FP4 inputs supported for nvfp4\"\n        # NVFP4 cuBLAS outputs FP16\n        output = torch.empty((M, N), dtype=torch.float16, device=\"cuda\")\n        cublas.block_scaled_matmul_nvfp4(a, b, output, a_scale, b_scale)\n    else:\n        raise ValueError(f\"Unsupported block_scale_type: {block_scale_type}\")\n\n    return output\n\n\ndef initialize_block_scaled(M, N, K, block_scale_type=\"nvfp4\", compute_reference=False):\n    BLOCK_M = 128\n    BLOCK_N = 256\n    BLOCK_K = 256 if \"fp4\" in block_scale_type else 128\n    VEC_SIZE = 16 if block_scale_type == \"nvfp4\" else 32\n    assert block_scale_type in [\"nvfp4\", \"mxfp4\", \"mxfp8\", \"mixed\"], f\"Invalid block scale type: {block_scale_type}\"\n    ELEM_PER_BYTE_A = 2 if \"fp4\" in block_scale_type else 1\n    ELEM_PER_BYTE_B = 1 if block_scale_type == \"mxfp8\" else 2\n\n    device = \"cuda\"\n    a_ref = MXFP4Tensor(size=(M, K), device=device).random()\n    # Similar to Hopper's wgmma symmetric fp8 instruction, the RHS is expected\n    # to be in col-major layout for Blackwell's tcgen05.mma when using fp4 operands.\n    # To conform to the expected semantics of tl.dot_scaled, (M, K) x (K, N),\n    # the data is generated in col-major layout, packed along K for fp4, and then\n    # logically transposed. Note that if one operand is of fp8 precision, unlike Hopper,\n    # Blackwell supports both row-major and col-major layouts for the RHS matrix.\n    # For the mixed-precision case, the fp4 RHS can be either in row or col-major layout.\n    # But for performance reason, it is recommended to use col-major layout. If TMA is used\n    # for the fp4 RHS operand load in mixed-precision dot, as in this tutorial, it must be\n    # in col-major layout.\n    b_ref = MXFP4Tensor(size=(N, K), device=device).random()\n    if block_scale_type in [\"mxfp8\", \"mixed\"]:\n        a_ref = a_ref.to(torch.float32)\n        a = a_ref.to(torch.float8_e4m3fn)\n    else:\n        # Pack two fp4 elements per byte along K\n        a = a_ref.to_packed_tensor(dim=1)\n\n    if block_scale_type == \"mxfp8\":\n        b_ref = b_ref.to(torch.float32)\n        b = b_ref.to(torch.float8_e4m3fn)\n    else:\n        b = b_ref.to_packed_tensor(dim=1)\n\n    b_ref = b_ref.to(torch.float32).T\n\n    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_M, BLOCK_K // ELEM_PER_BYTE_A])\n    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_N, BLOCK_K // ELEM_PER_BYTE_B])\n\n    a_scale_shape = [M // 128, K // VEC_SIZE // 4, 32, 16]\n    b_scale_shape = [N // 128, K // VEC_SIZE // 4, 32, 16]\n    epsilon = 1e-8\n    a_scale = torch.rand(a_scale_shape, device=device) + epsilon\n    b_scale = torch.rand(b_scale_shape, device=device) + epsilon\n\n    # Store original scales for cublas nvfp4 before any layout conversion.\n    # For cublas nvfp4, the scales are in the original 4D layout.\n    a_scale_orig = a_scale.clone()\n    b_scale_orig = b_scale.clone()\n\n    if block_scale_type == \"nvfp4\":\n        a_scale = a_scale.to(torch.float8_e4m3fn)\n        b_scale = b_scale.to(torch.float8_e4m3fn)\n        a_scale_ref = a_scale\n        b_scale_ref = b_scale\n    elif block_scale_type in [\"mxfp4\", \"mxfp8\", \"mixed\"]:\n        a_scale_ref = MXScaleTensor(a_scale)\n        b_scale_ref = MXScaleTensor(b_scale)\n        a_scale = a_scale_ref.data\n        b_scale = b_scale_ref.data\n\n    rep_m = BLOCK_M // 128\n    rep_n = BLOCK_N // 128\n    rep_k = BLOCK_K // VEC_SIZE // 4\n\n    # Use 5D TMA descriptor [1, rep_m, rep_k, 2, 256] with uint8 elements.\n    # With 256 elements we better utilize the L2 and don't require the TMA\n    # engine to emit many small messages (16B) messages as with 32x16xu8.\n    a_scale_block_shape = [1, rep_m, rep_k, 2, 256]\n    b_scale_block_shape = [1, rep_n, rep_k, 2, 256]\n    a_scale = a_scale.reshape(1, a_scale_shape[0], a_scale.shape[1], 2, 256)\n    b_scale = b_scale.reshape(1, b_scale_shape[0], b_scale.shape[1], 2, 256)\n    a_scale_desc = TensorDescriptor.from_tensor(a_scale, block_shape=a_scale_block_shape)\n    b_scale_desc = TensorDescriptor.from_tensor(b_scale, block_shape=b_scale_block_shape)\n\n    reference = None\n    if compute_reference:\n        a_scale_ref = a_scale_ref.to(torch.float32)\n        b_scale_ref = b_scale_ref.to(torch.float32)\n\n        def unpack_scale(packed):\n            packed = packed.reshape(*packed.shape[:-2], 32, 4, 4)\n            num_chunk_m, num_chunk_k, _, _, _ = packed.shape\n            return packed.permute(0, 3, 2, 1, 4).reshape(num_chunk_m * 128, num_chunk_k * 4).contiguous()\n\n        a_scale_ref = unpack_scale(a_scale_ref).repeat_interleave(VEC_SIZE, dim=1)[:M, :K]\n        b_scale_ref = unpack_scale(b_scale_ref).repeat_interleave(VEC_SIZE, dim=1).T.contiguous()[:K, :N]\n        reference = torch.matmul(a_ref.to(torch.float32) * a_scale_ref, b_ref * b_scale_ref)\n\n    configs = {\n        \"BLOCK_SIZE_M\": BLOCK_M,\n        \"BLOCK_SIZE_N\": BLOCK_N,\n        \"BLOCK_SIZE_K\": BLOCK_K,\n        \"num_stages\": 4,\n        \"ELEM_PER_BYTE_A\": ELEM_PER_BYTE_A,\n        \"ELEM_PER_BYTE_B\": ELEM_PER_BYTE_B,\n        \"VEC_SIZE\": VEC_SIZE,\n    }\n\n    # Flatten scales for cuBLAS\n    if block_scale_type == \"mxfp8\":\n        a_scale_cublas = a_scale.contiguous().flatten()\n        b_scale_cublas = b_scale.contiguous().flatten()\n    elif block_scale_type == \"nvfp4\":\n        a_scale_orig = a_scale_orig.to(torch.float8_e4m3fn)\n        b_scale_orig = b_scale_orig.to(torch.float8_e4m3fn)\n        a_scale_cublas = a_scale_orig.contiguous().flatten()\n        b_scale_cublas = b_scale_orig.contiguous().flatten()\n\n    return a_desc, a_scale_desc, b_desc, b_scale_desc, rep_m, rep_n, rep_k, configs, reference, a, b, a_scale_cublas, b_scale_cublas\n\n\ndef validate_block_scaled(M, N, K, block_scale_type=\"nvfp4\"):\n    results = initialize_block_scaled(M, N, K, block_scale_type, compute_reference=True)\n    a_desc, a_scale_desc, b_desc, b_scale_desc, rep_m, rep_n, rep_k, configs, reference = results[:9]\n    a, b, a_scale_cublas, b_scale_cublas = results[9:]\n\n    # Test Triton implementation\n    output = block_scaled_matmul(a_desc, a_scale_desc, b_desc, b_scale_desc, torch.float16, M, N, K, rep_m, rep_n,\n                                 rep_k, configs)\n    torch.testing.assert_close(reference, output.to(torch.float32), atol=1e-3, rtol=1e-3)\n\n    # Test cuBLAS implementation if available (available for mxfp8 and nvfp4 only as of 13.1)\n    if cublas and block_scale_type in [\"mxfp8\", \"nvfp4\"]:\n        cublas_output = cublas_block_scaled_matmul(a, a_scale_cublas, b, b_scale_cublas,\n                                                   block_scale_type=block_scale_type)\n        torch.testing.assert_close(reference, cublas_output.to(torch.float32), atol=1e-3, rtol=1e-3)\n        print(f\"\u2705 (pass {block_scale_type} - Triton and cuBLAS)\")\n    else:\n        print(f\"\u2705 (pass {block_scale_type} - Triton only)\")\n\n\ndef bench_block_scaled(K, block_scale_type=\"nvfp4\", reps=10, warmup_reps=10):\n    assert K % 128 == 0\n    M = 8192\n    N = 8192\n    print(f\"Problem Shape = {M}x{N}x{K}\")\n\n    results = initialize_block_scaled(M, N, K, block_scale_type, compute_reference=False)\n    a_desc, a_scale_desc, b_desc, b_scale_desc, rep_m, rep_n, rep_k, configs, _ = results[:9]\n    a, b, a_scale_cublas, b_scale_cublas = results[9:]\n\n    # Warmup\n    for _ in range(warmup_reps):\n        _ = block_scaled_matmul(a_desc, a_scale_desc, b_desc, b_scale_desc, torch.float16, M, N, K, rep_m, rep_n, rep_k,\n                                configs)\n        if cublas is not None and supports_block_scaling() and block_scale_type in [\"mxfp8\", \"nvfp4\"]:\n            _ = cublas_block_scaled_matmul(a, a_scale_cublas, b, b_scale_cublas, block_scale_type=block_scale_type)\n\n    # Benchmark\n    proton.activate(0)\n    for _ in range(reps):\n        _ = block_scaled_matmul(a_desc, a_scale_desc, b_desc, b_scale_desc, torch.float16, M, N, K, rep_m, rep_n, rep_k,\n                                configs)\n        if cublas is not None and supports_block_scaling() and block_scale_type in [\"mxfp8\", \"nvfp4\"]:\n            bytes_per_elem = a.element_size()\n            # For nvfp4, K is in elements but a.shape[1] is in bytes, so use K/2 for byte calculation\n            K_bytes = K if block_scale_type == \"mxfp8\" else K // 2\n            with proton.scope(f\"cublas [M={M}, N={N}, K={K}]\",\n                              {\"bytes\": bytes_per_elem * (M * K_bytes + N * K_bytes + M * N), \"flops\": 2. * M * N * K}):\n                _ = cublas_block_scaled_matmul(a, a_scale_cublas, b, b_scale_cublas, block_scale_type=block_scale_type)\n    proton.deactivate(0)\n    print(\"Done benchmarking\")\n\n\ndef show_profile(profile_name):\n    import triton.profiler.viewer as proton_viewer\n\n    metric_names = [\"time/ms\"]\n    metric_names = [\"tflop/s\"] + metric_names\n    file_name = f\"{profile_name}.hatchet\"\n    tree, metrics = proton_viewer.parse(metric_names, file_name)\n    proton_viewer.print_tree(tree, metrics)\n\n\n@triton.jit\ndef block_scaled_matmul_kernel_cdna4(a_ptr, b_ptr, c_ptr, a_scales_ptr, b_scales_ptr, M, N, K, stride_am, stride_ak,\n                                     stride_bk, stride_bn, stride_ck, stride_cm, stride_cn, stride_asm, stride_ask,\n                                     stride_bsn, stride_bsk,\n                                     # Meta-parameters\n                                     BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                                     mfma_nonkdim: tl.constexpr):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A and B inputs are in the microscale fp4 (mxfp4) format.\n    A_scales and B_scales are in e8m0 format.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n\n    pid = tl.program_id(axis=0)\n\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # We assume 32 elements along K share the same scale.\n    SCALE_GROUP_SIZE: tl.constexpr = 32\n    num_k_iter = tl.cdiv(K, BLOCK_K // 2)\n    # Create pointers for first block of A and B input matrices\n    # The BLOCK sizes are of the elements and in fp4 we pack 2 per uint8 container.\n    offs_k = tl.arange(0, BLOCK_K // 2)\n    offs_k_split = offs_k\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k_split[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k_split[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Create pointers for the first block of A and B scales\n    offs_asn = (pid_n * (BLOCK_N // 32) + tl.arange(0, (BLOCK_N // 32))) % N\n    offs_ks = tl.arange(0, BLOCK_K // SCALE_GROUP_SIZE * 32)\n\n    # B scales are N x K even though B operand is K x N.\n    b_scale_ptrs = (b_scales_ptr + offs_asn[:, None] * stride_bsn + offs_ks[None, :] * stride_bsk)\n    offs_asm = (pid_m * (BLOCK_M // 32) + tl.arange(0, (BLOCK_M // 32))) % M\n    a_scale_ptrs = (a_scales_ptr + offs_asm[:, None] * stride_asm + offs_ks[None, :] * stride_ask)\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, num_k_iter):\n        # Here we \"undo\" the shuffle done in global memory (shuffle_scales_cdna4 function).\n        if mfma_nonkdim == 32:\n            a_scales = tl.load(a_scale_ptrs).reshape(BLOCK_M // 32, BLOCK_K // SCALE_GROUP_SIZE // 8, 2, 32, 4,\n                                                     1).permute(0, 3, 1, 4, 2,\n                                                                5).reshape(BLOCK_M, BLOCK_K // SCALE_GROUP_SIZE)\n            b_scales = tl.load(b_scale_ptrs).reshape(BLOCK_N // 32, BLOCK_K // SCALE_GROUP_SIZE // 8, 2, 32, 4,\n                                                     1).permute(0, 3, 1, 4, 2,\n                                                                5).reshape(BLOCK_N, BLOCK_K // SCALE_GROUP_SIZE)\n        elif mfma_nonkdim == 16:\n            a_scales = tl.load(a_scale_ptrs).reshape(BLOCK_M // 32, BLOCK_K // SCALE_GROUP_SIZE // 8, 4, 16, 2, 2,\n                                                     1).permute(0, 5, 3, 1, 4, 2,\n                                                                6).reshape(BLOCK_M, BLOCK_K // SCALE_GROUP_SIZE)\n            b_scales = tl.load(b_scale_ptrs).reshape(BLOCK_N // 32, BLOCK_K // SCALE_GROUP_SIZE // 8, 4, 16, 2, 2,\n                                                     1).permute(0, 5, 3, 1, 4, 2,\n                                                                6).reshape(BLOCK_N, BLOCK_K // SCALE_GROUP_SIZE)\n\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs, cache_modifier=None)\n\n        accumulator += tl.dot_scaled(a, a_scales, \"e2m1\", b, b_scales, \"e2m1\")\n\n        # Advance the ptrs to the next K block.\n        a_ptrs += (BLOCK_K // 2) * stride_ak\n        b_ptrs += (BLOCK_K // 2) * stride_bk\n\n        a_scale_ptrs += BLOCK_K * stride_ask\n        b_scale_ptrs += BLOCK_K * stride_bsk\n\n    c = accumulator.to(c_ptr.type.element_ty)\n\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M).to(tl.int64)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N).to(tl.int64)\n    c_ptrs = (c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :])\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n\n    tl.store(c_ptrs, c, mask=c_mask, cache_modifier=\".wt\")\n\n\ndef shuffle_scales_cdna4(scales: torch.Tensor, mfma_nonkdim: int):\n    scales_shuffled = scales.clone()\n    sm, sn = scales_shuffled.shape\n\n    if mfma_nonkdim == 32:\n        scales_shuffled = scales_shuffled.view(sm // 32, 32, sn // 8, 4, 2, 1)\n        scales_shuffled = scales_shuffled.permute(0, 2, 4, 1, 3, 5).contiguous()\n    elif mfma_nonkdim == 16:\n        scales_shuffled = scales_shuffled.view(sm // 32, 2, 16, sn // 8, 2, 4, 1)\n        scales_shuffled = scales_shuffled.permute(0, 3, 5, 2, 4, 1, 6).contiguous()\n\n    scales_shuffled = scales_shuffled.view(sm // 32, sn * 32)\n    return scales_shuffled\n\n\ndef initialize_block_scaled_amd(M, N, K, mfma_nonkdim):\n\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_K = 256\n    configs = {\n        \"BLOCK_M\": BLOCK_M,\n        \"BLOCK_N\": BLOCK_N,\n        \"BLOCK_K\": BLOCK_K,\n        \"num_stages\": 2,\n        \"num_warps\": 8,\n        \"mfma_nonkdim\": mfma_nonkdim,\n    }\n\n    torch.manual_seed(5)\n\n    x = MXFP4Tensor(size=(M, K), device=\"cuda\").random()\n    w = MXFP4Tensor(size=(N, K), device=\"cuda\").random()\n\n    x_scales = torch.randint(124, 128, (K // 32, M), dtype=torch.uint8, device=\"cuda\")\n    w_scales = torch.randint(124, 128, (K // 32, N), dtype=torch.uint8, device=\"cuda\")\n    x_scales = x_scales.T\n    w_scales = w_scales.T\n    x_scales_shuffled = shuffle_scales_cdna4(x_scales, configs[\"mfma_nonkdim\"])\n    w_scales_shuffled = shuffle_scales_cdna4(w_scales, configs[\"mfma_nonkdim\"])\n\n    return (\n        x,\n        w,\n        x_scales,\n        w_scales,\n        x_scales_shuffled,\n        w_scales_shuffled,\n        configs,\n    )\n\n\ndef validate_block_scaled_amd(M, N, K, block_scale_type=\"mxfp4\", mfma_nonkdim=16):\n\n    def e8m0_to_f32(x):\n        x_f32 = 2**((x - 127).to(torch.float32))\n        x_f32[x_f32 == 128] = float(\"nan\")\n        return x_f32\n\n    def run_torch(x, w, x_scales, w_scales, dtype):\n        # First convert the x and w inputs to f32.\n        x_f32 = x.to(torch.float32)\n        w_f32 = w.to(torch.float32)\n        # Next convert the e8m0 scales to f32.\n        x_scales = x_scales.repeat_interleave(32, dim=1).to(torch.float32)\n        x_scales_f32 = e8m0_to_f32(x_scales)\n        x_f32 = x_f32 * x_scales_f32\n        w_scales = w_scales.repeat_interleave(32, dim=1).to(torch.float32)\n        w_scales_f32 = e8m0_to_f32(w_scales)\n        w_f32 = w_f32 * w_scales_f32\n        return torch.mm(x_f32, w_f32.T).to(dtype)\n\n    x_mxfp4, w_mxfp4, x_scales, w_scales, x_scales_triton, w_scales_triton, configs = \\\n    initialize_block_scaled_amd(M, N, K, mfma_nonkdim)\n\n    x = x_mxfp4.to_packed_tensor(dim=1)\n    w = w_mxfp4.to_packed_tensor(dim=1)\n\n    triton_out = torch.empty((M, N), device=x.device)\n    triton_out = block_scaled_matmul_amd(x, w, x_scales_triton, w_scales_triton, configs)\n    triton_out = triton_out.to(torch.float32)\n\n    torch_out = run_torch(x_mxfp4, w_mxfp4, x_scales, w_scales, torch.float32)\n    torch.testing.assert_close(torch_out, triton_out)\n    print(f\"\u2705 (pass {block_scale_type}, mfma_nonk_dim {mfma_nonkdim})\")\n\n\ndef block_scaled_matmul_amd(x, w, x_scales_triton, w_scales_triton, configs):\n    M, K = x.shape\n    N, K = w.shape\n    w = w.T\n    triton_out = torch.empty((M, N), device=x.device)\n\n    kernel_kwargs = {}\n    kernel_kwargs[\"matrix_instr_nonkdim\"] = configs[\"mfma_nonkdim\"]\n\n    BLOCK_M = configs[\"BLOCK_M\"]\n    BLOCK_N = configs[\"BLOCK_N\"]\n\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)\n\n    triton_out = torch.empty((M, N), device=\"cuda\")\n\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N), 1)\n    block_scaled_matmul_kernel_cdna4[grid](x, w, triton_out, x_scales_triton, w_scales_triton, M, N, K, x.stride(0),\n                                           x.stride(1), w.stride(0), w.stride(1), 0, triton_out.stride(0),\n                                           triton_out.stride(1), x_scales_triton.stride(0), x_scales_triton.stride(1),\n                                           w_scales_triton.stride(0), w_scales_triton.stride(1), BLOCK_M, BLOCK_N,\n                                           configs[\"BLOCK_K\"], configs[\"mfma_nonkdim\"], num_warps=configs[\"num_warps\"],\n                                           num_stages=configs[\"num_stages\"], **kernel_kwargs)\n    triton_out = triton_out.to(torch.float32)\n\n    return triton_out\n\n\ndef bench_block_scaled_amd(K, block_scale_type=\"mxfp4\", reps=10, mfma_nonkdim=16):\n    assert K % 128 == 0\n    M = 8192\n    N = 8192\n    print(f\"Problem Shape = {M}x{N}x{K}\")\n\n    x_mxfp4, w_mxfp4, x_scales, w_scales, x_scales_triton, w_scales_triton, configs = \\\n    initialize_block_scaled_amd(M, N, K, mfma_nonkdim)\n\n    x = x_mxfp4.to_packed_tensor(dim=1)\n    w = w_mxfp4.to_packed_tensor(dim=1)\n\n    proton.activate(0)\n    for _ in range(reps):\n        _ = block_scaled_matmul_amd(x, w, x_scales_triton, w_scales_triton, configs)\n    proton.deactivate(0)\n    print(\"Done benchmarking\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-K\", type=int, required=False, default=512)\n    parser.add_argument(\"--K_range\", type=int, nargs=2)\n    parser.add_argument(\"--K_step\", type=int, default=512)\n    parser.add_argument(\"--bench\", action=\"store_true\", default=True)\n    parser.add_argument(\"--format\", type=str, choices=[\"mxfp4\", \"nvfp4\", \"mxfp8\", \"mixed\"], default=\"nvfp4\")\n    args = parser.parse_args()\n\n    if not supports_block_scaling():\n        print(\"\u26d4 This example requires GPU support for block scaled matmul\")\n    else:\n        if args.K and args.K_range is None:\n            args.K_range = [args.K, args.K]\n            args.K_step = 1  # doesn't matter as long as it's not 0\n\n        torch.manual_seed(42)\n\n        if is_cuda():\n            validate_block_scaled(8192, 8192, 8192, block_scale_type=args.format)\n        elif is_hip_cdna4():\n            assert args.format == \"mxfp4\", \"AMD tutorial only supports mxpf4 format currently\"\n            validate_block_scaled_amd(8192, 8192, 8192, block_scale_type=args.format, mfma_nonkdim=16)\n            validate_block_scaled_amd(8192, 8192, 8192, block_scale_type=args.format, mfma_nonkdim=32)\n\n        if args.bench:\n            proton.start(\"block_scaled_matmul\", hook=\"triton\")\n            proton.deactivate(0)  # Skip argument creation\n            for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n                if is_cuda():\n                    bench_block_scaled(K, reps=10000, block_scale_type=args.format)\n                elif is_hip_cdna4():\n                    bench_block_scaled_amd(K, reps=10000, block_scale_type=args.format, mfma_nonkdim=16)\n                    bench_block_scaled_amd(K, reps=10000, block_scale_type=args.format, mfma_nonkdim=32)\n            proton.finalize()\n            show_profile(\"block_scaled_matmul\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Agenda: &mdash; Triton  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />

  
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Triton
              <img src="https://cdn.openai.com/triton/assets/triton-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started/tutorials/index.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.html">triton</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.language.html">triton.language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton.testing.html">triton.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python-api/triton-semantics.html">Triton Semantics</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Triton MLIR Dialects</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dialects/dialects.html">Triton MLIR Dialects and Ops</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Programming Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../programming-guide/chapter-1/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../programming-guide/chapter-2/related-work.html">Related Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../programming-guide/chapter-3/debugging.html">Debugging Triton</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Triton</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Agenda:</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/meetups/11-05-2025/notes.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="agenda">
<h1>Agenda:<a class="headerlink" href="#agenda" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p>Community discussion:  <em>Gluon, TLX, CuTeDSL, cutile, tileIR etc. … with so many choices, how do I decide on what I should use to write my next kernel/model</em></p></li>
<li><p>Post Triton Conference discussion:</p>
<ul>
<li><p>Ofer: recap of the event.</p></li>
<li><p>What did you like</p></li>
<li><p>What was shocking</p></li>
<li><p>What would you like to see more of/less of next year.</p></li>
</ul>
</li>
<li><p>Flex Attention questions - (Whitney, Intel)</p></li>
</ul>
</section>
<section id="notes">
<h1>Notes:<a class="headerlink" href="#notes" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p>Post Triton Conference discussion:</p>
<ul>
<li><p>Luka - Liked the breadth and interest in Triton, extensions and examples. Liked talks on warp specializaiton. Interestes: vLLM,  torch.compile() and  abstractions.</p></li>
<li><p>Simon Waters, kernelize.ai - Lots of great content. Next time, try and get presentations on the big screen center stage.</p></li>
<li><p>Bryan Bowyer, kernelize.ai - Liked the step by step walk throughs. Lets you see exactly how to use Triton/extensions. Would like to see more talks about novel AI hardware. Knows more devices are ready. Would like to see more Triton demos/especially hardware demos.</p></li>
<li><p>Puyan Lotfi, Meta - Also saw good talks at <a class="reference external" href="https://pytorch.org/event/pytorch-conference-2025/">PTC 2025</a> &amp; <a class="reference external" href="https://llvm.swoogo.com/2025devmtg/home">2025 LLVM Developers Meeting</a>- quite a few DSL extensions for more hardware features. Would like a more unified extension system. Proposed/saw an interesting idea: creating an MLIR dialect that doesn’t take fixed sized tensors, imbeds them in inline assembly.  Maybe we could do this in Triton.</p></li>
<li><p>Sara - Enjoyed presenting posters with colleagues. Liked Helion talk. Looking at Helion tutorials now. Interested in Triton kernels for vLLM and deploying to different hardware platforms (Nvidia, AMD and ???)</p></li>
<li><p>Corbin Robeck, Meta - is working on Triton extensions. Currently reviewing proposals from teams interested in adding distributed Triton, Triton for different architectures (integrated in an extension). Looking for mostly mature implementations. He’s currrently in the process of open sourcing this extension framework.</p></li>
<li><p>Dhruva Kaushal, Meta - Flex attention make the attention context parallel (Monarch announcement), Pytorch support for different data types MXFP8 and NVFP4, can Triton adopt and emulate these.</p></li>
<li><p>Jason Furmanek, AMD - AMD sharing some of their latest improvements (e.g. performant flash attention on MI350s) at both Triton conference and PTC.</p></li>
<li><p>Hongtao Yu, Meta - Liked seeing kernel performance numbers on AMD and GPU platforms, Triton DSL, understanding what the hard blockers are for customers adopting these DSLs. Happy to see more people using Triton and building more Triton libraries.</p></li>
<li><p>Jamie Yang - Seeing some divergence in the ML compiler landscape, of the different levels of abstraction, which will survive? He’s seeing attempts to do similar things as <a class="reference external" href="https://arxiv.org/abs/2504.19442">Triton-distributed</a> like what Meta is doing. Will they converge?  Interested in vLLM gpu kernels like llama 2 in Triton.</p></li>
<li><p>Jie Liu, Meta - Talks on Nvidia Blackwell extension &amp; abstractions were good.  ByteDance talk was good (nice to see presentations).  Would like to see a panel discussion. Suggested topics: common concerns &amp; directions and collaboration and brainstorming. Interested in: optimizing Blackwell attention &amp; automatic warp specialization (that is, the compiler should handle partitioning and scheduling.)</p></li>
<li><p>Keshav Singh - Thought presentations were insightful. Liked that he could review them online.  Interested in non-transformer models. Disappointed that there aren’t a lot of good example kernels though.</p></li>
<li><p>Kuy Mainwaring, Google - Leads XLA effort at Google. He’s an unusual user of Triton. They generate Triton IR! He’s interested in AMD &amp; Nvidia roadmaps. Wants to know what is the evolving future of these architectures. Where is Triton is going in the future?  Interested in families of templates, attention masking, scaling topologies. Currently, Google’s TPUs aren’t supported by Triton. There are quantization schemes that are unique to TPUs… how to map from one to another?  They want to be sure that Gemini works well on GPUs. Examples include INT4 dtype and proprietary data types, looking at normalization diamonds and softmax. Currently, XLA runs on many platforms. Maybe we could have covolution in Triton?
Ettore Tiotto, Intel - more important Jason’s talk on Helion, because triton is only mostly portable.  Intel has AMD, OAI doesn’t care about Intel.  MSFT asked how AMD got its backend into.  Get more backends into OpenAI community.  How to get its backends into triton.  Would like an easyway to push a plugin. (Reach out to Corbin Robeck</p></li>
<li><p>Luka Govedic - I’d like to make this more of a community similar to vLLM. Triton doesn’t support plugable backends. Would like to do something like vLLM where Huawei and other companies can add their own backends. You shouldn’t need to fork to support a new backend.</p></li>
</ul>
</li>
<li><p>Community discussion:  “Gluon, TLX, CuTeDSL, cutile, tileIR etc. … with so many choices, how do I decide on what I should use to write my next kernel/model”</p>
<ul>
<li><p>Hongtao Yu, Meta - Most people start with Triton. Once they get a kernel that does functionall what they want, they then think about performance. Typically, they try optimizations directly available in Triton. Some customers will go directly to cutlass/CuTeDSL. Scheduling is usually a question that drives this choice (how soo do you need it and what is acceptable performance). Other critera folks use when deciding on what language/framework to pick include: feature completeness and maturity.  Is the language/framework in startup phase, are there teams using/supporting it, is it still evolving.</p></li>
<li><p>Minjang Kim, Meta - Has similar concerns. Our customers want hardware heterogeneity but the introduction of Nvidia Blackwell introduced lots of divergence in the codebase. The PyTorch org has voiced lots of concern about this. Tile-based programming is a good thing. We don’t know what the winner will be but we would hope the winner enables hardware portability.  Helion is a good approach.</p></li>
<li><p>Sara - Looking forward to trying them all out!</p></li>
<li><p>Prithvi Patel, Meta - The Triton/Helion/Gluon/etc. tutorials give me a good handle on how to use these languages.</p></li>
<li><p>Hongtao Yu, Meta - If you want to see performance numbers, Meta/tritonbench has benchmark numbers for cuDNN, gluon, and cutlass too.</p></li>
<li><p>Whitney Tsang, Intel - I could try all of them but its still not clear which one to pick. I’d like a better idea of what the future for each of these solutions looks like. I’ve heard TLX is temporary and should be gone. Is Gluon is expected to stay in place and never be replaced? What are the choices if you want 100% or 90% of the hardware limit? I’d like it if triton, as a whole, were better.</p></li>
<li><p>Hongtao Yu, Meta - Meta is still looking at making the compiler more intelligent.</p></li>
<li><p>Luka - Gluon is not a short term soluton. It is a lower level dialect meant to help compiler writers.  Nvidia demonstrated they can successly implement autoWS in Gluon.</p></li>
<li><p>Whitney Tsang, Intel - Gluon is used in OpenAI’s production models.</p></li>
<li><p>Hongtao Yu, Meta - It depends on how the hardware is designed. If scheduling is better on chip, we won’t need to do it in software. Nvidia HW is super configurable but the HW can’t schedule efficiently.  Nvidia needs to invest more in hardware scheduling.  We’ll be keeping an eye on this.</p></li>
<li><p>Whitney Tsang, Intel - Triton isn’t dead because PyTorch continues to use Triton.</p></li>
<li><p>Corbin Robeck, Meta - Triton and CUTLASS have different internal layout systems and debugging porting a CUTLASS kernel to Triton requires very solid knowledge of both. Writing a CuTeDSL kernel requires knowledge of the underlying CUTLASS layouts as well.</p></li>
<li><p>Jason Furmanek, AMD  - AMD likes Triton and gluon for empowering developers. The closer you get to the hardware, the more you’re locked in. What are benefits of a new DSL? Gluon allows you to go deeper than out-of-the-box Triton. The question is do we need another DSL? What is the niche? Are people going to use inductor or XLA?</p></li>
<li><p>Luka - Announced TileIR is going into the LLVM stack. It will be like PTX and can be compiled into something more portable.  Is AMD interested in supporting this?</p></li>
<li><p>Jason Furmanek, AMD - AMD hasn’t looked at this level, that is, layers below DSLs, lowering paths, etc. AMD relies on LLVM both for good and for bad. It would be interesting to standardize on a different backend.</p></li>
<li><p>Kui Mainwaring, Google - We want our customers to identify the best DSL for themselves.  Jax on GPUs uses a mixture of interface: foreign function calls to cutlass, pallas lowering to TPU and mosaicGPU to gpus. AMD uses pallas to lower too.</p></li>
<li><p>Bryan Bowyer, kernelize.ai - Everyone uses what they want. Do what you can to reuse what you can and don’t diverge too soon in the stack.</p></li>
</ul>
</li>
<li><p>What is the status of flex attention tensor descriptor? PR for flex attention in PyTorch created by Intel [Whitney Tseng, Intel]</p>
<ul>
<li><p>Dhruva Kaushal, Meta - Saw the draft and commenting on it. Happy to see folks contributing to flex attention.</p></li>
<li><p>Whitney Tsang, Intel - Tensor descriptors are critical for Intel and Nvidia Blackwell. Can we change tutorials/etc. to use tensor descriptors?  .</p></li>
<li><p>Dhruva Kaushal, Meta - Please suggest changes to docs. If it improves performance, by all means please do.</p></li>
<li><p>Whitney Tsang, Intel - Any benchmarks on tensor descriptor vs regular pointer performance on non TMA hardware?</p></li>
<li><p>Dhruva Kaushal, Meta - No. Meta has benchmarks only for TMA hardware. Flex Attention for document Mask +30%-50% win. Sliding window, lower.</p></li>
<li><p>Ettore Tiotto, Intel - Tensor descriptors have more information than Tensor pointers. Pass exists to lower tensor descriptors to tensor pointers. Tensor descriptors should always have at least the same level of performance as tensor pointers on any architecture. Not true for Nvidia GPUs though! On Nvidia,indexes for offsets are 64-bit and tensor pointers use 32-bit (we should upstream this)</p></li>
</ul>
</li>
</ul>
</section>
<section id="minutes">
<h1>Minutes<a class="headerlink" href="#minutes" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p>Recording link <a class="reference external" href="https://www.youtube.com/watch?v=gaP6PpfPiEk">here</a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Philippe Tillet.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
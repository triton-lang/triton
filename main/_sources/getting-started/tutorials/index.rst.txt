:orphan:

Tutorials
=========

Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.

To install the dependencies for the tutorials:

.. code-block:: bash

    cd triton
    pip install -e './python[tutorials]'



.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, you will write a simple vector addition using Triton.">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_01-vector-add_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Vector Addition</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, you will write a fused softmax operation that is significantly faster than PyTorch&#x27;s native op for a particular class of matrices: those whose rows can fit in the GPU&#x27;s SRAM.">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_02-fused-softmax_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Fused Softmax</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="You will specifically learn about:">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_03-matrix-multiplication_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Matrix Multiplication</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, you will write a memory-efficient implementation of dropout whose state will be composed of a single int32 seed. This differs from more traditional implementations of dropout, whose state is generally composed of a bit mask tensor of the same shape as the input.">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_04-low-memory-dropout_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Low-Memory Dropout</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In doing so, you will learn about:">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_05-layer-norm_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Layer Normalization</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Fused Attention</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Please refer to CUDA libdevice-users-guide and/or HIP device-lib source code regarding the semantics of all available libdevice functions.">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_07-extern-functions_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_07-extern-functions.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Libdevice (`tl.extra.libdevice`) function</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Group GEMM">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_08-grouped-gemm_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_08-grouped-gemm.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Group GEMM</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Triton and cuBLAS implementations are benchmarked under different configurations and evaluated using the proton profiler. Users can pass command-line arguments to specify matrix dimensions and iteration steps flexibly.">

.. only:: html

  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_09-persistent-matmul_thumb.png
    :alt:

  :ref:`sphx_glr_getting-started_tutorials_09-persistent-matmul.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Persistent Matmul</div>
    </div>


.. thumbnail-parent-div-close

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /getting-started/tutorials/01-vector-add
   /getting-started/tutorials/02-fused-softmax
   /getting-started/tutorials/03-matrix-multiplication
   /getting-started/tutorials/04-low-memory-dropout
   /getting-started/tutorials/05-layer-norm
   /getting-started/tutorials/06-fused-attention
   /getting-started/tutorials/07-extern-functions
   /getting-started/tutorials/08-grouped-gemm
   /getting-started/tutorials/09-persistent-matmul


.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-gallery

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download all examples in Python source code: tutorials_python.zip </getting-started/tutorials/tutorials_python.zip>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download all examples in Jupyter notebooks: tutorials_jupyter.zip </getting-started/tutorials/tutorials_jupyter.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_


.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     473.028861    692.359690            208.636092
    1     384.0     660.370221    831.773850            263.292538
    2     512.0     818.170443    925.842798            299.155328
    3     640.0     920.935318    919.388108            329.844999
    4     768.0     975.385932    986.684199            348.558198
    5     896.0    1049.043653   1027.388995            352.531164
    6    1024.0    1077.526323   1073.347954            352.788761
    7    1152.0    1092.010575   1074.480017            349.748295
    8    1280.0    1128.472682   1114.870263            349.690121
    9    1408.0    1163.484731   1140.740455            341.508248
    10   1536.0    1199.162914   1166.318652            333.475050
    11   1664.0    1220.160301   1181.803327            329.585401
    12   1792.0    1238.242934   1201.871745            325.844729
    13   1920.0    1254.327645   1224.650383            324.511287
    14   2048.0    1274.045507   1252.778984            324.924393
    15   2176.0    1236.830632    959.331263            325.292698
    16   2304.0    1257.867564    999.281096            325.286081
    17   2432.0    1270.347897   1037.203046            326.221300
    18   2560.0    1288.622516   1072.570415            328.241094
    19   2688.0    1293.451416   1100.739230            329.519308
    20   2816.0    1307.336926   1125.940728            329.645451
    21   2944.0    1323.337547   1148.542981            331.976876
    22   3072.0    1322.719466   1174.647369            333.640840
    23   3200.0    1335.315628   1171.232796            334.478395
    24   3328.0    1350.098108   1199.765790            335.835831
    25   3456.0    1351.931005   1219.057027            336.941888
    26   3584.0    1362.384459   1245.101200            338.245821
    27   3712.0    1363.134980   1267.478266            340.335091
    28   3840.0    1368.960474   1280.370464            340.027080
    29   3968.0    1375.036184   1303.243351            341.333334
    30   4096.0    1392.496105   1317.086515            339.033911
    31   4224.0    1328.298855   1276.323987            343.333742
    32   4352.0    1347.311299   1299.434491            345.472986
    33   4480.0    1346.345072   1319.037216            345.640607
    34   4608.0    1356.714284   1336.014256            347.008754
    35   4736.0    1356.656281   1342.384889            347.687059
    36   4864.0    1364.018842   1359.643211            349.508980
    37   4992.0    1367.964853   1369.019930            349.837483
    38   5120.0    1374.826175   1389.153742            351.092063
    39   5248.0    1377.431857   1352.292910            351.523825
    40   5376.0    1378.541145   1369.893671            351.817785
    41   5504.0    1380.274497   1387.482725            354.000213
    42   5632.0    1393.319822   1397.854903            353.403644
    43   5760.0    1393.659768   1407.458916            354.921210
    44   5888.0    1393.765678   1417.288520            354.731030
    45   6016.0    1400.323756   1422.735389            356.945932
    46   6144.0    1406.941348   1426.942185            356.686234
    47   6272.0    1412.447847   1397.625081            357.946084
    48   6400.0    1414.530513   1407.185929            358.428990
    49   6528.0    1411.279524   1427.337032            359.144277
    50   6656.0    1417.377953   1425.173720            359.613512
    51   6784.0    1418.755785   1440.428651            360.399456
    52   6912.0    1423.386983   1444.972490            361.070679
    53   7040.0    1420.751837   1457.109376            360.802550
    54   7168.0    1421.388654   1457.985740            361.681384
    55   7296.0    1429.606420   1083.256622            362.250625
    56   7424.0    1423.847607   1097.618213            362.953591
    57   7552.0    1428.756750   1110.731399            363.796574
    58   7680.0    1430.745856   1120.252158            363.419283
    59   7808.0    1426.845785   1128.489091            364.412267
    60   7936.0    1431.727326   1143.063375            364.427363
    61   8064.0    1432.495475   1147.096526            365.189462
    62   8192.0    1431.953668   1149.075945            363.990464
    63   8320.0    1384.573965   1118.238670            362.311229
    64   8448.0    1383.883917   1124.067534            362.456474
    65   8576.0    1387.779018   1125.227464            363.067547
    66   8704.0    1383.355562   1132.508353            364.173420
    67   8832.0    1390.569513   1130.533525            365.018264
    68   8960.0    1383.236662   1140.606655            366.070040
    69   9088.0    1399.509954   1137.194536            366.760309
    70   9216.0    1406.997077   1144.440504            367.814206
    71   9344.0    1388.000658   1420.539976            367.499890
    72   9472.0    1394.768222   1431.019121            369.026981
    73   9600.0    1404.878055   1428.786875            369.181466
    74   9728.0    1399.488947   1444.439814            369.847757
    75   9856.0    1400.713233   1442.207221            370.303863
    76   9984.0    1399.394064   1448.684198            370.599905
    77  10112.0    1399.355359   1452.305696            370.719432
    78  10240.0    1409.963211   1467.172958            371.816586
    79  10368.0    1415.812790   1460.751131            369.828577
    80  10496.0    1406.745419   1469.220486            370.231053
    81  10624.0    1405.332464   1463.395409            370.814910
    82  10752.0    1398.398273   1471.486364            371.724427
    83  10880.0    1396.613630   1478.928644            371.745165
    84  11008.0    1420.708097   1475.726861            372.483086
    85  11136.0    1421.217857   1488.501258            372.739847
    86  11264.0    1411.732581   1485.362647            373.441176
    87  11392.0    1418.601899   1488.659832            374.469597
    88  11520.0    1412.437217   1494.226751            374.612008
    89  11648.0    1421.711720   1499.315170            374.462933
    90  11776.0    1433.279469   1501.149709            375.185603
    91  11904.0    1430.679833   1506.039459            375.579383
    92  12032.0    1414.423519   1507.287375            375.752423
    93  12160.0    1412.508739   1515.986058            376.169679
    94  12288.0    1428.998440   1419.867083            375.910202
    95  12416.0    1438.767510   1399.906053            374.696082
    96  12544.0    1444.416484   1393.540162            375.643552
    97  12672.0    1435.505626   1391.201826            375.545902




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.136 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_


.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   469.948329   688.644470     206.537326
    1     384.0   654.097332   826.118007     261.639870
    2     512.0   812.925759   915.630151     301.828035
    3     640.0   904.745334   913.826230     331.431646
    4     768.0   988.005898   989.579831     349.432545
    5     896.0  1043.599033  1029.824495     354.903521
    6    1024.0  1074.487846  1071.480596     352.906927
    7    1152.0  1100.387197  1077.719648     348.865092
    8    1280.0  1128.333658  1111.819092     349.427945
    9    1408.0  1162.231894  1140.850805     341.275077
    10   1536.0  1198.104313  1157.220496     333.061670
    11   1664.0  1217.506382  1184.387707     328.661848
    12   1792.0  1235.632969  1193.809034     325.475227
    13   1920.0  1260.982469  1227.234415     325.251452
    14   2048.0  1270.866103  1244.564277     324.631016
    15   2176.0  1239.254248   959.355981     325.261585
    16   2304.0  1258.068554   994.310293     325.887618
    17   2432.0  1277.724969  1029.652704     326.150943
    18   2560.0  1286.042718  1067.913375     327.984052
    19   2688.0  1294.610811  1101.667860     329.642513
    20   2816.0  1313.107441  1122.377520     329.227720
    21   2944.0  1315.377077  1143.799628     331.856760
    22   3072.0  1326.229940  1171.839449     332.928583
    23   3200.0  1334.860120  1166.097275     334.603368
    24   3328.0  1347.797292  1196.876605     336.611189
    25   3456.0  1355.452739  1218.819570     336.598322
    26   3584.0  1363.959392  1244.637824     338.002717
    27   3712.0  1369.774094  1261.964446     340.194569
    28   3840.0  1370.976655  1282.186750     340.204424
    29   3968.0  1373.935107  1300.502263     341.237573
    30   4096.0  1385.968646  1315.024976     339.060688
    31   4224.0  1326.390706  1278.998771     343.300003
    32   4352.0  1344.800397  1291.993376     344.776629
    33   4480.0  1349.082817  1313.021446     345.647989
    34   4608.0  1361.144102  1333.011896     347.312515
    35   4736.0  1360.437883  1345.500888     347.863905
    36   4864.0  1365.264288  1356.466861     348.885143
    37   4992.0  1368.690197  1372.570403     350.181401
    38   5120.0  1376.943649  1382.369575     350.768562
    39   5248.0  1378.758176  1350.834086     351.710311
    40   5376.0  1385.492614  1362.257343     351.918825
    41   5504.0  1384.929563  1376.155818     353.591272
    42   5632.0  1391.481390  1396.149605     352.929883
    43   5760.0  1391.876630  1406.930877     354.823604
    44   5888.0  1393.038137  1408.343899     354.216275
    45   6016.0  1402.742316  1416.765912     356.526954
    46   6144.0  1408.954210  1430.892391     356.857335
    47   6272.0  1410.808562  1392.281429     357.644806
    48   6400.0  1409.399351  1413.565897     358.382873
    49   6528.0  1410.120066  1416.870513     358.992279
    50   6656.0  1412.495786  1420.120364     359.403765
    51   6784.0  1419.451878  1440.581472     360.381047
    52   6912.0  1427.811701  1442.387151     361.259580
    53   7040.0  1415.856136  1446.531833     360.814542
    54   7168.0  1424.112101  1463.169185     361.433198
    55   7296.0  1427.963296  1083.368944     362.296327
    56   7424.0  1428.352142  1097.377743     363.094547
    57   7552.0  1429.674774  1108.432225     363.263394
    58   7680.0  1432.928580  1118.754496     363.572893
    59   7808.0  1428.971992  1128.066478     364.148983
    60   7936.0  1435.671003  1141.152548     364.489824
    61   8064.0  1431.844879  1146.939507     365.076160
    62   8192.0  1435.190126  1149.773702     364.214232
    63   8320.0  1383.454578  1116.347730     361.913470
    64   8448.0  1386.282706  1124.364787     362.438640
    65   8576.0  1384.030531  1126.937496     363.196573
    66   8704.0  1379.086909  1132.153114     364.115579
    67   8832.0  1393.567665  1132.534558     365.084990
    68   8960.0  1386.058725  1137.462000     366.101201
    69   9088.0  1396.287702  1137.891442     366.724388
    70   9216.0  1403.611807  1141.203421     367.191035
    71   9344.0  1388.827509  1418.004184     367.247653
    72   9472.0  1398.005326  1431.876771     368.646411
    73   9600.0  1397.072150  1429.066418     369.324930
    74   9728.0  1399.271063  1439.544694     368.869440
    75   9856.0  1399.677233  1440.702779     369.878540
    76   9984.0  1394.990529  1446.298894     370.604383
    77  10112.0  1405.359387  1454.207997     371.544159
    78  10240.0  1410.605454  1464.694501     370.852788
    79  10368.0  1419.241890  1458.177995     370.134690
    80  10496.0  1409.587583  1469.282047     369.889802
    81  10624.0  1407.182442  1467.572903     370.034303
    82  10752.0  1394.974896  1471.260742     371.804767
    83  10880.0  1396.284543  1479.235912     372.120420
    84  11008.0  1424.378317  1476.572687     372.819730
    85  11136.0  1421.788903  1484.288894     372.425547
    86  11264.0  1414.317351  1487.389218     372.881673
    87  11392.0  1422.917218  1490.054511     373.609395
    88  11520.0  1410.098362  1498.037436     373.980082
    89  11648.0  1417.872914  1497.254735     374.405405
    90  11776.0  1436.931322  1502.320263     374.637701
    91  11904.0  1429.924292  1508.927040     375.222803
    92  12032.0  1416.393407  1511.279229     375.637584
    93  12160.0  1412.978435  1513.833885     375.704604
    94  12288.0  1430.711640  1418.176539     376.154375
    95  12416.0  1443.021788  1398.698816     374.645918
    96  12544.0  1438.714916  1394.566564     375.239721
    97  12672.0  1433.653886  1391.699367     375.325902




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.985 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

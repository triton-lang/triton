
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     473.843835    694.012148            207.249137
    1     384.0     666.930659    820.096281            262.828759
    2     512.0     808.666960    916.587412            302.610060
    3     640.0     909.547291    932.654267            331.763676
    4     768.0     992.236339    976.541128            348.941669
    5     896.0    1052.679182   1039.983616            352.554085
    6    1024.0    1066.766785   1063.996242            353.958189
    7    1152.0    1088.463492   1065.215645            347.732061
    8    1280.0    1125.608699   1100.461293            348.496346
    9    1408.0    1167.782054   1129.423341            340.475973
    10   1536.0    1186.529888   1166.272947            334.276684
    11   1664.0    1210.696658   1190.441515            329.497795
    12   1792.0    1231.520951   1202.529506            326.113579
    13   1920.0    1254.097720   1227.652221            325.320600
    14   2048.0    1267.854461   1244.823598            325.109369
    15   2176.0    1236.721466    963.896907            325.078170
    16   2304.0    1247.876402   1002.841382            325.345100
    17   2432.0    1268.697980   1037.497172            326.810024
    18   2560.0    1284.031353   1070.642496            327.953768
    19   2688.0    1295.875539   1096.389356            329.256216
    20   2816.0    1303.708987   1126.400028            329.300084
    21   2944.0    1318.681811   1145.478322            331.309739
    22   3072.0    1328.217192   1174.103787            333.591277
    23   3200.0    1337.928439   1172.818933            334.837591
    24   3328.0    1344.329544   1199.351038            336.304927
    25   3456.0    1348.642138   1221.692806            337.407330
    26   3584.0    1357.891481   1246.208301            337.999178
    27   3712.0    1370.477039   1262.743624            340.542151
    28   3840.0    1376.546983   1284.866899            340.291518
    29   3968.0    1378.078365   1295.951173            340.882509
    30   4096.0    1388.890516   1319.065142            338.484740
    31   4224.0    1326.738924   1276.025147            343.299640
    32   4352.0    1337.005721   1301.214364            345.660832
    33   4480.0    1345.535464   1318.630780            346.006409
    34   4608.0    1363.229581   1336.394083            347.206665
    35   4736.0    1356.215290   1345.181171            347.894351
    36   4864.0    1364.129832   1358.033122            349.092189
    37   4992.0    1369.448511   1374.672519            350.449448
    38   5120.0    1374.270055   1385.425826            350.507952
    39   5248.0    1376.160716   1353.714369            351.406925
    40   5376.0    1380.022644   1373.741327            351.564030
    41   5504.0    1378.652255   1381.723990            353.393412
    42   5632.0    1390.256500   1392.706014            353.460001
    43   5760.0    1396.716363   1404.295962            354.972358
    44   5888.0    1391.840086   1416.506878            355.171470
    45   6016.0    1400.538944   1410.898059            356.526955
    46   6144.0    1410.147874   1434.007799            356.889723
    47   6272.0    1406.281702   1389.682707            357.598295
    48   6400.0    1411.902968   1402.973370            358.473686
    49   6528.0    1415.186515   1424.077157            359.213408
    50   6656.0    1416.464324   1433.094296            359.595114
    51   6784.0    1413.055288   1427.977862            359.728751
    52   6912.0    1421.803617   1442.455221            360.642930
    53   7040.0    1424.526376   1449.296876            360.548722
    54   7168.0    1417.586304   1466.087173            361.449733
    55   7296.0    1427.904287   1084.266164            362.492977
    56   7424.0    1431.103097   1095.175962            362.839996
    57   7552.0    1425.272778   1107.621515            363.349872
    58   7680.0    1431.999186   1124.017961            363.865043
    59   7808.0    1427.947598   1131.188226            364.494052
    60   7936.0    1433.705491   1139.979396            364.609354
    61   8064.0    1430.773164   1145.150346            364.705024
    62   8192.0    1432.790674   1154.137577            364.120495
    63   8320.0    1383.464198   1116.671151            362.007416
    64   8448.0    1386.312424   1121.407185            362.581353
    65   8576.0    1386.206552   1128.184415            363.722511
    66   8704.0    1381.435613   1133.066778            364.868953
    67   8832.0    1398.764264   1134.046867            365.040503
    68   8960.0    1386.357209   1138.396344            365.954342
    69   9088.0    1396.155528   1137.576172            366.737771
    70   9216.0    1405.133142   1142.140915            366.956631
    71   9344.0    1390.080787   1420.259530            367.658148
    72   9472.0    1396.842534   1429.480431            368.851842
    73   9600.0    1403.702441   1428.578677            369.194910
    74   9728.0    1404.518870   1442.327728            370.118702
    75   9856.0    1399.012445   1440.196334            370.075411
    76   9984.0    1394.224642   1447.915933            370.786506
    77  10112.0    1404.277972   1455.899974            371.486396
    78  10240.0    1411.362327   1463.405923            371.794382
    79  10368.0    1418.333212   1459.760460            370.347936
    80  10496.0    1407.432341   1465.071557            369.894229
    81  10624.0    1406.180628   1468.480575            371.063976
    82  10752.0    1394.167505   1470.859813            371.265377
    83  10880.0    1393.209147   1477.916360            371.992307
    84  11008.0    1422.087930   1477.640017            372.363636
    85  11136.0    1420.914514   1485.814916            372.819612
    86  11264.0    1415.612363   1489.629714            372.660114
    87  11392.0    1419.592211   1490.256711            374.566893
    88  11520.0    1414.267547   1494.865765            374.474837
    89  11648.0    1424.428287   1497.733724            374.511622
    90  11776.0    1433.332232   1503.872813            375.112739
    91  11904.0    1428.021894   1507.436782            375.254505
    92  12032.0    1417.242442   1511.164302            375.880592
    93  12160.0    1411.660264   1512.683017            375.908209
    94  12288.0    1424.953790   1418.170457            376.021151
    95  12416.0    1435.934388   1393.073422            374.570861
    96  12544.0    1446.092785   1394.063808            375.432750
    97  12672.0    1439.291944   1390.000654            375.572317




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.489 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

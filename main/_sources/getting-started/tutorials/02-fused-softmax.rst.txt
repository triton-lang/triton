
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     499.081843    708.879234            208.469698
    1     384.0     707.257160    817.946442            263.762684
    2     512.0     846.934635    940.850922            302.319121
    3     640.0     841.950574    911.937235            331.420519
    4     768.0     908.156886    981.315367            350.901647
    5     896.0     977.593657   1034.054025            357.179577
    6    1024.0    1026.091220   1085.894518            354.110603
    7    1152.0    1098.285067   1077.194784            348.985225
    8    1280.0    1146.588500   1104.609437            349.040930
    9    1408.0    1171.059051   1140.245872            340.857854
    10   1536.0    1200.853104   1153.959099            332.880553
    11   1664.0    1225.670539   1185.747487            329.559143
    12   1792.0    1243.381206   1198.339599            325.986567
    13   1920.0    1259.453069   1219.551756            324.724111
    14   2048.0    1274.344366   1244.776191            324.671462
    15   2176.0    1256.793968    960.114723            325.850290
    16   2304.0    1270.296331    999.838440            326.812117
    17   2432.0    1306.050077   1039.933032            327.225623
    18   2560.0    1302.295139   1068.595695            328.043545
    19   2688.0    1326.748784   1101.321114            328.719879
    20   2816.0    1333.608163   1122.338932            329.977137
    21   2944.0    1333.220610   1147.421175            331.718313
    22   3072.0    1347.643459   1169.783584            333.484517
    23   3200.0    1356.509435   1170.140698            335.147687
    24   3328.0    1367.961912   1203.394248            336.310344
    25   3456.0    1376.980557   1226.739964            337.377486
    26   3584.0    1381.741426   1248.059744            338.737885
    27   3712.0    1390.704575   1267.082371            340.822586
    28   3840.0    1392.241855   1282.139835            341.015841
    29   3968.0    1397.068785   1302.249479            341.461534
    30   4096.0    1395.800887   1317.221049            338.726677
    31   4224.0    1334.493364   1280.207295            342.746531
    32   4352.0    1349.186651   1301.776035            345.483995
    33   4480.0    1357.166198   1319.861141            345.942086
    34   4608.0    1367.146709   1337.659471            347.018475
    35   4736.0    1364.885557   1342.206628            348.168788
    36   4864.0    1377.243276   1363.485765            349.244262
    37   4992.0    1377.646344   1372.081353            350.325394
    38   5120.0    1389.631800   1388.857328            351.215972
    39   5248.0    1383.868151   1357.165781            351.990915
    40   5376.0    1394.127304   1362.804523            351.537070
    41   5504.0    1390.867755   1380.394761            353.901289
    42   5632.0    1399.903550   1394.521731            353.245586
    43   5760.0    1400.809741   1404.689138            355.456673
    44   5888.0    1398.247555   1410.104190            354.823112
    45   6016.0    1408.258313   1421.845841            356.876032
    46   6144.0    1423.693100   1436.570925            356.871215
    47   6272.0    1419.055479   1401.610564            358.089546
    48   6400.0    1427.334080   1411.255108            358.895461
    49   6528.0    1425.880248   1412.919180            359.340999
    50   6656.0    1421.830279   1432.661511            359.880494
    51   6784.0    1426.708735   1439.800941            360.017858
    52   6912.0    1433.970546   1441.364952            360.799195
    53   7040.0    1428.817412   1444.422866            360.461732
    54   7168.0    1433.958495   1466.934102            361.796364
    55   7296.0    1431.325449   1086.363777            362.584513
    56   7424.0    1436.666767   1100.795624            363.021782
    57   7552.0    1435.152442   1111.117785            363.801135
    58   7680.0    1438.324144   1119.241091            363.572893
    59   7808.0    1439.040850   1133.316672            364.519325
    60   7936.0    1443.369506   1146.372937            364.545634
    61   8064.0    1439.982909   1148.814032            365.053508
    62   8192.0    1437.970162   1155.389285            364.259019
    63   8320.0    1383.836193   1118.026662            361.690431
    64   8448.0    1387.230994   1121.877534            362.666141
    65   8576.0    1386.925757   1128.466142            363.539595
    66   8704.0    1383.847889   1131.130161            364.471816
    67   8832.0    1392.752311   1135.376702            364.982687
    68   8960.0    1385.999740   1138.461119            365.745386
    69   9088.0    1397.304464   1138.703348            366.955799
    70   9216.0    1404.898150   1143.537320            367.587204
    71   9344.0    1393.029079   1421.808900            367.801432
    72   9472.0    1396.921897   1434.082633            368.801851
    73   9600.0    1401.933244   1430.760841            369.338386
    74   9728.0    1394.531710   1441.185200            370.181261
    75   9856.0    1405.003500   1439.839148            369.865125
    76   9984.0    1398.283644   1450.797834            371.035106
    77  10112.0    1409.433971   1453.525182            371.402008
    78  10240.0    1410.321026   1462.958011            371.435051
    79  10368.0    1418.338300   1462.718940            370.010408
    80  10496.0    1408.081507   1468.547647            370.902090
    81  10624.0    1410.330492   1463.014511            371.056042
    82  10752.0    1396.229178   1471.606558            371.568311
    83  10880.0    1397.086900   1479.828575            372.032058
    84  11008.0    1424.787612   1476.206211            372.851825
    85  11136.0    1418.754527   1484.750120            373.067990
    86  11264.0    1414.184125   1489.404109            373.561289
    87  11392.0    1421.644056   1492.396722            374.222160
    88  11520.0    1419.706040   1495.223497            374.320084
    89  11648.0    1422.373247   1498.069264            375.341313
    90  11776.0    1437.192986   1501.574058            374.819582
    91  11904.0    1427.473851   1509.554302            375.184107
    92  12032.0    1421.420326   1511.485278            376.053097
    93  12160.0    1414.444096   1516.653577            375.872784
    94  12288.0    1427.614093   1421.172520            375.763855
    95  12416.0    1440.298747   1394.273762            374.575278
    96  12544.0    1443.913668   1393.450861            375.775423
    97  12672.0    1438.870257   1393.550432            375.471074




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 37.375 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

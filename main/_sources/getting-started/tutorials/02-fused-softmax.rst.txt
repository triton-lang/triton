
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     468.610158    702.522999            206.058824
    1     384.0     666.135385    816.793736            259.378631
    2     512.0     809.019404    921.830455            301.307973
    3     640.0     917.709353    915.517443            330.255862
    4     768.0     989.649138    985.516691            348.713503
    5     896.0    1041.320475   1041.916548            353.757935
    6    1024.0    1070.520715   1065.594749            352.616200
    7    1152.0    1100.848660   1074.248510            348.232182
    8    1280.0    1129.075484   1110.672483            348.845127
    9    1408.0    1169.887409   1138.708929            340.175833
    10   1536.0    1184.550230   1164.079012            332.937298
    11   1664.0    1214.423577   1181.208152            329.571285
    12   1792.0    1238.076139   1189.503882            325.371028
    13   1920.0    1257.354110   1217.457254            323.666056
    14   2048.0    1267.847870   1240.010796            324.566428
    15   2176.0    1232.208829    958.662473            326.001267
    16   2304.0    1259.960295   1003.910110            326.018981
    17   2432.0    1269.282839   1030.595312            326.459133
    18   2560.0    1286.283574   1072.385946            328.076482
    19   2688.0    1298.354705   1101.620480            328.519828
    20   2816.0    1315.256306   1120.158577            329.448737
    21   2944.0    1318.527259   1144.816014            330.945076
    22   3072.0    1318.355327   1173.219106            332.702528
    23   3200.0    1335.652195   1174.388515            335.019711
    24   3328.0    1345.332740   1195.434831            335.917735
    25   3456.0    1350.522007   1218.373382            336.966314
    26   3584.0    1362.893505   1243.510445            338.348882
    27   3712.0    1363.846080   1264.248821            340.432719
    28   3840.0    1370.093040   1283.118672            340.094371
    29   3968.0    1376.637071   1301.060402            341.224254
    30   4096.0    1392.540970   1313.612704            338.583539
    31   4224.0    1338.569113   1278.459164            343.065991
    32   4352.0    1339.678548   1297.127385            345.325535
    33   4480.0    1341.726471   1313.323502            345.299811
    34   4608.0    1358.182405   1336.448493            346.965071
    35   4736.0    1356.976108   1346.036716            347.873438
    36   4864.0    1367.460384   1361.717555            349.202703
    37   4992.0    1366.026226   1370.805196            350.301387
    38   5120.0    1374.994901   1388.116112            351.068254
    39   5248.0    1369.467841   1354.229478            351.776854
    40   5376.0    1381.207029   1367.727789            351.897748
    41   5504.0    1378.740655   1382.831530            353.355850
    42   5632.0    1398.493631   1393.031924            353.217704
    43   5760.0    1385.612880   1395.550406            354.935160
    44   5888.0    1394.189732   1411.140733            354.507850
    45   6016.0    1398.047480   1424.199169            356.489760
    46   6144.0    1408.245834   1426.377565            356.783325
    47   6272.0    1405.608554   1400.597738            357.825851
    48   6400.0    1416.018134   1417.829354            358.484351
    49   6528.0    1413.055839   1412.390474            359.508391
    50   6656.0    1412.041646   1432.384212            359.480168
    51   6784.0    1416.516457   1430.841617            360.206258
    52   6912.0    1419.994958   1447.539292            360.987809
    53   7040.0    1417.635870   1451.941872            360.910880
    54   7168.0    1421.359196   1453.961144            361.185644
    55   7296.0    1424.255215   1084.766232            362.393482
    56   7424.0    1427.168966   1101.781401            362.640241
    57   7552.0    1429.322882   1109.417500            363.213347
    58   7680.0    1429.930337   1124.499524            363.943901
    59   7808.0    1431.682321   1133.114182            364.316896
    60   7936.0    1436.195742   1140.066555            364.531987
    61   8064.0    1437.029743   1148.678437            365.067099
    62   8192.0    1431.436052   1150.044269            363.950216
    63   8320.0    1379.740259   1118.768742            361.922417
    64   8448.0    1384.382781   1121.875112            362.492144
    65   8576.0    1387.713751   1127.342605            363.526221
    66   8704.0    1383.733218   1130.813239            364.355963
    67   8832.0    1392.236456   1134.330893            365.258587
    68   8960.0    1389.509786   1136.645507            366.007731
    69   9088.0    1396.872212   1138.827464            366.980483
    70   9216.0    1405.925750   1141.650787            367.538276
    71   9344.0    1388.658817   1423.897836            367.810393
    72   9472.0    1396.406528   1429.276165            369.135375
    73   9600.0    1400.846350   1432.907248            369.244217
    74   9728.0    1397.015162   1441.832016            370.069000
    75   9856.0    1401.442049   1436.689397            369.860654
    76   9984.0    1396.583376   1449.772731            370.671197
    77  10112.0    1405.421420   1454.563505            371.464185
    78  10240.0    1412.086826   1466.040426            371.426187
    79  10368.0    1416.184308   1458.267365            370.054785
    80  10496.0    1409.080706   1464.061879            370.102455
    81  10624.0    1402.910031   1467.890056            371.055076
    82  10752.0    1398.318354   1471.195558            371.779996
    83  10880.0    1392.351933   1479.165239            371.956981
    84  11008.0    1417.220567   1473.949699            372.385751
    85  11136.0    1419.245804   1486.297477            372.691117
    86  11264.0    1415.877158   1486.522257            372.739843
    87  11392.0    1420.309319   1491.698994            374.195668
    88  11520.0    1416.670006   1494.785741            373.372277
    89  11648.0    1420.776592   1497.080791            373.972304
    90  11776.0    1432.650307   1500.702487            374.877288
    91  11904.0    1426.038162   1508.789612            375.668634
    92  12032.0    1418.184204   1508.928383            376.633695
    93  12160.0    1412.087622   1513.095906            375.713451
    94  12288.0    1426.098465   1420.414546            375.972327
    95  12416.0    1433.430258   1392.507495            374.478187
    96  12544.0    1443.536024   1392.525032            375.301118
    97  12672.0    1434.399272   1391.018765            375.532692




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.022 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_


.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   473.028861   693.449967     208.132464
    1     384.0   666.913787   821.722835     263.007197
    2     512.0   814.681617   937.435857     303.485931
    3     640.0   925.184478   921.120569     332.968575
    4     768.0   981.046982   974.982848     349.709408
    5     896.0  1035.561707  1036.601663     355.008908
    6    1024.0  1071.670016  1068.878908     354.390369
    7    1152.0  1090.266437  1067.570920     349.806961
    8    1280.0  1130.280042  1114.345266     349.014430
    9    1408.0  1162.607898  1141.291161     341.769717
    10   1536.0  1190.027062  1158.194676     333.469078
    11   1664.0  1212.075678  1191.484268     329.716116
    12   1792.0  1239.370297  1192.786411     326.048064
    13   1920.0  1260.460605  1226.601634     324.509171
    14   2048.0  1277.586994  1246.038581     325.141398
    15   2176.0  1229.467288   960.252131     325.898898
    16   2304.0  1250.338256  1004.830010     326.018982
    17   2432.0  1279.168240  1033.693165     326.962977
    18   2560.0  1289.181678  1068.130368     328.350916
    19   2688.0  1295.044183  1101.476683     329.557683
    20   2816.0  1314.332565  1121.871989     329.672539
    21   2944.0  1313.251073  1143.234920     331.812001
    22   3072.0  1319.769829  1170.501763     333.918443
    23   3200.0  1336.797657  1168.453152     334.688079
    24   3328.0  1350.061185  1203.706006     336.702974
    25   3456.0  1347.476501  1221.133971     336.836977
    26   3584.0  1358.233559  1242.583378     338.166448
    27   3712.0  1369.328718  1261.131346     340.882848
    28   3840.0  1374.204214  1282.214690     340.438240
    29   3968.0  1375.711358  1296.969649     340.952695
    30   4096.0  1387.177739  1315.463779     339.162252
    31   4224.0  1330.393723  1275.423797     342.715091
    32   4352.0  1341.090695  1297.974401     345.442323
    33   4480.0  1347.292519  1313.243400     345.572394
    34   4608.0  1356.229013  1335.716689     346.706163
    35   4736.0  1356.651889  1340.098839     347.669713
    36   4864.0  1370.715079  1355.409231     349.249190
    37   4992.0  1374.083320  1371.128436     350.306188
    38   5120.0  1373.058610  1386.888031     351.208716
    39   5248.0  1370.706310  1353.999502     351.765402
    40   5376.0  1377.138669  1369.730515     351.779857
    41   5504.0  1384.437721  1384.919326     353.271362
    42   5632.0  1398.129931  1396.101434     353.454814
    43   5760.0  1391.450890  1408.715911     355.074699
    44   5888.0  1391.775830  1410.081052     354.725690
    45   6016.0  1399.482955  1424.511794     357.062489
    46   6144.0  1410.273917  1429.445768     357.074918
    47   6272.0  1406.373798  1400.818079     357.816602
    48   6400.0  1412.166435  1414.264601     358.263017
    49   6528.0  1416.519317  1414.343829     359.218019
    50   6656.0  1416.617006  1430.709776     359.392860
    51   6784.0  1418.666444  1428.091990     360.321235
    52   6912.0  1424.353700  1441.294698     361.033843
    53   7040.0  1419.684290  1454.666741     360.347330
    54   7168.0  1420.149778  1457.830114     361.997349
    55   7296.0  1427.226029  1082.116204     362.223209
    56   7424.0  1432.001609  1097.168367     362.849080
    57   7552.0  1424.025201  1107.001586     363.577644
    58   7680.0  1435.519778  1119.106818     363.939372
    59   7808.0  1432.923022  1130.117780     364.217037
    60   7936.0  1432.030905  1140.245345     364.559287
    61   8064.0  1435.239956  1146.569986     364.488123
    62   8192.0  1434.278747  1149.348783     363.845241
    63   8320.0  1384.339896  1117.306248     361.416183
    64   8448.0  1385.998758  1121.446951     362.278225
    65   8576.0  1388.086749  1128.338741     363.628800
    66   8704.0  1380.655506  1133.617719     364.775169
    67   8832.0  1390.569513  1129.770711     365.084989
    68   8960.0  1387.363443  1139.240382     365.927654
    69   9088.0  1395.821946  1135.832223     367.196377
    70   9216.0  1408.062063  1143.614461     367.338268
    71   9344.0  1386.259411  1418.207154     367.649194
    72   9472.0  1398.487239  1431.411131     368.528944
    73   9600.0  1400.850978  1434.375829     369.118736
    74   9728.0  1400.372867  1436.395180     369.674735
    75   9856.0  1399.437704  1442.461727     370.268008
    76   9984.0  1391.405022  1451.588604     370.662608
    77  10112.0  1408.534795  1455.843663     371.273279
    78  10240.0  1412.092137  1465.771104     371.342004
    79  10368.0  1418.401791  1460.266382     369.899513
    80  10496.0  1411.202588  1469.194882     370.519613
    81  10624.0  1405.906737  1466.741387     370.890486
    82  10752.0  1396.899724  1474.118427     371.475636
    83  10880.0  1399.352149  1479.492095     372.186720
    84  11008.0  1426.504727  1477.114209     372.806430
    85  11136.0  1420.709201  1485.583231     373.125697
    86  11264.0  1409.538803  1487.955523     373.641405
    87  11392.0  1426.110508  1490.990052     374.182423
    88  11520.0  1412.295651  1496.819758     374.156632
    89  11648.0  1424.397169  1498.837878     375.025863
    90  11776.0  1432.797193  1499.947499     375.557792
    91  11904.0  1429.848164  1507.605730     375.927703
    92  12032.0  1416.377655  1511.073186     376.473962
    93  12160.0  1411.234437  1516.351098     376.271704
    94  12288.0  1422.694974  1421.826138     376.381072
    95  12416.0  1439.584526  1397.579908     375.089121
    96  12544.0  1445.052959  1395.829959     376.167187
    97  12672.0  1434.550308  1393.398263     375.466671




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.778 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

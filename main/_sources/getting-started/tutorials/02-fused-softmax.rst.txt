
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     473.045639    702.171420            208.314332
    1     384.0     665.991701    821.354373            264.373527
    2     512.0     813.806502    930.238483            302.856676
    3     640.0     908.913407    922.607617            331.009327
    4     768.0     979.403687    991.245792            349.699797
    5     896.0    1047.260707   1027.984937            353.717858
    6    1024.0    1072.513261   1065.017575            351.707716
    7    1152.0    1100.448711   1069.328278            349.744922
    8    1280.0    1130.991336   1104.436297            348.390712
    9    1408.0    1171.856169   1143.762651            341.772185
    10   1536.0    1191.259372   1164.387104            333.905348
    11   1664.0    1210.585568   1184.358753            329.147615
    12   1792.0    1227.201078   1199.322402            326.161869
    13   1920.0    1267.344955   1220.308707            325.171296
    14   2048.0    1270.152615   1246.201477            324.473001
    15   2176.0    1240.330024    960.115696            325.471164
    16   2304.0    1252.086910    998.761625            325.409837
    17   2432.0    1276.234178   1030.285231            326.944832
    18   2560.0    1285.746731   1068.050990            327.985365
    19   2688.0    1294.010305   1100.757938            328.960668
    20   2816.0    1311.214771   1122.462144            329.840089
    21   2944.0    1314.748589   1147.577812            331.944357
    22   3072.0    1325.749405   1171.379449            333.147026
    23   3200.0    1338.562106   1175.779933            335.270526
    24   3328.0    1344.410033   1202.829547            336.450872
    25   3456.0    1352.320220   1223.321908            337.050760
    26   3584.0    1360.004812   1243.965393            337.679741
    27   3712.0    1368.050164   1269.291665            340.638670
    28   3840.0    1366.398039   1284.737518            339.970971
    29   3968.0    1369.348990   1303.228538            341.257311
    30   4096.0    1382.499348   1316.251090            338.951088
    31   4224.0    1330.339166   1276.511346            343.222088
    32   4352.0    1342.163016   1301.051471            345.658522
    33   4480.0    1348.852758   1317.287103            345.647845
    34   4608.0    1361.610779   1332.778120            347.322312
    35   4736.0    1360.875087   1349.445377            348.233332
    36   4864.0    1372.919824   1358.101461            348.865085
    37   4992.0    1367.348273   1377.103417            350.210191
    38   5120.0    1376.334639   1385.677059            350.404281
    39   5248.0    1374.036731   1356.047651            351.825955
    40   5376.0    1375.957597   1367.986571            351.653272
    41   5504.0    1377.739242   1375.261148            354.330359
    42   5632.0    1396.563543   1392.228687            353.417851
    43   5760.0    1397.679199   1407.373604            355.079354
    44   5888.0    1389.282176   1413.329094            354.652748
    45   6016.0    1402.535967   1428.338102            356.829451
    46   6144.0    1409.723727   1424.422431            356.797199
    47   6272.0    1410.752892   1401.905262            357.775006
    48   6400.0    1409.552837   1403.004190            358.309104
    49   6528.0    1413.184343   1412.781161            359.183048
    50   6656.0    1413.906676   1431.657542            359.590513
    51   6784.0    1418.144070   1427.858275            360.114332
    52   6912.0    1423.771065   1455.471862            360.776206
    53   7040.0    1419.358470   1458.123000            361.034750
    54   7168.0    1421.849660   1455.135329            361.294875
    55   7296.0    1424.607539   1087.806676            362.383193
    56   7424.0    1429.952493   1099.301155            363.180993
    57   7552.0    1429.404554   1109.247757            363.586759
    58   7680.0    1432.743645   1121.389326            363.694971
    59   7808.0    1433.685993   1128.886579            364.521323
    60   7936.0    1436.455010   1138.487620            364.531985
    61   8064.0    1435.883316   1147.266285            365.094285
    62   8192.0    1432.957993   1153.616767            364.424825
    63   8320.0    1382.766818   1118.429002            362.142516
    64   8448.0    1385.709131   1123.329576            362.729900
    65   8576.0    1385.757097   1126.678539            363.352410
    66   8704.0    1381.794226   1134.033965            364.578824
    67   8832.0    1392.661910   1131.735739            364.893778
    68   8960.0    1389.656106   1137.852712            366.021081
    69   9088.0    1402.016957   1136.279749            366.911283
    70   9216.0    1401.032721   1142.502048            367.487100
    71   9344.0    1390.555227   1420.389446            367.730304
    72   9472.0    1400.377832   1431.630944            368.811447
    73   9600.0    1404.470253   1426.163325            369.199706
    74   9728.0    1401.957482   1441.237325            369.976517
    75   9856.0    1400.161398   1441.913846            370.262658
    76   9984.0    1391.806315   1449.318537            370.764326
    77  10112.0    1408.793131   1454.335652            371.284257
    78  10240.0    1412.009722   1468.315382            371.732226
    79  10368.0    1413.316291   1460.219488            370.179095
    80  10496.0    1408.534482   1466.066740            370.590711
    81  10624.0    1405.806888   1463.360867            370.766026
    82  10752.0    1394.276143   1469.526665            371.140782
    83  10880.0    1401.528324   1478.831309            372.009972
    84  11008.0    1419.277057   1479.415556            373.170308
    85  11136.0    1416.074220   1483.714983            372.930452
    86  11264.0    1418.247878   1485.488600            372.837341
    87  11392.0    1422.821053   1491.257052            374.147111
    88  11520.0    1412.413795   1493.351335            374.311246
    89  11648.0    1428.179601   1499.114165            374.316934
    90  11776.0    1434.831267   1500.256241            374.988313
    91  11904.0    1432.917166   1509.203347            375.329704
    92  12032.0    1417.185007   1509.823400            375.831964
    93  12160.0    1413.637738   1514.605620            376.631456
    94  12288.0    1424.507968   1419.804037            375.741689
    95  12416.0    1438.084915   1395.769042            375.065898
    96  12544.0    1445.965559   1391.329779            375.476647
    97  12672.0    1439.985667   1391.853963            375.400673




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.949 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

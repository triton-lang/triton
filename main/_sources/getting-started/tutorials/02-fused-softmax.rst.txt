
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     479.340821    692.750767            208.142367
    1     384.0     658.424511    830.928633            262.057985
    2     512.0     804.613845    918.259317            300.490994
    3     640.0     919.726772    926.271205            329.671714
    4     768.0     985.103049    990.577378            348.282928
    5     896.0    1040.635372   1041.074350            356.437112
    6    1024.0    1070.874950   1071.949935            354.120419
    7    1152.0    1095.940194   1066.256805            348.122790
    8    1280.0    1131.678329   1102.456027            349.027445
    9    1408.0    1157.910008   1140.630755            339.889052
    10   1536.0    1185.871035   1164.835076            333.572349
    11   1664.0    1209.949751   1185.344226            329.770308
    12   1792.0    1238.042504   1200.572957            325.721508
    13   1920.0    1261.023110   1227.187294            324.189475
    14   2048.0    1269.042567   1250.098099            324.331065
    15   2176.0    1236.948549    959.166354            326.015896
    16   2304.0    1252.333778   1004.418976            326.133481
    17   2432.0    1279.281028   1034.694744            327.130787
    18   2560.0    1282.044079   1071.827586            328.048334
    19   2688.0    1299.735375   1096.436711            328.933082
    20   2816.0    1312.234089   1126.717712            329.369215
    21   2944.0    1314.441588   1147.120253            331.985017
    22   3072.0    1318.259045   1170.081876            333.572576
    23   3200.0    1341.703347   1171.096375            334.954822
    24   3328.0    1350.468508   1200.235288            336.244167
    25   3456.0    1354.694286   1222.672611            336.773449
    26   3584.0    1363.827746   1242.443002            338.296927
    27   3712.0    1369.298756   1263.016492            340.903775
    28   3840.0    1374.881101   1283.591589            340.444496
    29   3968.0    1375.990823   1298.413498            340.887809
    30   4096.0    1391.344743   1315.867600            338.928425
    31   4224.0    1325.296798   1279.236300            342.771082
    32   4352.0    1342.445820   1301.236902            345.568237
    33   4480.0    1352.347976   1317.056238            345.973612
    34   4608.0    1359.059014   1332.241109            347.033996
    35   4736.0    1358.625594   1348.340316            348.148026
    36   4864.0    1366.366562   1357.470383            349.162693
    37   4992.0    1373.270324   1372.878876            350.402240
    38   5120.0    1376.908215   1389.914208            351.069742
    39   5248.0    1371.248793   1355.354862            351.474625
    40   5376.0    1379.204927   1368.247471            351.714360
    41   5504.0    1384.199313   1379.572101            353.468560
    42   5632.0    1391.724302   1388.688584            353.595885
    43   5760.0    1390.404410   1403.969564            355.135201
    44   5888.0    1397.271335   1416.976887            354.855432
    45   6016.0    1398.867086   1413.517777            356.354992
    46   6144.0    1407.471349   1433.207882            357.098078
    47   6272.0    1403.708516   1389.315913            358.149740
    48   6400.0    1415.678681   1407.545507            358.488964
    49   6528.0    1417.433682   1423.281887            358.872616
    50   6656.0    1415.788825   1424.302307            359.765226
    51   6784.0    1418.245852   1438.603498            360.307432
    52   6912.0    1419.678483   1438.638200            360.905631
    53   7040.0    1423.451957   1453.981614            360.734314
    54   7168.0    1422.289021   1455.529918            361.695955
    55   7296.0    1423.288318   1084.688096            362.606815
    56   7424.0    1427.756722   1096.355263            362.803662
    57   7552.0    1424.699224   1106.493791            363.450057
    58   7680.0    1431.544322   1118.621984            363.769193
    59   7808.0    1429.076641   1132.064197            364.584968
    60   7936.0    1433.869693   1140.272842            364.704971
    61   8064.0    1434.391238   1149.492485            364.976512
    62   8192.0    1430.817325   1154.297349            364.106787
    63   8320.0    1382.802124   1117.750137            361.584223
    64   8448.0    1384.258245   1122.041483            362.590277
    65   8576.0    1388.630415   1128.236245            363.392505
    66   8704.0    1385.092036   1131.999456            364.449532
    67   8832.0    1397.713392   1132.079976            365.102787
    68   8960.0    1383.783784   1142.405967            365.598814
    69   9088.0    1398.798000   1138.325621            367.022594
    70   9216.0    1402.057449   1143.278088            367.608355
    71   9344.0    1391.868428   1423.190127            367.873118
    72   9472.0    1398.481747   1429.735839            368.779640
    73   9600.0    1403.269499   1434.290790            369.168021
    74   9728.0    1394.669840   1442.976890            369.692474
    75   9856.0    1400.684175   1441.594820            370.097794
    76   9984.0    1396.540127   1448.520919            370.779112
    77  10112.0    1404.702192   1455.667721            370.939280
    78  10240.0    1413.358677   1467.013352            371.519276
    79  10368.0    1416.640369   1460.588052            369.970479
    80  10496.0    1408.231592   1465.461264            370.657392
    81  10624.0    1407.795461   1464.922841            371.344576
    82  10752.0    1399.168890   1470.865874            371.568312
    83  10880.0    1389.155339   1481.964182            371.961396
    84  11008.0    1419.928089   1479.190106            372.744375
    85  11136.0    1419.239532   1483.080715            373.063551
    86  11264.0    1415.591152   1486.743427            373.476756
    87  11392.0    1422.488998   1489.517414            373.719457
    88  11520.0    1412.978711   1494.094352            374.090407
    89  11648.0    1418.306870   1499.199005            374.387706
    90  11776.0    1432.580280   1500.769999            374.704220
    91  11904.0    1433.689077   1508.638131            375.722206
    92  12032.0    1418.006295   1508.599993            376.239048
    93  12160.0    1415.381342   1513.318641            375.660371
    94  12288.0    1427.089455   1422.601685            375.781585
    95  12416.0    1440.955729   1393.881572            374.438484
    96  12544.0    1448.659842   1396.277968            375.854591
    97  12672.0    1437.980839   1392.701386            375.453470




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.922 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

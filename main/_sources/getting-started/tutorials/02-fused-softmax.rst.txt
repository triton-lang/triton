
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     469.599989    702.493589            204.299836
    1     384.0     656.198177    828.116094            261.050070
    2     512.0     802.051770    916.992191            303.981595
    3     640.0     924.623109    931.635450            330.854007
    4     768.0     982.488430    988.610605            349.857942
    5     896.0    1043.666747   1035.611153            352.787479
    6    1024.0    1068.626771   1075.993335            352.298764
    7    1152.0    1092.116283   1077.685416            349.114465
    8    1280.0    1127.474013   1105.076750            350.077464
    9    1408.0    1167.583068   1142.007464            341.755152
    10   1536.0    1187.805948   1154.614369            332.986492
    11   1664.0    1219.819726   1181.844661            329.617536
    12   1792.0    1236.385160   1190.330876            324.988868
    13   1920.0    1264.023414   1226.963406            325.265587
    14   2048.0    1269.836166   1248.817633            324.688489
    15   2176.0    1239.782756    959.223135            325.330351
    16   2304.0    1257.315952   1003.507094            325.965758
    17   2432.0    1273.101305   1033.785258            326.991597
    18   2560.0    1286.670532   1068.363784            328.109838
    19   2688.0    1290.632919   1098.354710            329.308711
    20   2816.0    1314.539514   1127.476612            329.360067
    21   2944.0    1317.140631   1146.803676            331.556913
    22   3072.0    1322.604247   1174.043614            333.250189
    23   3200.0    1341.442889   1170.123416            334.955488
    24   3328.0    1350.638507   1200.670376            336.453938
    25   3456.0    1350.639926   1221.220225            336.698949
    26   3584.0    1358.002852   1246.542087            338.051875
    27   3712.0    1370.475367   1262.634177            340.436592
    28   3840.0    1369.204404   1284.265850            340.488685
    29   3968.0    1377.586795   1295.945456            341.223661
    30   4096.0    1388.109854   1317.631643            338.971010
    31   4224.0    1336.657414   1279.362367            342.621530
    32   4352.0    1337.804723   1295.816585            345.361214
    33   4480.0    1346.573518   1317.043417            345.255627
    34   4608.0    1355.844750   1336.231310            347.198941
    35   4736.0    1357.697074   1349.106111            348.140086
    36   4864.0    1371.584285   1356.409557            348.751480
    37   4992.0    1372.836238   1369.735443            349.815694
    38   5120.0    1375.098523   1388.347021            350.872920
    39   5248.0    1374.581994   1353.542865            352.014375
    40   5376.0    1375.702460   1363.200410            351.629783
    41   5504.0    1381.173055   1382.383104            354.030811
    42   5632.0    1391.094356   1393.039571            353.445509
    43   5760.0    1397.253706   1408.440028            354.980545
    44   5888.0    1390.508114   1418.340595            354.373574
    45   6016.0    1402.939104   1424.236070            356.629281
    46   6144.0    1408.641378   1420.637544            356.912862
    47   6272.0    1406.813905   1399.964477            357.491361
    48   6400.0    1409.536858   1404.243420            358.258409
    49   6528.0    1415.523529   1412.608327            359.029116
    50   6656.0    1416.235352   1439.104906            359.585915
    51   6784.0    1415.731556   1434.357285            360.275235
    52   6912.0    1422.317205   1444.529414            360.854379
    53   7040.0    1419.094098   1450.832237            360.539564
    54   7168.0    1419.722788   1456.443918            361.869424
    55   7296.0    1422.926338   1083.763851            362.561624
    56   7424.0    1428.245113   1101.732020            362.903602
    57   7552.0    1425.407667   1109.165496            363.295250
    58   7680.0    1433.335480   1119.423050            363.450057
    59   7808.0    1433.944015   1132.246162            364.335058
    60   7936.0    1432.297995   1141.136410            364.764189
    61   8064.0    1431.145018   1150.783244            364.922181
    62   8192.0    1430.533644   1149.774080            364.188245
    63   8320.0    1382.145895   1118.058938            361.818849
    64   8448.0    1387.083172   1126.571049            362.265916
    65   8576.0    1386.950738   1128.383940            363.414784
    66   8704.0    1384.461431   1134.920418            364.427250
    67   8832.0    1393.844533   1132.780439            365.280855
    68   8960.0    1389.126813   1141.197679            366.034433
    69   9088.0    1399.691247   1137.250586            366.675007
    70   9216.0    1403.324817   1143.368960            367.756316
    71   9344.0    1390.734886   1424.468777            367.876676
    72   9472.0    1397.941226   1433.545774            368.828511
    73   9600.0    1400.274595   1432.167837            369.195270
    74   9728.0    1401.066436   1439.776614            369.750135
    75   9856.0    1397.890257   1435.578169            368.999663
    76   9984.0    1392.092393   1450.288151            370.461141
    77  10112.0    1404.012795   1452.392476            371.450860
    78  10240.0    1408.246668   1465.916152            371.590232
    79  10368.0    1417.055190   1460.773504            370.170212
    80  10496.0    1406.630061   1464.815954            369.885372
    81  10624.0    1410.257186   1468.239836            370.894932
    82  10752.0    1400.319867   1471.545976            372.036218
    83  10880.0    1393.937802   1479.381030            372.266312
    84  11008.0    1421.750864   1477.847692            373.028223
    85  11136.0    1411.960757   1482.858172            372.695547
    86  11264.0    1410.755389   1486.471082            373.319019
    87  11392.0    1422.897945   1488.574253            373.803147
    88  11520.0    1416.722911   1495.877777            373.517442
    89  11648.0    1422.656494   1501.332966            374.781866
    90  11776.0    1435.174285   1502.261042            374.549039
    91  11904.0    1430.205305   1508.638332            375.632928
    92  12032.0    1416.860361   1506.761234            375.672913
    93  12160.0    1412.569844   1514.480117            376.836070
    94  12288.0    1427.752436   1421.471134            376.164722
    95  12416.0    1438.431444   1397.850078            375.181004
    96  12544.0    1445.095074   1395.975152            375.257262
    97  12672.0    1437.652034   1395.608754            375.027118




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.065 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

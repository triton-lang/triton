
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     477.445382    690.026943            208.907743
    1     384.0     662.828917    810.445598            264.928988
    2     512.0     815.138098    924.383130            301.553411
    3     640.0     828.662125    915.447370            329.095113
    4     768.0     899.528239    984.552051            348.242905
    5     896.0     955.505024   1025.105472            355.285234
    6    1024.0    1009.458121   1081.503511            354.436112
    7    1152.0    1103.451739   1074.573309            348.015241
    8    1280.0    1139.730560   1108.991521            348.344766
    9    1408.0    1171.627481   1137.155442            340.806727
    10   1536.0    1195.330757   1159.811234            333.764962
    11   1664.0    1216.441372   1183.027544            329.818509
    12   1792.0    1230.156082   1200.504109            325.950945
    13   1920.0    1252.999571   1218.407722            325.002365
    14   2048.0    1265.077987   1251.394636            324.368226
    15   2176.0    1238.286170    964.005539            326.081812
    16   2304.0    1251.893635   1004.818538            326.659006
    17   2432.0    1276.802279   1039.103213            327.476892
    18   2560.0    1283.119718   1071.006668            328.207424
    19   2688.0    1294.665171   1096.435598            329.342811
    20   2816.0    1306.456477   1122.988171            329.769032
    21   2944.0    1317.726005   1143.464246            332.050835
    22   3072.0    1325.703352   1171.634472            333.730818
    23   3200.0    1336.436269   1169.764232            335.249183
    24   3328.0    1344.089126   1198.932739            336.127119
    25   3456.0    1353.912160   1223.376481            336.981614
    26   3584.0    1356.044807   1247.680213            338.193176
    27   3712.0    1372.161195   1268.669600            340.938374
    28   3840.0    1375.975784   1281.984586            340.280811
    29   3968.0    1371.161303   1297.937186            340.762694
    30   4096.0    1384.188445   1315.721118            338.721567
    31   4224.0    1328.704506   1275.543914            342.802488
    32   4352.0    1340.984556   1299.079033            345.292483
    33   4480.0    1342.690231   1321.548100            345.936127
    34   4608.0    1357.451665   1336.219817            346.615473
    35   4736.0    1353.348226   1343.072442            347.894942
    36   4864.0    1368.881054   1363.103478            348.976826
    37   4992.0    1366.955090   1372.832470            350.433267
    38   5120.0    1376.341420   1384.004926            351.047136
    39   5248.0    1376.294363   1356.686549            351.797608
    40   5376.0    1380.401346   1362.865344            351.165340
    41   5504.0    1379.078304   1383.591496            353.577263
    42   5632.0    1395.956675   1397.518636            353.450161
    43   5760.0    1393.764068   1407.958921            355.195723
    44   5888.0    1385.880656   1413.620419            354.716416
    45   6016.0    1398.069364   1413.594190            356.782878
    46   6144.0    1403.905501   1430.789157            356.566099
    47   6272.0    1412.512572   1397.070525            357.807359
    48   6400.0    1416.119333   1405.502665            359.062022
    49   6528.0    1415.412399   1425.714175            359.374818
    50   6656.0    1416.828431   1423.399949            359.135771
    51   6784.0    1418.995110   1433.463059            360.128117
    52   6912.0    1419.403331   1448.882112            361.167409
    53   7040.0    1417.764624   1445.271611            361.163446
    54   7168.0    1420.226201   1469.440733            361.947083
    55   7296.0    1425.790481   1086.122104            362.314611
    56   7424.0    1429.499317   1094.950997            362.844539
    57   7552.0    1429.227820   1106.099061            363.404511
    58   7680.0    1428.348724   1119.197959            363.880503
    59   7808.0    1433.170229   1133.820711            364.525867
    60   7936.0    1434.353333   1139.804968            364.598091
    61   8064.0    1430.178663   1146.337584            364.700502
    62   8192.0    1434.370910   1150.617863            363.825050
    63   8320.0    1377.808682   1118.279577            362.053782
    64   8448.0    1382.287805   1123.007127            362.685660
    65   8576.0    1382.425803   1127.878107            363.704658
    66   8704.0    1377.160094   1133.186534            364.396056
    67   8832.0    1386.441508   1132.864289            365.178444
    68   8960.0    1382.387608   1140.227100            365.780935
    69   9088.0    1393.026268   1137.436921            367.205292
    70   9216.0    1400.414045   1142.086071            367.613895
    71   9344.0    1387.433839   1421.781144            367.014742
    72   9472.0    1392.941181   1430.563170            368.468929
    73   9600.0    1396.817155   1431.156919            369.172504
    74   9728.0    1393.965435   1440.243566            368.999189
    75   9856.0    1397.617697   1441.075404            370.200572
    76   9984.0    1387.167149   1450.230544            370.919642
    77  10112.0    1404.767959   1454.335849            371.486397
    78  10240.0    1405.420179   1462.984709            371.891578
    79  10368.0    1409.804457   1463.700075            370.094732
    80  10496.0    1407.032137   1465.738858            370.510726
    81  10624.0    1407.722151   1464.868036            371.344577
    82  10752.0    1391.060867   1469.249445            370.763060
    83  10880.0    1393.396590   1478.399988            372.095346
    84  11008.0    1412.467268   1474.496409            372.531773
    85  11136.0    1413.391118   1486.630713            372.739846
    86  11264.0    1406.426875   1489.978019            372.930452
    87  11392.0    1415.184325   1489.048949            373.723862
    88  11520.0    1412.130281   1494.594983            373.425052
    89  11648.0    1420.777540   1498.417282            374.733104
    90  11776.0    1429.766348   1502.188272            375.006080
    91  11904.0    1427.508150   1508.836485            375.531960
    92  12032.0    1412.052628   1512.121738            375.924809
    93  12160.0    1410.020640   1512.199705            375.881640
    94  12288.0    1423.336076   1418.788540            376.136607
    95  12416.0    1435.241467   1391.968156            374.769603
    96  12544.0    1439.290216   1391.675878            375.380086
    97  12672.0    1429.648815   1392.071154            375.647184




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.214 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

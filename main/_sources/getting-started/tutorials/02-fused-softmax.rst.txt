
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     470.649762    700.949859            206.173204
    1     384.0     663.029681    826.220171            262.469004
    2     512.0     803.456608    927.075005            302.209760
    3     640.0     918.493090    921.898901            329.858990
    4     768.0     977.213978    989.025756            348.297607
    5     896.0    1034.866612   1040.582724            353.767810
    6    1024.0    1078.113582   1065.344608            353.829691
    7    1152.0    1086.496579   1065.479294            349.259196
    8    1280.0    1125.932590   1111.913929            347.673531
    9    1408.0    1167.015673   1139.202495            340.962254
    10   1536.0    1188.409982   1164.643071            333.535395
    11   1664.0    1209.064786   1184.286940            329.054835
    12   1792.0    1228.437550   1190.672105            326.068362
    13   1920.0    1260.505289   1219.272911            324.406371
    14   2048.0    1267.226235   1250.418015            324.768286
    15   2176.0    1238.232400    964.456167            325.521988
    16   2304.0    1253.503034   1003.430025            325.727736
    17   2432.0    1273.504049   1037.684051            326.357395
    18   2560.0    1284.799543   1067.307162            327.735784
    19   2688.0    1291.858488   1100.379842            329.289418
    20   2816.0    1312.133720   1125.910218            329.348313
    21   2944.0    1320.643951   1146.666527            331.786725
    22   3072.0    1317.221654   1169.859724            333.653387
    23   3200.0    1340.747461   1170.712017            334.771389
    24   3328.0    1342.413706   1199.748341            336.194267
    25   3456.0    1346.734387   1219.557913            336.906164
    26   3584.0    1363.754362   1246.676085            338.075955
    27   3712.0    1372.097955   1265.363081            340.486732
    28   3840.0    1373.548130   1277.103605            340.175029
    29   3968.0    1370.577524   1301.165107            340.809118
    30   4096.0    1385.861311   1317.711977            338.542348
    31   4224.0    1329.880561   1279.341515            342.819462
    32   4352.0    1342.089177   1301.404346            344.927506
    33   4480.0    1348.985185   1317.647053            345.782080
    34   4608.0    1362.871896   1335.110871            347.253743
    35   4736.0    1358.201449   1343.962762            347.968114
    36   4864.0    1371.331171   1360.546161            349.226736
    37   4992.0    1367.327734   1374.470333            350.199136
    38   5120.0    1375.208141   1389.027364            350.559557
    39   5248.0    1373.913207   1355.764351            351.631486
    40   5376.0    1383.972691   1367.051161            351.274364
    41   5504.0    1382.700590   1385.133328            353.492052
    42   5632.0    1396.137062   1396.234010            353.073736
    43   5760.0    1395.463935   1404.902984            355.093315
    44   5888.0    1388.555701   1409.028959            354.735638
    45   6016.0    1402.192324   1426.750032            356.396806
    46   6144.0    1411.710475   1434.162372            356.843455
    47   6272.0    1410.886035   1397.722770            357.225889
    48   6400.0    1411.118307   1410.271740            358.719816
    49   6528.0    1414.051490   1419.019739            358.803614
    50   6656.0    1412.683963   1423.556478            359.177066
    51   6784.0    1413.039678   1440.849546            360.040824
    52   6912.0    1422.419861   1441.210904            360.776205
    53   7040.0    1417.230348   1456.482866            360.759510
    54   7168.0    1424.372338   1449.727641            361.842024
    55   7296.0    1427.339732   1083.296232            361.949283
    56   7424.0    1428.255037   1094.838667            363.003595
    57   7552.0    1427.233888   1106.635235            363.577644
    58   7680.0    1432.281005   1125.650276            363.545777
    59   7808.0    1431.351982   1130.147199            364.280576
    60   7936.0    1430.659916   1139.025222            364.318255
    61   8064.0    1433.345870   1145.074767            364.931237
    62   8192.0    1435.150613   1148.824159            363.610686
    63   8320.0    1382.421180   1117.939880            361.734664
    64   8448.0    1383.287376   1122.720475            362.398523
    65   8576.0    1387.058038   1128.365378            363.365774
    66   8704.0    1379.906314   1131.405017            364.004396
    67   8832.0    1392.391758   1130.588667            364.973796
    68   8960.0    1385.932326   1139.154370            365.896523
    69   9088.0    1395.006824   1134.326903            366.813384
    70   9216.0    1404.893791   1142.548776            367.440468
    71   9344.0    1389.988917   1420.765524            367.559703
    72   9472.0    1396.869570   1431.668995            368.531030
    73   9600.0    1402.665830   1432.457045            369.078421
    74   9728.0    1401.532774   1442.786291            369.634830
    75   9856.0    1397.945336   1441.280822            369.954578
    76   9984.0    1392.863212   1450.645815            370.631301
    77  10112.0    1403.576282   1454.549905            371.224474
    78  10240.0    1410.315105   1468.411733            371.940976
    79  10368.0    1416.010910   1458.203432            369.762097
    80  10496.0    1407.891808   1465.359969            370.626271
    81  10624.0    1407.305104   1463.921210            371.032824
    82  10752.0    1400.051061   1469.047897            370.896287
    83  10880.0    1392.430263   1477.504944            372.169037
    84  11008.0    1421.195262   1475.361973            372.615900
    85  11136.0    1418.874027   1483.655215            373.445625
    86  11264.0    1411.197725   1484.556825            373.494551
    87  11392.0    1420.651212   1485.797211            373.681825
    88  11520.0    1414.838909   1492.640199            373.944795
    89  11648.0    1423.949381   1499.155138            374.511621
    90  11776.0    1438.218482   1502.293989            374.974777
    91  11904.0    1428.092967   1510.301556            376.124476
    92  12032.0    1414.184001   1510.118274            375.642002
    93  12160.0    1418.729620   1514.973544            376.054410
    94  12288.0    1429.711691   1418.608932            375.741689
    95  12416.0    1440.060373   1394.207656            374.800537
    96  12544.0    1443.984176   1395.798974            375.634764
    97  12672.0    1440.172812   1392.588615            374.987609




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.154 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

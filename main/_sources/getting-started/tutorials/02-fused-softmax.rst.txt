
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     477.452758    687.625491            204.135601
    1     384.0     665.835507    812.187671            262.202148
    2     512.0     806.037452    936.898899            303.757566
    3     640.0     926.791970    925.363243            331.645967
    4     768.0     988.292467    990.759307            351.347010
    5     896.0    1040.388770   1038.266958            354.034783
    6    1024.0    1068.102623   1066.505669            353.410295
    7    1152.0    1087.004869   1074.595808            351.102605
    8    1280.0    1133.922087   1103.656560            349.782519
    9    1408.0    1168.183362   1131.738416            342.682426
    10   1536.0    1188.519873   1166.211807            333.018689
    11   1664.0    1217.227825   1188.981376            329.701890
    12   1792.0    1225.379157   1192.672733            325.587791
    13   1920.0    1261.612764   1221.098491            325.422429
    14   2048.0    1272.351976   1251.710789            324.896064
    15   2176.0    1241.964486    961.869200            325.876729
    16   2304.0    1256.998276   1003.686361            326.296539
    17   2432.0    1268.541280   1033.902197            326.155710
    18   2560.0    1287.491876   1067.791487            328.507782
    19   2688.0    1289.173083   1097.443785            329.287754
    20   2816.0    1306.653811   1122.377288            329.842193
    21   2944.0    1316.678432   1146.530710            331.157084
    22   3072.0    1319.580777   1174.198382            333.563027
    23   3200.0    1337.405182   1173.395327            335.109113
    24   3328.0    1345.468914   1204.992492            336.796329
    25   3456.0    1348.641625   1219.630836            337.108758
    26   3584.0    1361.649227   1247.724762            338.425893
    27   3712.0    1371.439258   1262.110306            340.517076
    28   3840.0    1368.995235   1283.574631            340.377726
    29   3968.0    1377.172237   1302.592021            341.398529
    30   4096.0    1390.684327   1316.454075            338.934112
    31   4224.0    1334.119474   1279.395840            343.416586
    32   4352.0    1341.009330   1294.145067            345.450678
    33   4480.0    1349.079788   1318.564179            345.445785
    34   4608.0    1356.377570   1329.788058            346.965012
    35   4736.0    1358.390410   1342.455134            348.154333
    36   4864.0    1366.093060   1359.663154            349.271758
    37   4992.0    1372.425957   1371.992356            350.171805
    38   5120.0    1373.127656   1381.810725            350.719578
    39   5248.0    1374.844090   1355.263718            351.401688
    40   5376.0    1378.899566   1370.111701            351.751620
    41   5504.0    1380.384102   1383.717162            353.736539
    42   5632.0    1394.203204   1396.525347            352.674950
    43   5760.0    1393.162517   1396.332734            354.704489
    44   5888.0    1392.492898   1408.517627            354.368944
    45   6016.0    1403.446510   1420.830572            357.006533
    46   6144.0    1410.528268   1434.786873            357.292765
    47   6272.0    1406.634105   1397.788382            357.557925
    48   6400.0    1410.997237   1403.963584            358.539726
    49   6528.0    1412.462441   1424.461807            359.176536
    50   6656.0    1414.484041   1424.354301            359.746672
    51   6784.0    1416.401038   1432.055414            360.523088
    52   6912.0    1427.926843   1452.961139            360.964797
    53   7040.0    1423.754673   1439.954706            360.547930
    54   7168.0    1420.639054   1463.100835            361.303981
    55   7296.0    1424.958006   1085.711317            362.401483
    56   7424.0    1429.643608   1095.454030            363.180394
    57   7552.0    1427.140009   1110.618950            363.614113
    58   7680.0    1429.930340   1118.463386            363.943899
    59   7808.0    1432.800399   1128.558454            364.588103
    60   7936.0    1435.334241   1138.642893            364.674821
    61   8064.0    1431.618726   1147.401548            364.772858
    62   8192.0    1432.818841   1150.928320            364.017301
    63   8320.0    1383.931620   1116.886363            361.837456
    64   8448.0    1385.499485   1120.814617            362.831368
    65   8576.0    1384.832534   1125.757057            363.067547
    66   8704.0    1379.867259   1132.074877            364.757311
    67   8832.0    1393.599179   1131.468496            364.991580
    68   8960.0    1383.584223   1138.444933            365.963241
    69   9088.0    1398.696974   1134.789373            366.675525
    70   9216.0    1404.290614   1140.607470            367.419770
    71   9344.0    1390.093463   1419.325064            367.672673
    72   9472.0    1397.665099   1432.176055            368.730777
    73   9600.0    1404.400415   1427.665401            369.571769
    74   9728.0    1395.845642   1441.489698            369.580326
    75   9856.0    1397.525657   1441.049288            369.802535
    76   9984.0    1396.700256   1451.178011            370.027604
    77  10112.0    1407.961352   1454.725238            371.526385
    78  10240.0    1411.498706   1464.918335            371.980978
    79  10368.0    1416.005544   1459.179971            369.447752
    80  10496.0    1408.218304   1466.740582            370.599599
    81  10624.0    1405.501218   1466.915228            370.886037
    82  10752.0    1399.498307   1471.170191            371.278732
    83  10880.0    1394.994418   1476.947032            371.648164
    84  11008.0    1423.836685   1480.945401            372.177976
    85  11136.0    1422.004913   1485.785484            373.072429
    86  11264.0    1411.497659   1487.156322            372.739844
    87  11392.0    1416.090782   1489.529370            373.846698
    88  11520.0    1412.365912   1495.397502            373.869828
    89  11648.0    1420.943825   1499.065512            374.396555
    90  11776.0    1435.368210   1503.546918            375.134967
    91  11904.0    1431.247082   1507.082986            375.641855
    92  12032.0    1413.827862   1509.027774            375.770095
    93  12160.0    1410.624201   1514.669669            376.005661
    94  12288.0    1427.637865   1419.621907            376.746140
    95  12416.0    1440.460181   1395.573432            374.690083
    96  12544.0    1441.705281   1391.055385            375.415194
    97  12672.0    1439.269172   1389.637689            375.229186




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.972 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_


.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     470.577320    682.666653            204.510603
    1     384.0     657.076696    829.586919            262.115602
    2     512.0     821.806754    923.713322            302.475710
    3     640.0     921.223493    916.618032            332.235586
    4     768.0     989.672364    991.454430            350.501715
    5     896.0    1042.017782   1033.440197            356.334886
    6    1024.0    1080.563820   1075.187878            353.596324
    7    1152.0    1100.973882   1076.845159            347.773587
    8    1280.0    1129.271005   1110.342642            348.505266
    9    1408.0    1170.314579   1140.135952            340.299217
    10   1536.0    1196.558777   1165.944780            333.430629
    11   1664.0    1216.396211   1189.454628            329.357241
    12   1792.0    1226.763467   1191.396856            325.771746
    13   1920.0    1251.373812   1219.374186            325.092343
    14   2048.0    1274.068726   1246.330531            324.588972
    15   2176.0    1233.656036    964.672364            326.287153
    16   2304.0    1251.488642   1001.199862            326.334892
    17   2432.0    1269.353421   1032.556579            326.928110
    18   2560.0    1286.701576   1066.792986            327.758436
    19   2688.0    1296.326956   1096.881820            328.574869
    20   2816.0    1306.644097   1123.160142            329.180334
    21   2944.0    1315.893512   1146.021684            331.430632
    22   3072.0    1326.841466   1165.104855            333.184791
    23   3200.0    1341.994315   1174.133241            335.106082
    24   3328.0    1349.301459   1198.927055            336.411846
    25   3456.0    1351.758484   1218.153311            337.475782
    26   3584.0    1364.387566   1244.143829            338.446484
    27   3712.0    1365.132673   1268.806483            340.498328
    28   3840.0    1368.673459   1285.545788            340.469821
    29   3968.0    1377.586795   1298.006810            340.857669
    30   4096.0    1389.803827   1314.677482            338.456415
    31   4224.0    1325.292848   1275.699852            343.026190
    32   4352.0    1343.721299   1297.692721            345.102946
    33   4480.0    1349.982253   1320.772317            345.445784
    34   4608.0    1362.328297   1332.100256            347.284083
    35   4736.0    1358.632127   1341.718123            348.149898
    36   4864.0    1366.929589   1357.039634            349.039037
    37   4992.0    1371.725201   1371.666019            350.415405
    38   5120.0    1371.212050   1383.546220            351.079323
    39   5248.0    1374.867325   1357.105037            351.388581
    40   5376.0    1377.367702   1366.304848            352.349980
    41   5504.0    1378.082763   1379.644736            353.426286
    42   5632.0    1393.128727   1396.316719            352.795419
    43   5760.0    1392.443698   1401.444010            354.925859
    44   5888.0    1393.348652   1413.017329            354.804549
    45   6016.0    1403.173949   1421.291084            356.466776
    46   6144.0    1413.457858   1420.887858            357.339150
    47   6272.0    1409.128957   1402.916730            358.015485
    48   6400.0    1411.208806   1406.776581            358.585884
    49   6528.0    1413.547766   1424.340235            358.973864
    50   6656.0    1414.307620   1422.865343            359.278042
    51   6784.0    1415.055039   1431.541306            360.339635
    52   6912.0    1424.816424   1442.042494            360.822185
    53   7040.0    1422.427836   1443.783439            361.085342
    54   7168.0    1419.190030   1463.715270            361.581921
    55   7296.0    1422.940085   1083.571742            362.443389
    56   7424.0    1434.325116   1098.661842            362.930866
    57   7552.0    1427.655264   1107.962582            363.431838
    58   7680.0    1430.952819   1124.730821            363.649747
    59   7808.0    1431.307118   1129.952737            364.735079
    60   7936.0    1434.131621   1139.387754            364.399648
    61   8064.0    1433.975242   1146.880001            365.135067
    62   8192.0    1431.558909   1148.719393            363.967024
    63   8320.0    1383.439529   1116.430548            361.658722
    64   8448.0    1386.842674   1126.026162            362.791856
    65   8576.0    1393.718273   1127.961059            363.298965
    66   8704.0    1381.697177   1136.434150            364.445077
    67   8832.0    1394.191379   1131.603724            365.343220
    68   8960.0    1382.751275   1136.738228            365.981036
    69   9088.0    1398.647132   1137.728324            366.875678
    70   9216.0    1403.484690   1143.200097            367.827567
    71   9344.0    1388.892786   1421.941653            367.456841
    72   9472.0    1398.131765   1428.378255            368.766577
    73   9600.0    1399.992687   1431.936755            369.150905
    74   9728.0    1397.680321   1440.843794            369.692475
    75   9856.0    1399.982174   1442.801819            370.070934
    76   9984.0    1395.890502   1447.967000            369.737660
    77  10112.0    1404.361211   1456.896140            371.571479
    78  10240.0    1409.563459   1466.370410            371.707577
    79  10368.0    1416.865585   1462.028327            370.405732
    80  10496.0    1410.956297   1466.916924            371.066890
    81  10624.0    1409.398978   1465.130669            370.153842
    82  10752.0    1396.830777   1469.404475            371.250997
    83  10880.0    1391.488425   1479.240654            371.873109
    84  11008.0    1420.288359   1477.139183            372.815297
    85  11136.0    1418.872585   1485.028244            373.156776
    86  11264.0    1411.851024   1487.911130            372.793017
    87  11392.0    1425.326772   1489.039729            373.886875
    88  11520.0    1414.029612   1496.458796            374.351022
    89  11648.0    1424.413331   1500.043429            375.021424
    90  11776.0    1437.095550   1500.950513            375.001641
    91  11904.0    1428.924522   1507.963546            375.021412
    92  12032.0    1415.315573   1509.303679            375.924809
    93  12160.0    1417.066431   1513.605363            376.098738
    94  12288.0    1426.587798   1418.755605            376.105515
    95  12416.0    1438.951749   1391.767485            374.473775
    96  12544.0    1446.728593   1393.424303            375.423971
    97  12672.0    1437.088542   1389.746042            375.352291




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 38.362 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

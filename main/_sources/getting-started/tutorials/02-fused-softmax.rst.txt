
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     480.209474    694.237275            208.277816
    1     384.0     659.475250    834.605308            262.863800
    2     512.0     806.015612    932.308686            301.995271
    3     640.0     924.152723    931.268425            330.822821
    4     768.0     976.944680    989.105594            349.954386
    5     896.0    1048.478610   1028.444209            355.222650
    6    1024.0    1084.054144   1068.835742            353.906217
    7    1152.0    1104.121137   1076.306678            349.773435
    8    1280.0    1140.511934   1105.395399            348.998613
    9    1408.0    1161.915168   1139.740927            340.758864
    10   1536.0    1191.344857   1159.391962            333.434667
    11   1664.0    1218.542308   1190.702015            329.357601
    12   1792.0    1240.071626   1193.529372            325.727273
    13   1920.0    1255.428723   1219.176261            324.898081
    14   2048.0    1275.850618   1252.692401            325.144361
    15   2176.0    1234.934297    964.050598            325.479744
    16   2304.0    1259.806860   1000.462988            326.304572
    17   2432.0    1271.690542   1033.372983            327.391342
    18   2560.0    1287.675555   1072.606222            328.501953
    19   2688.0    1294.167115   1101.711636            329.466990
    20   2816.0    1311.664694   1125.801198            330.250171
    21   2944.0    1320.907441   1148.176289            331.823523
    22   3072.0    1323.542788   1171.333194            333.559320
    23   3200.0    1343.031308   1172.339672            334.743419
    24   3328.0    1342.830515   1198.628757            336.265170
    25   3456.0    1351.158640   1220.758230            337.301903
    26   3584.0    1362.193367   1244.083554            338.561065
    27   3712.0    1365.373363   1263.730861            340.734405
    28   3840.0    1370.011014   1285.457749            340.372843
    29   3968.0    1378.988533   1298.567730            341.104097
    30   4096.0    1386.006122   1320.294692            339.062128
    31   4224.0    1332.891076   1275.009010            343.561436
    32   4352.0    1342.657359   1296.106023            345.304212
    33   4480.0    1349.421789   1313.630988            345.852895
    34   4608.0    1358.212808   1331.405154            346.767783
    35   4736.0    1356.152470   1342.875014            348.337517
    36   4864.0    1367.130442   1359.338355            349.034854
    37   4992.0    1370.403060   1370.862568            349.842213
    38   5120.0    1374.998172   1388.833197            351.076251
    39   5248.0    1378.115120   1351.896393            351.764543
    40   5376.0    1378.295558   1361.945844            351.593579
    41   5504.0    1383.226000   1384.345822            353.637763
    42   5632.0    1395.922415   1396.290321            353.292058
    43   5760.0    1392.309246   1400.668749            355.149166
    44   5888.0    1390.222235   1416.320167            354.976305
    45   6016.0    1402.444589   1425.347365            356.778221
    46   6144.0    1408.685750   1425.529198            356.894351
    47   6272.0    1409.025401   1397.917838            357.784251
    48   6400.0    1415.369840   1403.901729            358.576652
    49   6528.0    1417.479629   1418.633756            359.388660
    50   6656.0    1414.927902   1421.473256            359.403765
    51   6784.0    1421.057760   1433.595742            360.110484
    52   6912.0    1425.505405   1437.455698            361.038447
    53   7040.0    1421.777822   1453.957101            360.910881
    54   7168.0    1423.808344   1455.512803            361.957547
    55   7296.0    1429.165927   1083.138070            362.806759
    56   7424.0    1428.556643   1094.876517            362.909871
    57   7552.0    1426.495666   1107.905726            363.399958
    58   7680.0    1432.930637   1123.202296            363.640703
    59   7808.0    1434.715794   1129.211424            364.416809
    60   7936.0    1433.272045   1140.702011            364.727744
    61   8064.0    1435.809387   1145.746263            365.255269
    62   8192.0    1432.783061   1148.974708            364.021175
    63   8320.0    1381.745676   1117.084768            361.942926
    64   8448.0    1385.211315   1121.371102            362.310055
    65   8576.0    1389.135449   1125.369301            363.227728
    66   8704.0    1379.560709   1129.311418            364.151171
    67   8832.0    1393.371839   1132.277318            364.898221
    68   8960.0    1386.839944   1136.744026            365.652100
    69   9088.0    1396.784952   1135.464695            366.831181
    70   9216.0    1405.462539   1141.558027            367.469143
    71   9344.0    1390.760553   1421.306412            367.631291
    72   9472.0    1398.232938   1431.025188            368.726337
    73   9600.0    1404.479329   1431.253268            368.973552
    74   9728.0    1397.550556   1444.145348            369.843319
    75   9856.0    1401.668934   1440.968606            370.124661
    76   9984.0    1394.522713   1448.192624            370.782069
    77  10112.0    1408.561730   1455.985958            371.362049
    78  10240.0    1410.118738   1462.247537            371.705594
    79  10368.0    1416.060447   1460.500201            370.592583
    80  10496.0    1404.954767   1464.700621            370.604045
    81  10624.0    1409.107886   1465.726157            370.850471
    82  10752.0    1402.763392   1475.418076            371.625548
    83  10880.0    1396.477276   1478.614252            372.275158
    84  11008.0    1421.876286   1478.384385            372.549480
    85  11136.0    1417.814286   1486.159305            372.713265
    86  11264.0    1407.055704   1488.006288            373.583540
    87  11392.0    1421.439153   1491.481650            374.239824
    88  11520.0    1418.230634   1496.118826            374.439453
    89  11648.0    1419.503803   1499.208537            374.038529
    90  11776.0    1431.310897   1502.514954            374.825586
    91  11904.0    1428.535804   1506.825581            375.417406
    92  12032.0    1415.152742   1509.167692            375.924808
    93  12160.0    1409.272187   1514.435188            375.412859
    94  12288.0    1426.442010   1420.010047            376.038909
    95  12416.0    1438.951743   1397.027190            374.700444
    96  12544.0    1441.009958   1394.841985            375.423971
    97  12672.0    1435.353855   1393.727321            375.207214




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.999 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

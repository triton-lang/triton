
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   476.241580   701.365991     204.135601
    1     384.0   656.780075   815.190865     260.455706
    2     512.0   805.214005   934.668187     298.929807
    3     640.0   900.550224   912.096384     329.470125
    4     768.0   975.387227   982.971382     348.108390
    5     896.0  1047.183648  1042.115143     355.788072
    6    1024.0  1081.355699  1075.345326     354.863560
    7    1152.0  1092.613296  1076.393761     349.023971
    8    1280.0  1129.505586  1104.721647     350.564869
    9    1408.0  1168.688079  1133.238535     341.121635
    10   1536.0  1188.405651  1159.298380     331.735793
    11   1664.0  1206.207956  1188.518354     329.577433
    12   1792.0  1226.792225  1191.314086     325.443004
    13   1920.0  1260.164085  1216.928759     324.186126
    14   2048.0  1265.195092  1248.943744     324.081193
    15   2176.0  1232.559423   963.267303     325.704898
    16   2304.0  1256.371950  1004.271403     326.421076
    17   2432.0  1276.042088  1035.187434     327.280490
    18   2560.0  1280.882282  1072.110702     327.697171
    19   2688.0  1293.585466  1099.625401     329.847155
    20   2816.0  1308.776079  1118.899208     329.973270
    21   2944.0  1317.506199  1139.425164     331.333093
    22   3072.0  1317.730567  1170.085777     333.044836
    23   3200.0  1335.029272  1168.612276     334.253437
    24   3328.0  1346.573978  1198.381764     336.119176
    25   3456.0  1346.784674  1217.925642     336.924414
    26   3584.0  1356.506369  1240.567375     338.048874
    27   3712.0  1372.739620  1264.037852     340.623831
    28   3840.0  1371.460112  1277.748066     340.444189
    29   3968.0  1372.595717  1296.213366     341.428627
    30   4096.0  1383.777480  1311.358901     339.026927
    31   4224.0  1331.058548  1279.437370     343.211174
    32   4352.0  1343.370786  1300.602633     345.315635
    33   4480.0  1345.702986  1311.577113     345.882593
    34   4608.0  1357.845466  1329.081400     346.557404
    35   4736.0  1357.795151  1340.846550     347.586550
    36   4864.0  1362.370908  1354.712677     348.522566
    37   4992.0  1375.240422  1375.381210     350.613736
    38   5120.0  1377.010322  1382.664512     350.844810
    39   5248.0  1377.387947  1353.110096     352.232263
    40   5376.0  1374.703393  1370.571787     351.836813
    41   5504.0  1383.618763  1383.908033     353.854202
    42   5632.0  1392.038413  1397.241366     352.809324
    43   5760.0  1389.819518  1403.305150     354.870079
    44   5888.0  1391.907430  1416.186321     355.078510
    45   6016.0  1404.505246  1424.178688     356.457220
    46   6144.0  1405.465235  1426.532694     356.676991
    47   6272.0  1408.156660  1397.849517     358.145110
    48   6400.0  1413.500474  1414.002718     358.608524
    49   6528.0  1416.380367  1423.385092     359.015959
    50   6656.0  1417.625401  1424.006512     359.714734
    51   6784.0  1420.017199  1441.018452     360.293633
    52   6912.0  1425.321628  1449.459986     360.557731
    53   7040.0  1418.180010  1447.984654     360.961871
    54   7168.0  1422.375446  1466.198415     361.645771
    55   7296.0  1425.131201  1082.328571     362.099892
    56   7424.0  1426.214540  1095.325572     362.790037
    57   7552.0  1428.435578  1110.064837     363.049652
    58   7680.0  1429.976087  1119.711682     363.835235
    59   7808.0  1430.322170  1128.776640     364.257881
    60   7936.0  1434.699786  1139.088438     364.454650
    61   8064.0  1432.418978  1145.261326     364.781904
    62   8192.0  1431.611651  1148.669264     363.939954
    63   8320.0  1383.622109  1114.441199     361.663872
    64   8448.0  1382.336243  1122.654103     362.073455
    65   8576.0  1386.941889  1125.461598     362.889734
    66   8704.0  1382.068217  1130.359772     363.857741
    67   8832.0  1393.953149  1130.495997     364.778258
    68   8960.0  1384.332499  1137.815555     365.652099
    69   9088.0  1395.908206  1137.381402     367.013688
    70   9216.0  1404.162828  1141.074825     367.196169
    71   9344.0  1392.716313  1420.401403     367.635770
    72   9472.0  1398.966861  1433.778151     368.690323
    73   9600.0  1404.330223  1429.595693     368.988861
    74   9728.0  1399.146800  1436.330684     369.463952
    75   9856.0  1399.225100  1440.978065     369.883013
    76   9984.0  1393.956983  1451.510386     370.474563
    77  10112.0  1408.404966  1452.126085     371.633057
    78  10240.0  1413.856499  1465.926381     371.393846
    79  10368.0  1413.900149  1460.317395     370.045908
    80  10496.0  1411.643549  1467.827157     370.577377
    81  10624.0  1405.802764  1463.858877     370.868254
    82  10752.0  1395.389387  1471.166933     371.568313
    83  10880.0  1399.385761  1481.228137     371.948150
    84  11008.0  1418.227256  1479.318364     372.921728
    85  11136.0  1418.274024  1484.925259     373.530143
    86  11264.0  1414.538967  1488.450373     373.378927
    87  11392.0  1422.888373  1490.448521     374.027976
    88  11520.0  1416.495985  1494.553525     374.236130
    89  11648.0  1420.407442  1500.873832     374.888299
    90  11776.0  1435.142840  1501.719908     375.406352
    91  11904.0  1428.875153  1510.741110     376.304823
    92  12032.0  1417.313900  1510.522883     375.801028
    93  12160.0  1414.887411  1513.661354     376.440389
    94  12288.0  1427.732978  1418.496715     376.652470
    95  12416.0  1436.331853  1396.965666     374.822637
    96  12544.0  1441.886168  1396.517586     375.625975
    97  12672.0  1440.904373  1396.494793     375.299522




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.978 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_


.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     480.733980    695.421595            207.867220
    1     384.0     665.646927    818.127048            262.672032
    2     512.0     820.449437    933.778876            304.320368
    3     640.0     923.535401    919.094792            330.897175
    4     768.0     987.988537    982.564015            350.835652
    5     896.0    1054.529353   1028.370763            353.026559
    6    1024.0    1066.829363   1077.598816            352.398468
    7    1152.0    1087.850599   1074.537910            348.173748
    8    1280.0    1128.108915   1109.324681            349.128032
    9    1408.0    1160.739480   1140.959714            340.154687
    10   1536.0    1195.496890   1164.984496            334.032319
    11   1664.0    1220.849086   1183.598917            330.676455
    12   1792.0    1237.354947   1193.915954            325.884552
    13   1920.0    1264.446344   1226.501894            324.979683
    14   2048.0    1277.144767   1244.320711            324.676440
    15   2176.0    1240.871813    964.077185            325.222265
    16   2304.0    1248.965580   1003.555114            325.669693
    17   2432.0    1270.834287   1037.226839            326.197805
    18   2560.0    1285.373947   1066.365815            328.097072
    19   2688.0    1291.357228   1096.464419            328.809490
    20   2816.0    1312.080089   1121.501514            328.919170
    21   2944.0    1315.557135   1147.059558            331.857040
    22   3072.0    1325.454238   1169.927768            333.974333
    23   3200.0    1340.662559   1173.678903            335.227220
    24   3328.0    1350.529490   1199.906400            336.503820
    25   3456.0    1349.377115   1221.246566            337.250594
    26   3584.0    1363.538994   1244.089730            338.000783
    27   3712.0    1365.333328   1263.825939            340.225984
    28   3840.0    1376.255983   1280.265380            340.343617
    29   3968.0    1369.931702   1300.900758            341.212007
    30   4096.0    1387.552582   1314.295190            338.792583
    31   4224.0    1326.995487   1274.201911            343.046535
    32   4352.0    1344.549261   1301.311589            345.606779
    33   4480.0    1349.940669   1320.563926            345.675564
    34   4608.0    1358.254790   1335.387564            347.366349
    35   4736.0    1361.965095   1345.080648            348.172263
    36   4864.0    1366.751738   1359.483162            349.214325
    37   4992.0    1374.183308   1377.665810            350.040958
    38   5120.0    1376.336012   1384.305558            350.777337
    39   5248.0    1377.236203   1354.997854            351.750373
    40   5376.0    1380.437208   1374.185177            351.714359
    41   5504.0    1384.398001   1380.210170            353.628359
    42   5632.0    1395.756534   1395.232047            352.966995
    43   5760.0    1391.360798   1397.820106            355.428695
    44   5888.0    1390.370394   1416.292354            354.790630
    45   6016.0    1400.010843   1414.039654            356.457547
    46   6144.0    1407.304939   1420.457292            357.130513
    47   6272.0    1407.696173   1401.053263            357.784250
    48   6400.0    1412.292405   1410.414671            358.535110
    49   6528.0    1415.717702   1427.415966            359.210912
    50   6656.0    1413.423176   1431.320390            359.397453
    51   6784.0    1418.263966   1427.834791            360.408661
    52   6912.0    1422.645916   1453.973317            361.158194
    53   7040.0    1420.846254   1451.711134            361.084873
    54   7168.0    1420.064271   1462.753181            361.654894
    55   7296.0    1428.785258   1085.824467            362.570781
    56   7424.0    1431.969184   1098.523497            363.135492
    57   7552.0    1424.130546   1113.755476            363.504726
    58   7680.0    1431.063723   1119.967483            363.672358
    59   7808.0    1431.853455   1135.512850            364.239727
    60   7936.0    1433.263907   1142.894472            364.661278
    61   8064.0    1430.523609   1152.453383            364.985570
    62   8192.0    1430.366880   1152.991617            363.664256
    63   8320.0    1383.064931   1117.750136            361.663187
    64   8448.0    1383.664719   1123.161562            362.541204
    65   8576.0    1387.765927   1127.715184            363.312326
    66   8704.0    1381.182470   1135.806358            364.520854
    67   8832.0    1395.985075   1133.902331            364.827123
    68   8960.0    1387.413726   1139.634511            365.785381
    69   9088.0    1399.931696   1139.553626            366.262567
    70   9216.0    1405.169577   1144.498962            367.433235
    71   9344.0    1391.080618   1416.949729            367.649196
    72   9472.0    1399.392154   1430.362882            368.730777
    73   9600.0    1400.761797   1430.849230            369.173086
    74   9728.0    1398.557308   1439.776613            369.813294
    75   9856.0    1400.913727   1440.824130            370.241122
    76   9984.0    1394.359203   1447.361103            369.742118
    77  10112.0    1408.945437   1454.734350            371.375368
    78  10240.0    1412.536542   1466.014005            371.452072
    79  10368.0    1417.724978   1464.127763            370.001533
    80  10496.0    1408.718846   1465.855020            370.373057
    81  10624.0    1408.946315   1465.102539            370.406456
    82  10752.0    1394.196646   1471.601822            370.767402
    83  10880.0    1393.627888   1477.589259            371.983474
    84  11008.0    1422.004962   1477.348317            372.682342
    85  11136.0    1417.838190   1485.806658            372.682260
    86  11264.0    1417.955029   1484.310314            372.584842
    87  11392.0    1423.073531   1490.382810            373.508197
    88  11520.0    1416.925757   1497.279041            374.214045
    89  11648.0    1422.631120   1499.034857            374.356741
    90  11776.0    1431.039243   1502.644754            374.828458
    91  11904.0    1431.284761   1509.052617            375.074162
    92  12032.0    1413.796820   1510.575756            375.840805
    93  12160.0    1410.491040   1516.910685            375.518896
    94  12288.0    1428.315870   1420.301036            375.998958
    95  12416.0    1434.292915   1398.680835            374.560890
    96  12544.0    1448.603388   1392.845231            375.371310
    97  12672.0    1439.274565   1391.839713            375.317108




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.018 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

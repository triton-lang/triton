
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     510.021102    698.137592            203.601783
    1     384.0     695.789859    813.314176            261.789571
    2     512.0     832.942161    932.880113            303.362284
    3     640.0     840.197388    917.202446            333.280229
    4     768.0     906.504859    992.725122            350.362973
    5     896.0     967.867626   1041.681231            353.421255
    6    1024.0    1022.364591   1074.328931            353.296065
    7    1152.0    1031.531798   1063.083945            349.957906
    8    1280.0    1077.035865   1107.139721            349.822350
    9    1408.0    1114.414210   1140.135952            341.042224
    10   1536.0    1154.254407   1168.191612            332.827427
    11   1664.0    1183.231210   1181.043845            329.445074
    12   1792.0    1209.921901   1193.421017            325.176391
    13   1920.0    1227.412506   1221.853697            325.066381
    14   2048.0    1250.866996   1255.142941            325.252742
    15   2176.0    1188.881351    964.533712            325.623123
    16   2304.0    1193.784192    998.865116            325.462821
    17   2432.0    1226.034512   1037.905488            326.903925
    18   2560.0    1241.520487   1069.342876            328.518770
    19   2688.0    1261.073907   1096.321832            329.863597
    20   2816.0    1278.552264   1122.243569            330.037214
    21   2944.0    1288.379913   1147.535685            330.910383
    22   3072.0    1301.133815   1168.646995            332.569182
    23   3200.0    1320.641384   1173.912561            334.787099
    24   3328.0    1326.744547   1202.819276            336.636970
    25   3456.0    1340.469673   1217.808245            337.090860
    26   3584.0    1337.752859   1246.202298            338.082939
    27   3712.0    1356.734770   1263.490253            340.371033
    28   3840.0    1351.184778   1277.051298            340.279641
    29   3968.0    1362.026271   1296.301722            341.362936
    30   4096.0    1368.290067   1318.920579            339.004078
    31   4224.0    1333.829661   1279.160663            343.484248
    32   4352.0    1344.353776   1297.394751            345.284303
    33   4480.0    1349.261943   1312.891800            345.547444
    34   4608.0    1363.884152   1331.637897            346.892507
    35   4736.0    1366.859855   1347.494237            348.448484
    36   4864.0    1373.564331   1359.131189            349.000859
    37   4992.0    1377.418676   1374.398922            350.353968
    38   5120.0    1376.348010   1383.075024            351.132041
    39   5248.0    1378.658011   1353.200090            351.800219
    40   5376.0    1385.875600   1363.493365            352.034237
    41   5504.0    1388.602392   1383.971753            353.811835
    42   5632.0    1399.221745   1397.397477            353.041242
    43   5760.0    1397.368870   1394.541098            354.670334
    44   5888.0    1397.463254   1416.130858            354.526381
    45   6016.0    1410.101457   1421.094103            356.271395
    46   6144.0    1411.681877   1430.715502            357.130512
    47   6272.0    1412.623894   1390.453118            357.608707
    48   6400.0    1418.907006   1409.772716            358.239978
    49   6528.0    1417.079471   1424.321137            358.918629
    50   6656.0    1422.146491   1434.566795            359.636512
    51   6784.0    1424.026495   1436.148322            360.528372
    52   6912.0    1432.021914   1448.832539            361.102916
    53   7040.0    1423.360438   1444.161169            360.745756
    54   7168.0    1427.190639   1465.502890            361.395061
    55   7296.0    1430.298067   1083.761327            362.776892
    56   7424.0    1438.121965   1096.505196            362.999048
    57   7552.0    1430.140785   1113.543252            363.823957
    58   7680.0    1435.878152   1123.304387            363.722110
    59   7808.0    1435.186097   1128.899436            364.780594
    60   7936.0    1440.566488   1138.741474            364.507864
    61   8064.0    1440.749638   1148.529170            365.012741
    62   8192.0    1438.045919   1150.431013            363.498381
    63   8320.0    1384.422141   1118.075268            361.801694
    64   8448.0    1384.018118   1122.231080            361.558105
    65   8576.0    1390.393001   1124.962993            363.045311
    66   8704.0    1384.641604   1133.541459            364.093337
    67   8832.0    1392.058010   1131.183296            364.951565
    68   8960.0    1391.006946   1138.203524            365.581058
    69   9088.0    1400.657166   1136.243464            366.360173
    70   9216.0    1404.292194   1141.263517            366.947765
    71   9344.0    1391.402595   1418.942619            367.443428
    72   9472.0    1400.043544   1433.078044            368.429020
    73   9600.0    1403.383227   1433.788277            368.986848
    74   9728.0    1404.012176   1438.817354            369.581636
    75   9856.0    1399.498946   1440.987744            370.209760
    76   9984.0    1390.297162   1445.265378            370.608861
    77  10112.0    1408.709215   1455.316386            371.131336
    78  10240.0    1407.632690   1467.674117            371.545881
    79  10368.0    1415.486751   1462.897299            369.461024
    80  10496.0    1406.258799   1467.147575            370.435218
    81  10624.0    1406.947374   1464.331594            370.694946
    82  10752.0    1396.265085   1473.009447            371.140782
    83  10880.0    1399.201778   1477.357582            372.049725
    84  11008.0    1422.194235   1472.994039            372.284044
    85  11136.0    1423.059951   1485.003718            372.390168
    86  11264.0    1413.497977   1484.546960            372.474205
    87  11392.0    1421.675882   1490.767685            373.935366
    88  11520.0    1414.478349   1494.970674            373.768449
    89  11648.0    1422.988537   1501.677299            374.852815
    90  11776.0    1431.472841   1503.114345            375.477602
    91  11904.0    1428.773589   1505.395297            375.761276
    92  12032.0    1420.496507   1509.796828            376.389715
    93  12160.0    1418.057258   1514.311276            376.156372
    94  12288.0    1426.371122   1420.596194            375.905769
    95  12416.0    1433.398490   1394.251919            374.531140
    96  12544.0    1443.375763   1393.974000            375.397638
    97  12672.0    1435.645510   1389.565919            375.185238




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 36.675 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

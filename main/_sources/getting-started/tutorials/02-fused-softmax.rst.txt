
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     480.043818    707.231076            208.446849
    1     384.0     668.541653    832.070811            262.380810
    2     512.0     802.211996    908.774858            300.291927
    3     640.0     917.516801    915.447370            329.130642
    4     768.0     986.414126    988.403161            348.778566
    5     896.0    1038.009285   1027.650616            354.184486
    6    1024.0    1083.785440   1079.056410            352.580458
    7    1152.0    1101.464622   1074.804102            350.243381
    8    1280.0    1138.546240   1104.881943            349.261977
    9    1408.0    1159.158862   1140.162485            341.333340
    10   1536.0    1190.942616   1156.332744            333.545671
    11   1664.0    1213.245561   1191.114094            329.104788
    12   1792.0    1236.189426   1191.719329            326.127019
    13   1920.0    1254.607502   1226.231735            324.129433
    14   2048.0    1267.867212   1242.485167            324.826793
    15   2176.0    1232.343979    963.787255            325.413590
    16   2304.0    1251.284001    998.805864            325.851905
    17   2432.0    1274.642415   1032.235255            326.522706
    18   2560.0    1289.779371   1068.352487            328.603488
    19   2688.0    1298.377494   1098.044408            329.679055
    20   2816.0    1309.207209   1125.964387            329.656607
    21   2944.0    1320.760400   1144.400347            332.124601
    22   3072.0    1318.887500   1174.483615            333.526012
    23   3200.0    1342.523477   1168.829616            334.790694
    24   3328.0    1345.545929   1201.999552            336.001739
    25   3456.0    1351.827129   1216.551027            336.685582
    26   3584.0    1360.085346   1239.820352            337.555506
    27   3712.0    1368.995574   1263.269490            340.532250
    28   3840.0    1369.512021   1281.238291            340.035217
    29   3968.0    1378.207940   1296.864160            341.483003
    30   4096.0    1391.818039   1319.595488            339.186092
    31   4224.0    1328.530185   1275.753746            343.056262
    32   4352.0    1339.181912   1297.776300            345.431871
    33   4480.0    1351.325909   1318.269249            345.894582
    34   4608.0    1359.553277   1329.296971            346.491515
    35   4736.0    1354.321742   1340.784334            347.659539
    36   4864.0    1367.542207   1355.563477            348.759678
    37   4992.0    1373.281245   1370.557628            349.968255
    38   5120.0    1372.427521   1385.002971            350.464697
    39   5248.0    1375.041581   1351.135635            351.520326
    40   5376.0    1378.087992   1371.252653            351.988367
    41   5504.0    1383.171275   1374.190249            353.698904
    42   5632.0    1397.816534   1392.415159            353.863274
    43   5760.0    1397.115001   1405.145032            355.079352
    44   5888.0    1391.531461   1416.568796            354.809191
    45   6016.0    1394.732885   1420.072042            356.609923
    46   6144.0    1413.569896   1425.314125            356.750955
    47   6272.0    1409.487842   1397.365275            357.747280
    48   6400.0    1412.165651   1411.587766            358.051163
    49   6528.0    1415.589004   1412.310930            359.144277
    50   6656.0    1421.169170   1431.503733            359.636512
    51   6784.0    1415.432824   1429.208252            360.583652
    52   6912.0    1420.099131   1447.954604            360.711853
    53   7040.0    1425.163521   1448.630985            360.553304
    54   7168.0    1424.341777   1466.359722            361.965359
    55   7296.0    1429.322841   1083.546858            362.273475
    56   7424.0    1432.666165   1095.026389            362.799120
    57   7552.0    1424.106230   1108.282899            363.491056
    58   7680.0    1432.418541   1118.266844            363.378643
    59   7808.0    1429.940823   1130.176767            363.954034
    60   7936.0    1430.701161   1139.246687            364.503352
    61   8064.0    1437.452338   1145.486253            364.981040
    62   8192.0    1431.172799   1149.042671            364.044143
    63   8320.0    1385.757372   1113.360000            361.649791
    64   8448.0    1385.482750   1122.613417            362.199731
    65   8576.0    1389.317561   1125.221364            363.236632
    66   8704.0    1381.460412   1132.302179            364.235731
    67   8832.0    1391.836285   1131.150381            364.653936
    68   8960.0    1386.859652   1140.363631            365.812049
    69   9088.0    1393.729880   1136.018490            366.684416
    70   9216.0    1404.291610   1139.619491            367.294156
    71   9344.0    1388.857036   1421.791400            367.595494
    72   9472.0    1398.187827   1432.301138            368.762091
    73   9600.0    1401.429654   1430.167604            369.275153
    74   9728.0    1395.513736   1442.893459            369.869950
    75   9856.0    1400.838458   1439.254590            370.058738
    76   9984.0    1398.031919   1444.658928            370.586980
    77  10112.0    1407.874240   1451.100371            371.508612
    78  10240.0    1411.483591   1465.088038            371.404028
    79  10368.0    1414.585236   1459.890564            370.028156
    80  10496.0    1411.226891   1467.074336            370.795274
    81  10624.0    1404.993691   1464.357132            371.059526
    82  10752.0    1402.149019   1472.439536            371.771164
    83  10880.0    1394.515176   1481.430224            372.173458
    84  11008.0    1420.752768   1475.702136            371.966020
    85  11136.0    1416.894361   1484.055004            373.614698
    86  11264.0    1409.965302   1483.622935            373.103498
    87  11392.0    1422.550391   1491.552545            373.640206
    88  11520.0    1415.242872   1496.711324            374.267213
    89  11648.0    1423.895987   1500.107837            374.135704
    90  11776.0    1437.168626   1501.272226            375.455335
    91  11904.0    1432.870548   1508.576357            375.031419
    92  12032.0    1412.191775   1510.889671            375.659663
    93  12160.0    1413.607888   1514.874835            375.585201
    94  12288.0    1426.315056   1421.210301            376.083310
    95  12416.0    1438.623459   1397.390120            374.888950
    96  12544.0    1444.340694   1394.559489            375.946995
    97  12672.0    1439.109792   1394.050687            375.462273




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.089 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

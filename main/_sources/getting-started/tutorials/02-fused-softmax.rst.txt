
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N  Triton (GB/s)  Torch (GB/s)  Naive Softmax (GB/s)
    0     256.0     479.496079    708.386477            208.583523
    1     384.0     659.698564    818.931938            262.877354
    2     512.0     817.211837    933.956602            304.171941
    3     640.0     911.732112    925.982871            331.218488
    4     768.0     978.554959    987.736932            350.652933
    5     896.0    1048.434407   1041.115843            355.068722
    6    1024.0    1072.646665   1069.026959            355.357350
    7    1152.0    1095.359069   1078.722610            349.893187
    8    1280.0    1139.967720   1110.963008            348.906860
    9    1408.0    1170.741300   1133.988863            342.032077
    10   1536.0    1197.056444   1164.322277            333.276306
    11   1664.0    1210.008851   1190.997069            330.192105
    12   1792.0    1236.209555   1193.802021            326.171503
    13   1920.0    1261.818023   1220.378403            324.692665
    14   2048.0    1268.527683   1249.878803            325.729342
    15   2176.0    1236.403535    964.408404            325.769696
    16   2304.0    1261.724726   1004.584974            326.423843
    17   2432.0    1274.868192   1032.675056            327.066347
    18   2560.0    1281.695611   1072.524948            328.258625
    19   2688.0    1291.531518   1097.363216            329.997671
    20   2816.0    1308.429119   1122.391486            329.789364
    21   2944.0    1321.121047   1147.125266            331.823026
    22   3072.0    1318.719796   1174.925875            333.869838
    23   3200.0    1341.165803   1173.113253            334.991269
    24   3328.0    1344.596689   1202.151911            336.421788
    25   3456.0    1347.952351   1217.778590            337.374361
    26   3584.0    1364.681867   1245.440466            338.503124
    27   3712.0    1365.570550   1264.637372            340.453370
    28   3840.0    1375.574879   1277.077386            340.463460
    29   3968.0    1380.017635   1299.116690            341.074221
    30   4096.0    1387.043490   1318.514379            339.118293
    31   4224.0    1331.104833   1276.462294            343.144719
    32   4352.0    1340.206723   1297.522413            345.739820
    33   4480.0    1344.494980   1321.422699            345.770337
    34   4608.0    1363.108075   1333.834947            346.753720
    35   4736.0    1359.820533   1347.374964            348.371568
    36   4864.0    1367.597437   1360.127663            348.885143
    37   4992.0    1371.309828   1372.617315            350.656480
    38   5120.0    1378.404405   1386.763388            350.697282
    39   5248.0    1372.364497   1352.781326            352.014715
    40   5376.0    1375.639191   1363.616446            351.562455
    41   5504.0    1379.387815   1372.834671            353.530570
    42   5632.0    1395.098243   1399.828971            353.572449
    43   5760.0    1395.224388   1408.355166            354.653066
    44   5888.0    1391.823441   1406.936797            354.994882
    45   6016.0    1402.504719   1426.546308            356.703734
    46   6144.0    1408.920124   1426.580742            357.339148
    47   6272.0    1409.898557   1392.437126            358.140477
    48   6400.0    1414.278897   1411.975138            358.669001
    49   6528.0    1411.429361   1426.340340            359.365592
    50   6656.0    1413.784016   1427.440705            359.894314
    51   6784.0    1416.356642   1435.439023            360.445489
    52   6912.0    1423.225012   1444.857684            360.960195
    53   7040.0    1420.909343   1445.157955            361.204808
    54   7168.0    1425.110113   1459.963672            361.504419
    55   7296.0    1425.954094   1089.219482            362.653199
    56   7424.0    1430.811121   1099.255339            362.949046
    57   7552.0    1428.406756   1112.339254            363.636912
    58   7680.0    1431.970110   1124.447845            363.826183
    59   7808.0    1430.029699   1131.613552            364.817009
    60   7936.0    1439.137359   1144.913031            364.864447
    61   8064.0    1433.684142   1148.883927            365.307370
    62   8192.0    1432.258845   1153.932744            364.303817
    63   8320.0    1383.093674   1118.917699            361.934061
    64   8448.0    1387.865437   1126.153049            362.773299
    65   8576.0    1385.759061   1125.700508            363.887737
    66   8704.0    1382.129516   1136.047534            364.529772
    67   8832.0    1394.500550   1131.813352            365.378867
    68   8960.0    1383.830199   1138.281732            365.940998
    69   9088.0    1397.798814   1138.653767            366.986968
    70   9216.0    1403.064019   1142.590008            367.702895
    71   9344.0    1389.368897   1417.786723            368.147554
    72   9472.0    1395.312710   1433.830126            368.892243
    73   9600.0    1401.253491   1432.722301            369.499928
    74   9728.0    1400.301678   1439.959181            369.869949
    75   9856.0    1399.925428   1438.491294            370.285935
    76   9984.0    1393.690367   1447.889711            369.871423
    77  10112.0    1407.235159   1455.977387            371.335413
    78  10240.0    1411.321279   1468.075066            371.634594
    79  10368.0    1414.810612   1461.188145            370.410179
    80  10496.0    1409.384007   1464.440347            370.066996
    81  10624.0    1401.966183   1467.236431            370.167129
    82  10752.0    1396.709510   1472.482093            371.700548
    83  10880.0    1392.039988   1480.092295            372.208825
    84  11008.0    1419.829142   1472.801691            372.624756
    85  11136.0    1419.534904   1483.831649            372.926019
    86  11264.0    1417.468015   1487.588036            372.939322
    87  11392.0    1422.221255   1491.652593            373.882467
    88  11520.0    1415.863754   1498.561445            373.675934
    89  11648.0    1420.784891   1500.577334            374.038530
    90  11776.0    1431.532143   1501.773317            374.939455
    91  11904.0    1430.041181   1508.159494            375.619540
    92  12032.0    1417.118108   1508.211762            376.146048
    93  12160.0    1413.130050   1515.093365            376.010095
    94  12288.0    1428.121914   1419.820932            376.098909
    95  12416.0    1441.615855   1397.316591            374.531139
    96  12544.0    1437.021484   1395.581367            375.665523
    97  12672.0    1438.192469   1393.902016            375.576721




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.108 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

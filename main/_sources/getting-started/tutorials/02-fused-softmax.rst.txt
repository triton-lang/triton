
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-62

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver

    DEVICE = triton.runtime.driver.active.get_active_torch_device()


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 63-71

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 73-82

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 82-112

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 113-114

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 114-178

.. code-block:: Python


    properties = driver.active.utils.get_device_properties(DEVICE.index)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software pipelining stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                       num_stages=num_stages, num_warps=num_warps, grid=(1, ))
        kernel._init_handles()
        n_regs = kernel.n_regs
        size_smem = kernel.metadata.shared
        if is_hip():
            # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
            # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
            # ISA SECTION (3.6.4 for CDNA3)
            # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
            # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
            # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
            # not required to be equal numbers of both types.
            NUM_GPRS = NUM_REGS
            if is_cdna():
                NUM_GPRS = NUM_REGS * 2

            # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
            # When we divide this number with WARP_SIZE we get maximum number of waves that can
            # execute on a CU (multi-processor)  in parallel.
            MAX_NUM_THREADS = properties["max_threads_per_sm"]
            max_num_waves = MAX_NUM_THREADS // WARP_SIZE
            occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
        else:
            occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
        occupancy = min(occupancy, SIZE_SMEM // size_smem)
        num_programs = NUM_SM * occupancy

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)
        return y









.. GENERATED FROM PYTHON SOURCE LINES 179-181

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 185-192

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device=DEVICE)
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 193-194

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 196-201

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 201-231

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``
            line_names=["Triton", "Torch", "Naive Softmax"],  # label name for the lines
            styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)
        stream = getattr(torch, DEVICE.type).Stream()
        getattr(torch, DEVICE.type).set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        if provider == 'naive_softmax':
            ms = triton.testing.do_bench(lambda: naive_softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch  Naive Softmax
    0     256.0   480.811386   691.489486     208.745812
    1     384.0   658.027688   831.549747     262.559797
    2     512.0   817.556690   912.418785     298.719775
    3     640.0   902.363318   921.338787     329.174635
    4     768.0   980.356920   977.970310     347.926655
    5     896.0  1043.160077  1041.646083     355.691899
    6    1024.0  1079.834584  1082.898589     354.211996
    7    1152.0  1100.817132  1067.084449     347.848357
    8    1280.0  1126.935114  1100.461294     348.283521
    9    1408.0  1160.343923  1129.266460     340.088792
    10   1536.0  1183.398476  1157.181097     333.868596
    11   1664.0  1209.715817  1191.051326     330.133648
    12   1792.0  1235.518463  1193.865939     325.790864
    13   1920.0  1255.608198  1224.407307     324.108056
    14   2048.0  1275.367722  1242.280273     324.429297
    15   2176.0  1236.976854   957.918394     325.753117
    16   2304.0  1256.663509   999.638901     326.477732
    17   2432.0  1276.141494  1035.357756     327.020833
    18   2560.0  1289.718631  1070.768148     327.703197
    19   2688.0  1291.906963  1100.976360     329.206880
    20   2816.0  1314.230628  1120.369801     329.138507
    21   2944.0  1321.999890  1139.334151     331.842157
    22   3072.0  1318.533512  1165.354466     332.852340
    23   3200.0  1331.805981  1172.519078     334.603725
    24   3328.0  1344.329543  1198.313915     335.885632
    25   3456.0  1355.518963  1222.145401     336.381846
    26   3584.0  1363.734449  1241.393012     338.432274
    27   3712.0  1369.095581  1267.674427     340.501389
    28   3840.0  1376.916333  1281.307852     340.617320
    29   3968.0  1369.417016  1296.566973     340.849751
    30   4096.0  1387.178647  1314.755089     338.617739
    31   4224.0  1328.887823  1275.331096     342.958463
    32   4352.0  1345.599049  1296.907732     345.162535
    33   4480.0  1346.471021  1319.794211     345.915858
    34   4608.0  1359.041470  1335.885736     347.009726
    35   4736.0  1356.309512  1345.091979     347.528288
    36   4864.0  1369.290805  1357.783554     348.933422
    37   4992.0  1368.612689  1372.989441     350.719343
    38   5120.0  1382.095044  1390.034159     350.677281
    39   5248.0  1379.151227  1355.595916     351.407425
    40   5376.0  1379.593454  1369.582651     351.342484
    41   5504.0  1380.167495  1378.574774     353.978696
    42   5632.0  1392.815082  1397.343205     353.175896
    43   5760.0  1388.990637  1403.728421     354.911913
    44   5888.0  1391.130813  1417.032262     354.980947
    45   6016.0  1402.304363  1413.469516     356.531409
    46   6144.0  1409.562317  1438.134425     356.917491
    47   6272.0  1409.588080  1396.885441     357.728796
    48   6400.0  1416.609557  1404.347626     358.433602
    49   6528.0  1411.948689  1424.056062     358.932437
    50   6656.0  1419.016156  1424.291452     359.103664
    51   6784.0  1418.450278  1433.224676     360.261438
    52   6912.0  1420.151993  1450.727720     360.698066
    53   7040.0  1421.016285  1447.965021     360.777853
    54   7168.0  1423.538497  1452.990348     361.741589
    55   7296.0  1426.274422  1089.219481     362.296327
    56   7424.0  1428.803841  1100.114982     362.886965
    57   7552.0  1424.307439  1108.144587     363.217896
    58   7680.0  1430.316887  1119.015001     363.767351
    59   7808.0  1432.199526  1133.328485     364.380470
    60   7936.0  1433.359871  1142.597192     364.973881
    61   8064.0  1437.073638  1150.992094     364.872393
    62   8192.0  1429.568927  1149.869438     364.093362
    63   8320.0  1379.791473  1118.579971     362.083505
    64   8448.0  1383.426544  1123.636046     362.676813
    65   8576.0  1386.882699  1127.846528     363.441523
    66   8704.0  1382.126168  1131.984465     364.743918
    67   8832.0  1396.137142  1132.182427     365.062745
    68   8960.0  1386.900460  1138.614495     365.470105
    69   9088.0  1398.051226  1138.865261     366.986968
    70   9216.0  1406.300341  1143.051164     367.491590
    71   9344.0  1390.355972  1420.542234     367.541810
    72   9472.0  1395.223207  1427.725525     368.632031
    73   9600.0  1401.562163  1433.537756     367.970960
    74   9728.0  1395.179596  1438.370777     369.351311
    75   9856.0  1398.169753  1443.148399     370.338077
    76   9984.0  1391.625856  1446.027482     370.550652
    77  10112.0  1402.467981  1455.315405     370.794658
    78  10240.0  1412.364095  1467.636213     371.603539
    79  10368.0  1414.896606  1460.537817     370.090295
    80  10496.0  1407.891805  1466.790821     370.515169
    81  10624.0  1406.473144  1466.764653     371.006127
    82  10752.0  1395.168283  1471.132348     371.034053
    83  10880.0  1394.434578  1479.550307     371.137536
    84  11008.0  1419.165993  1479.432936     372.169140
    85  11136.0  1420.184606  1484.363736     372.593696
    86  11264.0  1415.419851  1486.017853     373.045798
    87  11392.0  1424.529959  1488.357942     373.606120
    88  11520.0  1415.919898  1494.180300     373.416253
    89  11648.0  1418.674321  1498.913532     374.118031
    90  11776.0  1434.974416  1501.913683     374.624396
    91  11904.0  1429.397522  1509.754587     375.311883
    92  12032.0  1415.994221  1510.000134     376.048671
    93  12160.0  1417.346026  1511.932691     375.894923
    94  12288.0  1431.125346  1416.475107     375.874714
    95  12416.0  1438.321034  1397.412262     374.597351
    96  12544.0  1440.303416  1395.663576     375.415194
    97  12672.0  1431.081973  1395.762294     375.299520




.. GENERATED FROM PYTHON SOURCE LINES 232-236

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.960 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
